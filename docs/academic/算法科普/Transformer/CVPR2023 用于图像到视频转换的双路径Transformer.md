##  CVPR2023 | 用于图像到视频转换的双路径Transformer

解读：Freedom

Paper title: Dual-path Adaptation from Image to Video Transformers

Paper:  https://arxiv.org/pdf/2303.09857.pdf

Code:  https://github.com/park-jungin/DualPath

## 导读
传统的图像到视频转换方法，无法有效地处理视频序列中的长期依赖关系。因此，作者提出了一种基于Transformer的模型，并引入了双路径适应机制，旨在解决从单张图像生成高质量视频序列问题。该架构分为空间和时间自适应两条路径。冻结预训练的图像模型并仅训练额外的瓶颈适配器以进行调整。空间路径旨在对空间上下文进行编码，这些空间上下文可以通过预训练图像模型的最小调整从单个帧的外观中推断出来。为了降低计算成本，在空间路径中稀疏地使用低帧率的帧。时间路径对应于应该通过掌握以高帧速率采样的几个帧之间的动态关系来编码的时间上下文。特别是出于两个原因，构建一个由连续的低分辨率帧组成的网格状框架集作为时间路径的输入：（i）防止同时计算多个帧造成的计算效率损失； (ii) 精确模仿 ViT 推断输入标记之间的全局依赖关系的能力。每个Transformer block中都采用了轻量级瓶颈适配器。特别是对于动态时间建模问题，采用连续的帧合并到类似网格的帧集中，以精确模仿 ViT 捕捉tokens序列之间的关系。

作者从视频理解的统一角度在多个动作识别 benchmarks 与其他方法进行了比较，以极低的训练和推理计算成本实现了高性能。

![各方法性能比较](https://img-blog.csdnimg.cn/fabb3aabe5154beeb8198cb0534f8662.png#pic_center)




## 简介
图像到视频的转换是计算机视觉领域的一个研究热点。由于图像和视频之间存在很大的差异：视频由一系列帧（图像）按照时序关系组成，视频比图像具有更丰富的运动变化。因此，需要特殊的处理来适应视频数据。迁移学习根据特定任务目标对预训练模型的权重参数进行微调，然而这需要高质量的训练数据和大量的计算资源。一些微调方法将额外的多层 MLP 层训练到模型的顶部，被广泛用于低成本训练，但性能不太理想。目前，NLP 领域使用 PETL 迁移方法作为替代方法进行模型微调，以适用于大规模的 NLP 模型。它们仅学习少量额外的可训练参数，同时保持预训练模型的原始参数不变，已经成功地达到了与不进行微调的模型相当甚至超过的性能。类似于Transformer从 NLP 领域应用到CV领域的情况，一些研究者也试图将这些方法应用到图像领域中来，但是其中的大部分工作主要几种在将图像模型转移到图像任务  以及将视觉语言模型转移到视觉语言任务。受其启发，作者思考：(1) 是否可以对图像模型的参数进行微调，进而将其迁移到视频领域；(2) 迁移后的模型是否可以达到与对视频精心设计的模型一样的性能？

通常而言，Video Transformer需要比 图像ViT 更复杂的架构和更多的参数量用于推理上下文的关系，。因此，为视频理解传输图像模型的挑战是在利用预训练图像模型的判别空间上下文的同时，对视频的时间上下文进行编码。



## 贡献

本文的贡献主要有以下几点：

(1) 提出了一种新的从单张图像到高质量视频转换的方法，使用基于Transformer的模型来完成图像生成网络和视频生成网络的训练；

(2) 引入了双路径适应机制，通过将图像生成网络和视频生成网络的输出进行融合，以达到更好的转换效果。这种机制可有效缓解由于图像和视频之间存在差异所导致的信息丢失和误差累积问题；

(3) 与其他方法进行了大量的对比实验验证该方法在生成高质量视频方面具有较好的性能和泛化能力。此外，对部分模块进行消融实验以及可视化分析。



## 方法 Method

本文所提的整体框架结构如下图：

![DUALPATH的总体架构](https://img-blog.csdnimg.cn/5731adb176954cc4b71c6d1ccbe2c6ff.png#pic_center)

本文是首次将基于 CNN 设计的双路径分支结构应用到Transformer框架中，其中一条路径通过图像编码器和解码器生成单张图像，另一条路径通过视频编码器和解码器生成完整的视频序列，最终将两条路径的输出进行融合得到最终结果。这里通过两条不同的路径分别学习时间和空间上下文，并且没有造成额外的计算成本。在训练期间，只更新引入的适配器和分类器，同时冻结预训练层。

### Spatial adaptation（空间自适应）

空间适应是指如何将在图像域中训练的模型适应到视频域中。为了通过轻微的参数调整使出色的空间建模能力更适合视频理解，在每个转换器块中为 MHA 和 MLP 采用了两个并行适配器。并行适配器允许模型从目标视频数据的外观中学习动作识别的空间上下文，同时保持对象识别的原始上下文。首先从视频中采样 TS 帧并进行标记，Spatial Tokens 集合 $X^{SP}_t$ 包括可学习的位置编码 $p^{SP}$ 和 Spatial Class Token $X^{SP}_{t} {[CLS]}$ 。根据上面框图最右侧结构所示，输入先经过LN，再进行空间自适应，与MHA操作结果和原输入进行合并，得到第 $l$ 个Transformer块中的空间自适应，可以用以下式子形象的进行表示，分别用红色和蓝色表示可学习和冻结参数。

![](https://img-blog.csdnimg.cn/16fec5b5d16845308d85c17e6e7b9341.png#pic_center)

其中：

![](https://img-blog.csdnimg.cn/7cc6f9257ea64bf4932900c592251c62.png#pic_center)

然后，对最终Transformer块的空间 [CLS] 标记集计算均值，以获得全局空间表示 $ y^{SP}$

![](https://img-blog.csdnimg.cn/1361705bb94f4ab48e0857c9b92b9233.png#pic_center)

由于不需要高帧率进行外观语义理解，这反而会带来高计算成本，所以以低帧率对 TS 帧进行稀疏采样。

### Temporal adaptation（时间自适应）

时间适应是指如何在视频序列中学习到时间信息，以适应视频任务。视频 Transformer 对上下文建模的核心组件功能应该是学习视频中跨帧的局部 patches 之间的关系，本文提出了一种新颖的类网格框架集变换技术，该技术可将多个帧聚合到一个统一的网格类框架集中。首先从视频中采样 T 帧，并用 $w$ 和 $h$ 的因子对它们进行缩放，得到缩放后的帧大小：

![](https://img-blog.csdnimg.cn/2294ec5f182442b4b434b4a68336e75f.png#pic_center)

然后根据时间顺序堆叠 $w × h$ 缩放帧，并重塑堆叠帧以构建一组与原始帧大小相同的网格形式的帧（即 $[W × H ×3]$），网格状框架集的总数为 $TG = T /wh$。与空间自适应不同，为了考虑 patches 之间的绝对时间顺序和空间位置。，对 tokens 使用固定的 3D 位置编码，允许Transformer 在同一级别观察多个帧。

虽然在空间路径中使用了并行适配器，但按上述框图右侧的Temporal-path Adaptation结构，顺序的将适配器分别放在每个Transformer  Block 中 MHA 和 MLP 层的顶部。因此，依次是LN，MHA/MLP，再是经过适配器的结果与跳跃连接相加。得到第 $l$ 个块中的时间自适应可以描述为：

![](https://img-blog.csdnimg.cn/f19f970212cd4ee498598dc8b3586d03.png#pic_center)

其中：

![](https://img-blog.csdnimg.cn/f8c47af6e9894d57a398781d29ef933d.png#pic_center)

类似于空间适应，通过对最终 Transformer 块的时间 [CLS] tokens 集合进行平均得到全局的时间表示 $y^{TP}$ ：

![](https://img-blog.csdnimg.cn/7e84776305d74f1ca4b841bc7c02421e.png#pic_center)

最后将全局空间和时间表示连接起来，再将它们馈送到两个 FC 层之间含 GeLU 激活函数的分类器，进而得到最终预测结果。



## 实验

作者使用了四个动作识别数据集，分别是Kinetics400 (K400) 、HMDB51 、Something-something-v2 (SSv2)  和 Diving-48。各数据集介绍如下：

- Kinetics-400 (K400)   包含400种动作类别的视频片段，每个类别有大约400个片段，并且每个视频片段都有时间戳和对应的动作标签；
- HMDB51   包含51个动作类别的视频片段。该数据集采集自电影、电视剧和网络视频。涵盖了常见的动作类别，如走路、跑步、跳跃、打斗、骑车等。每个类别的视频片段时长不同，有的为几秒钟，有的为几分钟，分辨率为240x320；
- SSv2  由约220,000个视频剪辑组成，每个视频剪辑持续时间为2到4秒，并且包含有关人类互动和手势的各种动作；
- Diving-48  常用于水下动作识别任务的视频数据集，包含48个水下动作类别。其中每个类别都至少有27个视频，总共包含了约6000个视频样本。所有视频均以60fps的帧率进行采样，并且已标注了每个动作的开始和结束时间。

作者使用 CLIP 预训练的 ViT-B/16 和 ViT-L/14 作为主干，首先将所提出的方法与在大规模图像数据集上预训练并在 K400 上完全微调的最先进视频模型进行比较。在内存和计算效率方面，视频模型需要大量参数（~450M ）和计算量（~48912 GFLOPs ），本文仅需 10M 可学习参数，并且性能达到了SOTA

![Kinetics-400数据集上动作识别的性能比较](https://img-blog.csdnimg.cn/c65aa6a7eb3942ccbf6c40cbf98b79b8.png#pic_center)

接下来是SSv2数据集上的对比情况，可以看到使用 ViT-L/14 的 DUALPATH 表现出更具竞争力的性能，优于大多数先前的工作。时间建模能力相对较弱的 baselines 表现出明显较差的性能。对此，作者认为是由于 SSv2 数据集对强时间建模对于是强制性的导致的

![SSv2数据集上动作识别的性能比较](https://img-blog.csdnimg.cn/cacee7b376fc4d5581bc9be5e09bdefb.png#pic_center)

随后是在剩余两个数据集上的比较，本文方法性能出色

![在HMDB51和Diving-48两个数据集上的比较结果](https://img-blog.csdnimg.cn/a5dc64fecd61486ca1d11a673b47639b.png#pic_center)

还有一些可视化展示~

![SSv2数据集视频的每个路径的注意力图的可视化](https://img-blog.csdnimg.cn/5be5f5695766497f8d3a9fa870ac9aab.png)



## 结论 Conclusion

本文了新颖的图像到视频迁移学习方法 DUALPATH。通过将双路径设计纳入图像转换器，DUALPATH 使图像模型适应具有少量可训练参数的视频动作识别。空间路径自适应增强了视频数据的预训练图像转换器的固有空间上下文建模。时间路径自适应将多个帧转换为统一的网格状框架集，使图像模型能够捕获帧之间的关系。在每条路径中适当地使用瓶颈适配器来调整预训练特征以适应目标视频数据。此外，本文将最近的 PETL 方法 转换为图像到视频的自适应。实验结果证明了双路径设计和网格状框架集提示的优越性，优于现有其他方法。

未来的工作有很多可能的方向，包括跨领域迁移学习。虽然本文探索了图像到视频的迁移学习，但大型基础视觉语言模型还是可以继续使用的。将高级预训练 2D 知识扩展到 3D 空间建模也很有趣。希望本文的研究能够促进研究并为跨领域迁移学习奠定基础。



