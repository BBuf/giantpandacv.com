# 前言
之前投了很多次平安科技的实习，但都没消息回复。五一前平安科技hr打来电话，让我参与python开发面试（因为自己不确定能不能做算法，所以投递方向填了两个分别是python开发和算法实习生），后续她会发你一个邮件确认面试，十分感谢这个hr又把我捞回到了算法里面，另外需要做一份自我介绍的ppt，介绍项目博客经历什么的，回复给她即可。总的来说问的十分基础且深入，面试官不会特别刁难你，但是问的问题都是能区分你是否理解算法背后的基本原理

# 面试过程
1. 先简单介绍一下你自己
blablabla，这方面准备个一两分钟就差不多了
2. 你能谈谈你隐写检测项目吗
在这个项目里，我复杂的是论文复现。复现了一共有三个网络。Xunet则是将高通滤波器加入到卷积神经网络中，加速网络收敛。Yenet则是将传统隐写检测的算子，也可以看作是一种滤波器，加入到卷积神经网络中。进一步提高检测性能。SRnet则是完全抛弃传统方法，用了四种残差连接模块进行组装，进一步加深网络。由于比赛是彩图，我在SRnet基础上增加了RGB三通道分离，分别送入网络进行计算
3. 谈下百度目标检测识虫这个比赛
比赛用的是百度提供的农业林害病虫数据集，基于YOLOV3的基线代码，加入了seblock，图像增广，包括随机旋转，缩放。学习率热身，和指数衰减。提高基线代码的性能
4. 谈下Kaggle deepfake检测比赛
我先是对整个视频抽取20帧图片，送入EfficientNet进行卷积，得到的特征再送入LSTM网络里面，最后接入全连接层进行一个二分类，判别是真脸还是假脸。做deepfake检测用的方法有很多，有的人直接是CNN，也有的人是采用CNN+LSTM，还有的是应用3D CNN做。但我自己实际做效果不是很好
5. 也就是说你这个比赛是没有成绩的？
嗯对的，最后成绩并不是很好（这种我觉得直接说明实际情况就行，面试官也表示没事）
6. 你知道批量归一化层吗
就是BN层，自然界中数据大部分都是正态分布，我们假设一个批量样本是处于正态分布当中，统计这批数据的均值和方差，再加上拉伸和偏移参数，将这批数据进行归一化，有助于网络学习和收敛
7. 那么均值和方差在BN层训练和预测中是怎么统计的呢？
（这题就很明显是用来区分调包侠的）
训练的时候，采用的类似动量法，使用移动平均来进行均值和方差统计
预测的时候，是采用训练时估算的全局均值和方差
8. 常见的分类网络你知道有哪些
resnet，densenet，Googlenet，SEnet，resnext，res2net，还有最近新出的resnest加了注意力版本的
9. 能讲讲这几个网络的特征吗
resnet采用了shortcut，也就是短路连接结构，能一定程度上缓解梯度消失，让网络更深。而densenet是受resnet启发，将残差连接改为了在通道维上并联特征。 GoogleNet是用了不同大小卷积核和池化操作，得出来特征，再并联起来，丰富了通道的特征。Senet则是加入了注意力模块。
10. resnet为什么有效
（这个问题具体答案我也并不是很清楚，我认为现在学术界也没有一个完全的定论）resnet改变了学习目标，将学习目标f(x)改为f(x) - x。当网络较深的时候，我们根据链式法则求偏导，梯度是累乘起来的，因此深层网络反传回来，当梯度小于1，则累乘起来会越来越小。因此根据梯度优化就会很小，引入额外一条路径，能有助于梯度传播，进行优化。也可以认为残差映射f(x)-x更容易优化，即使没学到东西，网络参数也不会改动很奇怪，最差情况也是让网络学习到恒等映射。
11. 你提到了那么多注意力模块，能讲讲seblock实现原理么
首先是对特征图进行一个全局平均池化，得到(batch, 1, 1, channel）特征图
然后再将特征图通过两个全连接层进行缩放，最后通过一个sigmoid激活，来计算每个通道的重要性（也就是注意力）。之所以选择seblock是因为在其他任务上我使用效果也还可以，而且全连接层参数可以自己缩放，也不会引入太多的额外计算量
12. 你知道常用的优化器有哪些吗，背后的原理可以讲一讲吗
（还好tm复习了一早上优化器，这方面不难，优化器都是在前面基础改进，很容易记忆，这个点我认为也是区别调包侠的问题）
具体可以参考我这篇博文https://blog.csdn.net/weixin_44106928/article/details/102515546
首先是SGD随机梯度下降，统计一批量的样本的梯度，作为整个数据的梯度，进行梯度优化。动量法Momentum，在梯度基础上引入移动平均，计算出来的梯度可以看作是近几个时间步的移动平均，更加稳定。AdaGrad则是维护一个状态变量st = st-1 + 梯度的平方，实际优化是学习率除以根号下st
，能够实现训练过程中自动调节学习率大小，但缺点是学习率一直减小，可能网络还未收敛时候，学习率已经过小了。Rmsprop算法则是对Adagrad的状态变量st引入移动平均，移动平均并不是单调递增的函数，因此学习率是不会一直减小的。Adam算法则是结合了动量法和RMSprop算法，分别对梯度，状态变量st，都应用了指数移动平均，最后再进行一个偏差修正。
13. 你最近有阅读什么论文吗
（这个就实际情况自己答了，之前看论文多是做比赛那时期）
看了华为的两篇GhostNet, AdderNet。最近出的ResNest，Yolov4，其他大部分都是关于deepfake的论文
14. 谈下你对yolov3算法认识
（这个也是准备了一上午，yolov3并不是很难）
yolov3整体结构有两个，一个是骨干网络，原始论文采用的是darknet53，第二个是detectionBlock，专门用于目标检测
yolov3将输入图片640x480，以32x32小方块，划分成了20x15个方块。在每个方块内部生成不同大小的锚框，再基于原始锚框进行微调，也就把目标检测问题转化为回归问题。
总共需要预测5+K个值，一个是锚框内部是否有物体，而是锚框的xywh四个坐标值，K则是K个类别
15. 那你能讲讲yolov2，v1算法吗
（这个自己没了解过，有兴趣的还是得额外准备准备）不好意思，这方面没了解过
16. 那好，你具体想做哪个方面的呢
我可能更想做与实际落地相关的任务，比如图片分类和目标检测这方面的工作
17. 数据在神经网络是怎么流的呢？
（这个问题没答好，因为也不知道应该怎么答。。。） 
19. 问面试官问题，我问的是问部门方向是偏研究方向还是工业落地
我们部门比较偏向算法研究，比如人脸识别方面的，blabla后面我也忘了

# 总结
过完了五一我才来写这篇博文，真的面试印象十分深刻。我相信大部分问题都是学习深度学习中你肯定遇见过，也查过文档。但是能否理解深刻，还是需要多翻书多看博客。我手上的那本书都快被翻烂，而不要仅仅做一个没有梦想的调包侠。这些问题并不难，深刻理解了深度学习中的一些问题和解决方法，相信这些面试也不会难，在这个艰难的2020继续加油共勉！