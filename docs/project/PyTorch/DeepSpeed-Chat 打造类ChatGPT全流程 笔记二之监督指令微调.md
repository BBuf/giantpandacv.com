æ–‡ç« ç›®å½•

    ç³»åˆ—æ–‡ç« 
    0x0. å‰è¨€
    0x1. ğŸ• Supervised finetuning (SFT) æ•™ç¨‹ç¿»è¯‘
        ğŸƒ å¦‚ä½•è®­ç»ƒæ¨¡å‹
        ğŸƒ å¦‚ä½•å¯¹SFT checkpointè¿›è¡Œè¯„æµ‹?
        ğŸ’ æ¨¡å‹å’Œæ•°æ®
        â˜€ï¸æ¥è‡ªOPT-1.3BåŠå…¶SFTå˜ä½“ï¼ˆä½¿ç”¨ä¸åŒå¾®è°ƒæ•°æ®ï¼‰çš„æç¤ºç¤ºä¾‹
        â˜€ï¸ ä¸€äº›å‚æ•°è§£é‡Šå’Œå¯è®­ç»ƒçš„æœ€å¤§æ¨¡å‹
        ğŸ‘€ å…¶å®ƒ
    0x2. è¯„æµ‹è„šæœ¬è§£è¯»
    0x3. è®­ç»ƒè„šæœ¬è§£è¯»
        0x3.1 å¤´æ–‡ä»¶ç›¸å…³è§£æ
            create_prompt_datasetè§£æ
            å·¥å…·å‡½æ•°è§£æ
                print_rank_0
                to_device
                save_hf_format
                set_random_seed
                get_all_reduce_mean
                get_optimizer_grouped_parameters
                save_zero_three_model
                load_hf_tokenizer
                convert_linear_layer_to_lora
                convert_lora_to_linear_layer
                only_optimize_lora_parameters
                create_hf_model
        0x3.2 main.pyä¸»ä½“è§£æ
            parse_argsè§£æ
            mainå‡½æ•°è§£æ
    0x4. hybrid_engineçš„ç»†èŠ‚ & log
    0x5. æ€»ç»“

# ç³»åˆ—æ–‡ç« 
- [DeepSpeed-Chat æ‰“é€ ç±»ChatGPTå…¨æµç¨‹ ç¬”è®°ä¸€](https://mp.weixin.qq.com/s/QQC5ZxlyqqnK52TthqZdjw)
- [ã€DeepSpeed æ•™ç¨‹ç¿»è¯‘ã€‘ä¸‰ï¼Œåœ¨ DeepSpeedä¸­ä½¿ç”¨ PyTorch Profilerå’ŒFlops Profiler](https://mp.weixin.qq.com/s/2g1o4p_RQ8NP3DqkWoRasg)
- [DeepSpeedç»“åˆMegatron-LMè®­ç»ƒGPT2æ¨¡å‹ç¬”è®°ï¼ˆä¸Šï¼‰](https://mp.weixin.qq.com/s/r0Ta4dk8o9Aaj1AJ5x6O-Q)
- [ã€DeepSpeed æ•™ç¨‹ç¿»è¯‘ã€‘äºŒï¼ŒMegatron-LM GPT2ï¼ŒZero å’Œ ZeRO-Offload](https://mp.weixin.qq.com/s/UO7bLghblw-uoErSnnQyEQ)
- [ã€DeepSpeed æ•™ç¨‹ç¿»è¯‘ã€‘å¼€å§‹ï¼Œå®‰è£…ç»†èŠ‚å’ŒCIFAR-10 Tutorial](https://mp.weixin.qq.com/s/xpNQtl7hPs3fy9S7VRbIkg)
# 0x0. å‰è¨€
åœ¨  [DeepSpeed-Chat æ‰“é€ ç±»ChatGPTå…¨æµç¨‹ ç¬”è®°ä¸€](https://mp.weixin.qq.com/s/QQC5ZxlyqqnK52TthqZdjw) ä¸­è·‘é€šäº†DeepSpeed Chatçš„è®­ç»ƒå’Œæ¨ç†æµç¨‹ï¼ŒDeepSpeed Chatçš„è®­ç»ƒæµç¨‹åŒ…å«ç›‘ç£æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒRewardæ¨¡å‹å¾®è°ƒï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸‰ä¸ªæ­¥éª¤ã€‚æ¥ç€ä¸Šé¢æ–‡ç« çš„todoï¼Œè¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯è§£æä¸€ä¸‹ç›‘ç£æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µçš„ä»£ç å®ç°ã€‚

# 0x1. ğŸ• Supervised finetuning (SFT) æ•™ç¨‹ç¿»è¯‘
ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼ŒWikiText-103ï¼‰ä¸Šçš„æ ‡å‡†è¯­è¨€æ¨¡å‹å¾®è°ƒéå¸¸ç›¸ä¼¼ã€‚ä¸»è¦çš„åŒºåˆ«æ¥è‡ªäºæ•°æ®é›†èµ„æºï¼ŒSFTå°†æ”¶é›†é«˜è´¨é‡çš„æŸ¥è¯¢-å›ç­”å¯¹æ¥å¾®è°ƒæ¨¡å‹ä»¥è¾¾åˆ°äººç±»æ›´å€¾å‘çš„ç”Ÿæˆç»“æœã€‚

## ğŸƒ å¦‚ä½•è®­ç»ƒæ¨¡å‹
æˆ‘ä»¬æä¾›äº†å¤šä¸ªè„šæœ¬ç”¨äºåœ¨å•ä¸ªGPUï¼ˆä¾‹å¦‚ï¼Œå•ä¸ªA6000-48Gï¼ŒV100-32Gï¼ŒA100-40Gç­‰ï¼‰ï¼Œå•èŠ‚ç‚¹ï¼ˆä¾‹å¦‚ï¼Œ8/16x V100-32Gï¼Œ8 A100-40G/80Gï¼‰å’Œå¤šèŠ‚ç‚¹è®¾ç½®ï¼ˆä¾‹å¦‚ï¼Œ64x A100-80Gï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›å¯ä»¥åœ¨ `training_scripts` ç›®å½•ä¸­æ‰¾åˆ°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªå•ç‹¬çš„A6000-48Gï¼Œä½ å¯ä»¥ç®€å•åœ°è¿è¡Œå¯¹åº”çš„è„šæœ¬

```bash
 training_scripts/single_gpu/run_1.3b.sh
```

æ¥è®­ç»ƒä¸€ä¸ªOPT-1.3bæ¨¡å‹ã€‚æˆ‘ä»¬çš„å•èŠ‚ç‚¹è„šæœ¬å¾ˆå®¹æ˜“æ‰©å±•åˆ°å¤šèŠ‚ç‚¹ç³»ç»Ÿã€‚

## ğŸƒ å¦‚ä½•å¯¹SFT checkpointè¿›è¡Œè¯„æµ‹?
ä¸€æ—¦ä½ ä½¿ç”¨ä¸Šè¿°ä»£ç å®Œæˆè®­ç»ƒï¼Œä½ å¯ä»¥ç®€å•åœ°æ‰§è¡Œ `bash evaluation_scripts/run_prompt.sh`

å®ƒä¼šè¦æ±‚ç”¨æˆ·æä¾›ä¸¤ä¸ªæ¨¡å‹çš„è·¯å¾„ï¼š(a) åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå³ --model_name_or_path_baseline facebook/opt-1.3bï¼‰å’Œ (b) å¾®è°ƒåçš„æ¨¡å‹ï¼ˆå³ --model_name_or_path_finetune output/check_baseï¼‰ã€‚"prompt_eval.py" åŒ…å«äº†å‡ ä¸ªå¯ä»¥æ ¹æ®ä½ çš„å–œå¥½è¿›è¡Œæ›´æ–°çš„æç¤ºã€‚


## ğŸ’ æ¨¡å‹å’Œæ•°æ®

ç”±äºGPT3æ²¡æœ‰å¼€æºçš„checkpointï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Meta OPTå®¶æ—çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå³facebook/opt-1.3bï¼‰ã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚GPT-Neoï¼ŒBloomç­‰ï¼‰ã€‚è‡³äºæ•°æ®é›†ï¼Œæˆ‘ä»¬ä¹Ÿä½¿ç”¨äº†æ¥è‡ªHuggingfaceæ•°æ®é›†çš„å¼€æºæ•°æ®é›†ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

```powershell
Dahoas/rm-static
Dahoas/full-hh-rlhf
Dahoas/synthetic-instruct-gptj-pairwise
yitingxie/rlhf-reward-datasets
openai/webgpt_comparisons 
stanfordnlp/SHP
```

æ„Ÿè°¢DeepSpeed RLHFçš„æ•°æ®æŠ½è±¡å’ŒèåˆæŠ€æœ¯ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å°†å¤šä¸ªæ•°æ®æºåˆå¹¶ç”¨äºè®­ç»ƒã€‚ç„¶è€Œï¼Œé‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œä¸åŒçš„æ•°æ®é›†å¯èƒ½ä½¿ç”¨ä¸åŒçš„æç¤ºè¯ï¼ˆä¾‹å¦‚ï¼ŒDohas/rm-staticä½¿ç”¨"Human:"è¡¨ç¤ºæŸ¥è¯¢ï¼Œ"Assistant:"è¡¨ç¤ºå›ç­”ï¼‰ã€‚å› æ­¤ï¼Œç”¨æˆ·å¿…é¡»è‡ªè¡Œå¯¹é½è¿™äº›æç¤ºã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸€è‡´ä½¿ç”¨äº†Dohas/rm-staticçš„æ ¼å¼ã€‚é€šè¿‡æˆ‘ä»¬çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æ•´åˆå¤šæ ·åŒ–çš„æ•°æ®é›†å¯ä»¥æé«˜æ¨¡å‹çš„è´¨é‡ã€‚è¯·å‚è€ƒä¸‹ä¸€èŠ‚ä»¥è·å–ä¸åŒæŸ¥è¯¢-ç­”æ¡ˆå¯¹çš„ç¤ºä¾‹ã€‚


## â˜€ï¸æ¥è‡ªOPT-1.3BåŠå…¶SFTå˜ä½“ï¼ˆä½¿ç”¨ä¸åŒå¾®è°ƒæ•°æ®ï¼‰çš„æç¤ºç¤ºä¾‹

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/537aacf6bfb44c9a85f5a024beb679f9.png)
## â˜€ï¸ ä¸€äº›å‚æ•°è§£é‡Šå’Œå¯è®­ç»ƒçš„æœ€å¤§æ¨¡å‹
main.pyæ–‡ä»¶ä¸­ä½¿ç”¨çš„å¤§å¤šæ•°å‚æ•°éƒ½æœ‰æ¸…æ™°çš„è§£é‡Šï¼Œå¦‚æœä½ æœ‰è§£ç å™¨æ¨¡å‹å¾®è°ƒçš„ç»éªŒï¼Œé€šå¸¸å¾ˆå®¹æ˜“ç†è§£ã€‚ç„¶è€Œï¼Œå¦‚æœä½ å¯¹å…¶ä¸­ä»»ä½•ä¸€ä¸ªä¸æ¸…æ¥šï¼Œè¯·ä¸è¦çŠ¹è±«åœ¨GitHubé—®é¢˜ä¸Šå‘æˆ‘ä»¬æ±‚åŠ©ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€äº›å…·ä½“çš„å‚æ•°è§£é‡Šå’Œå®ƒä»¬çš„ä½¿ç”¨æ–¹æ³•ã€‚

|å‚æ•°   |	è§£é‡Š|  æ³¨æ„äº‹é¡¹ |
| -- | -- | --|
| --data_path | ç”¨äºå¾®è°ƒæ¨¡å‹çš„æ•°æ® | ä½ å¯ä»¥æŒ‡å®šå¤šä¸ªæ•°æ®èµ„æºæ¥è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚ï¼šDahoas/rm-static Dahoas/full-hh-rlhf |
| --data_split | 	ä¸ºä¸‰æ­¥è®­ç»ƒåˆ‡åˆ†æ•°æ® | æ ¹æ®InstructGPTï¼Œæˆ‘ä»¬æä¾›äº†åˆ‡åˆ†æ•°æ®é›†çš„èƒ½åŠ›ï¼Œä½¿å¾—æ¯ä¸ªåˆ†åŒºåªåœ¨ä¸€ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨ã€‚è®¾ç½®ä¸º"2,4,4"æ„å‘³ç€æˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨20%ï¼Œ40%ï¼Œ40%çš„æ•°æ®åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ã€‚å¦‚æœä½ åªåšSFTï¼Œæˆ–è€…ä½ å‘ç°åœ¨ä¸åŒæ­¥éª¤ä¸­ä½¿ç”¨é‡å æ•°æ®æ˜¯å¯ä»¥çš„/æœ‰å¸®åŠ©çš„ï¼Œä½ å¯ä»¥å°†å®ƒæ”¹ä¸º"10,0,0"ã€‚|
| --sft_only_data_path | ç”¨äºå¾®è°ƒæ¨¡å‹çš„å•å“åº”æ•°æ® | å¯¹äºåªåœ¨æ­¥éª¤1ä¸­ä½¿ç”¨çš„å•å“åº”æ•°æ®ï¼Œä½ åº”è¯¥å°†å®ƒä»¬ä½œä¸ºè¿™ä¸ªå‚æ•°çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ä¸Šé¢çš„data_pathå‚æ•°ã€‚è¿™ä¸ªå‚æ•°ä¸­çš„æ•°æ®é›†å°†ä¸ä¼šè¢«åˆ‡åˆ†ï¼Œè€Œåªåœ¨æ­¥éª¤1ä¸­å…¨é¢ä½¿ç”¨ã€‚|
| --gradient_checkpoint | 	ä¸ºæ¨¡å‹å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆä¹Ÿç§°ä¸ºæ¿€æ´»æ£€æŸ¥ç‚¹ï¼‰| è¿™å¯ä»¥æ˜¾è‘—é™ä½è®­ç»ƒå†…å­˜æˆæœ¬|
| --offload | DeepSpeedç‰¹å®šåŠŸèƒ½ã€‚å°†æ¨¡å‹å¸è½½åˆ°CPT/NVMEä»¥èŠ‚çœå†…å­˜ | è¿™å¯ä»¥åœ¨å†…å­˜æ¶ˆè€—è¾ƒå°‘çš„æƒ…å†µä¸‹è®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€‚ä½†æ˜¯å®ƒä¼šå‡æ…¢è®­ç»ƒçš„é€Ÿåº¦ã€‚|
| --zero_stage | DeepSpeedç‰¹å®šåŠŸèƒ½ï¼Œé€‚ç”¨äºå¤šGPUç³»ç»Ÿ | è¿™å¯ä»¥å¸®åŠ©å°†æ¨¡å‹/ä¼˜åŒ–å™¨åˆ†å¸ƒåœ¨å¤šä¸ªGPUä¸Šã€‚è¯·å‚è§https://www.deepspeed.ai/tutorials/zero/|
| --lora_dim | 	å½“å®ƒå¤§äº0æ—¶ï¼Œå°†å¯ç”¨LoRA | é€šå¸¸ï¼ŒLoRAéœ€è¦æ›´å¤§çš„å­¦ä¹ ç‡æ‰èƒ½æ›´å¥½åœ°æ”¶æ•› |
| --lora_module_name | å¯ç”¨LoRAæ¨¡å—çš„èŒƒå›´ã€‚| |
| --only_optimize_lora | å†»ç»“æ‰€æœ‰å…¶ä»–å‚æ•°ï¼Œåªä¼˜åŒ–LoRAç›¸å…³å‚æ•° | |
| --gradient_checkpoint,   --lora_dim, only_optimize_lora | å½“å¯ç”¨LoRAå’Œæ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œä¸èƒ½å¯ç”¨åªä¼˜åŒ–LoRA | å¦‚æœå…¨éƒ¨å¯ç”¨ï¼Œå°†å½±å“æ¢¯åº¦æµï¼ˆä¹Ÿå°±æ˜¯ç”±PyTorchæ”¯æŒçš„auto-gradç³»ç»Ÿåç«¯ï¼‰|

å¯¹äºç”¨æˆ·æ¥è¯´ï¼Œä¸€ä¸ªé‡è¦çš„è€ƒè™‘æ˜¯ç¡®å®šä»–ä»¬å¯ä»¥ä½¿ç”¨å½“å‰ç³»ç»Ÿè®­ç»ƒçš„æœ€å¤§æ¨¡å‹å¤§å°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¼°è®¡è¿™ä¸ªé™åˆ¶çš„æ–¹æ³•ã€‚å‡è®¾ä½ ä¸ä½¿ç”¨å¸è½½åŠŸèƒ½ï¼Œå¹¶å¯ç”¨(i)é›¶é˜¶æ®µ3ï¼ˆå¦‚æœä½¿ç”¨å¤šä¸ªGPUï¼‰ï¼Œ(ii)æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œä»¥åŠ(iii)LoRAï¼Œé‚£ä¹ˆä½ å¯ä»¥è®­ç»ƒçš„å¤§è‡´æœ€å¤§æ¨¡å‹å¤§å°ï¼ˆä»¥åäº¿å‚æ•°ä¸ºå•ä½ï¼‰å¯ä»¥ä¼°è®¡ä¸º"æ€»GPUå†…å­˜ï¼ˆGBï¼‰é™¤ä»¥3"ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€å°å•ä¸€çš„A6000-48G GPUï¼Œä½ å¯èƒ½å¯ä»¥è®­ç»ƒæœ€å¤š16åäº¿å‚æ•°çš„æ¨¡å‹ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç²—ç•¥çš„ä¼°è®¡ï¼Œä½ åº”è¯¥è‡ªå·±éªŒè¯ã€‚

## ğŸ‘€  å…¶å®ƒ
ä»InstructGPTçš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä¸ºäº†å¾—åˆ°æ›´å¥½çš„äººç±»åå¥½çš„ç­”æ¡ˆï¼Œè®©æ¨¡å‹è¿‡åº¦æ‹Ÿåˆï¼ˆå³æ›´é•¿çš„è®­ç»ƒå‘¨æœŸï¼‰ã€‚é€šè¿‡æˆ‘ä»¬çš„æ¢ç´¢ï¼Œæˆ‘ä»¬å‘ç°è¿™å¯¹äºè¾ƒå°æ¨¡å‹çš„å¾®è°ƒï¼Œå¦‚OPT-1.3Bï¼Œç‰¹åˆ«æœ‰å¸®åŠ©ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è„šæœ¬ä¸­æä¾›çš„è¶…å‚æ•°å¹¶æ²¡æœ‰ç»è¿‡å¤§é‡çš„è°ƒæ•´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¼“åŠ±ç”¨æˆ·å’Œå®è·µè€…è‡ªå·±æ‰¾åˆ°æœ€ä¼˜çš„é…ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°å…¶ä»–è¯­è¨€ï¼Œå¦‚ä¸­æ–‡å’Œæ—¥è¯­ã€‚ä¸ºäº†è¯æ˜è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨"training_scripts/other_language"ç›®å½•ä¸‹åŒ…å«äº†ä¸¤ä¸ªä¾‹å­ã€‚


# 0x2. è¯„æµ‹è„šæœ¬è§£è¯»
`DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/evaluation_scripts/run_prompt.sh`è¿™ä¸ªè„šæœ¬æ˜¯å¯¹è¾“å…¥çš„æ¨¡å‹è¿›è¡Œè¯„æµ‹ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹å†…å®¹ï¼š

```powershell
# You can provide two models to compare the performance of the baseline and the finetuned model
export CUDA_VISIBLE_DEVICES=0
python prompt_eval.py \
    --model_name_or_path_baseline XXX \
    --model_name_or_path_finetune XXX

```
è¿™è¡¨ç¤ºæˆ‘ä»¬å¯ä»¥è°ƒç”¨prompt_eval.pyæ¥å¯¹baselineæ¨¡å‹å’Œfinetuneæ¨¡å‹è¿›è¡Œå¯¹æ¯”è¯„æµ‹ã€‚æ‰€ä»¥è¯„æµ‹çš„å®ç°å°±æ˜¯è¿™é‡Œçš„`prompt_eval.py`ï¼Œæ¥ä¸‹æ¥å¯¹`prompt_eval.py`åšä¸€ä¸ªè§£è¯»ã€‚

1. é¦–å…ˆæ˜¯å¤´æ–‡ä»¶å¯¼å…¥éƒ¨åˆ†ï¼Œé™¤äº†ä¸€äº›å¸¸è§„çš„éœ€è¦ç”¨åˆ°çš„åŒ…è¿˜ä»DeepSpeed Chatçš„utilsä¸‹é¢å¯¼å…¥äº†2ä¸ªå‡½æ•°ã€‚

```python
# DeepSpeed Team
import argparse
import logging
import torch
import sys
import os

from transformers import (
    AutoModelForCausalLM, )

sys.path.append(
    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
from utils.model.model_utils import create_hf_model # ä»utilsä¸‹é¢å¯¼å…¥è‡ªå®šä¹‰å‡½æ•°
from utils.utils import load_hf_tokenizer

logger = logging.getLogger(__name__)
```

æ¥ä¸‹æ¥è§£æä¸€ä¸‹`create_hf_model`å’Œ`load_hf_tokenizer`ä¸¤ä¸ªå‡½æ•°çš„å®ç°ã€‚é¦–å…ˆæ˜¯`create_hf_model`å‡½æ•°ï¼š

```python
# è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºcreate_hf_modelçš„å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„ä½œç”¨æ˜¯åˆ›å»ºæˆ–åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥å‡½æ•°çš„ä¸»è¦å‚æ•°åŒ…æ‹¬ï¼š
# model_classï¼šæ¨¡å‹çš„ç±»åˆ«ï¼Œä¾‹å¦‚GPT-2ã€BERTç­‰ã€‚
# tokenizerï¼šç”¨äºæ¨¡å‹çš„åˆ†è¯å™¨ã€‚
# ds_config: DeepSpeedçš„é…ç½®å‚æ•°ã€‚
# rlhf_trainingï¼šä¸€ä¸ªæ ‡å¿—ï¼Œç”¨æ¥è¡¨ç¤ºæ˜¯å¦æ­£åœ¨è¿›è¡ŒRLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰è®­ç»ƒã€‚
# disable_dropoutï¼šä¸€ä¸ªæ ‡å¿—ï¼Œç”¨æ¥è¡¨ç¤ºæ˜¯å¦ç¦ç”¨dropoutã€‚Dropoutæ˜¯ä¸€ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ã€‚
def create_hf_model(model_class,
                    model_name_or_path,
                    tokenizer,
                    ds_config=None,
                    rlhf_training=False,
                    disable_dropout=False):
    # æ ¹æ®model_name_or_pathä»é¢„è®­ç»ƒæ¨¡å‹è·å–æ¨¡å‹é…ç½®model_configã€‚
    model_config = AutoConfig.from_pretrained(model_name_or_path)
    # å¦‚æœdisable_dropoutä¸ºçœŸï¼Œåˆ™å°†æ¨¡å‹é…ç½®ä¸­çš„dropoutè®¾ä¸º0.0ã€‚
    if disable_dropout:
        model_config.dropout = 0.0
    # Note: dschf is defined in function scope to avoid global effects
    # https://huggingface.co/docs/transformers/main_classes/deepspeed#nontrainer-deepspeed-integration
    # æ ¹æ®ds_configä¸­çš„è®¾ç½®ï¼Œåˆ›å»ºDeepSpeedçš„é…ç½®å¯¹è±¡dschfï¼Œä»¥ä¾¿è¿›è¡ŒDeepSpeedä¼˜åŒ–ã€‚
    if ds_config is not None and ds_config["zero_optimization"]["stage"] == 3:
        dschf = HfDeepSpeedConfig(ds_config)
    else:
        dschf = None
    # æ ¹æ®rlhf_trainingçš„å€¼ï¼Œç¡®å®šæ˜¯ä»é…ç½®ä¸­åˆ›å»ºæ¨¡å‹è¿˜æ˜¯ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½æ¨¡å‹ã€‚å¦‚æœrlhf_trainingä¸ºçœŸï¼Œåˆ™æ ¹æ®æ¨¡å‹é…ç½®åˆ›å»ºæ¨¡å‹ï¼›å¦åˆ™ï¼Œä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æ¨¡å‹ã€‚
    if rlhf_training:
        # the weight loading is handled by create critic model
        model = model_class.from_config(model_config)
    else:
        model = model_class.from_pretrained(
            model_name_or_path,
            from_tf=bool(".ckpt" in model_name_or_path),
            config=model_config)
    # å°†æ¨¡å‹çš„ç»“æŸæ ‡è®°å’Œå¡«å……æ ‡è®°è®¾ä¸ºåˆ†è¯å™¨çš„ç»“æŸæ ‡è®°idã€‚
    model.config.end_token_id = tokenizer.eos_token_id
    model.config.pad_token_id = model.config.eos_token_id
    # è°ƒæ•´æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ï¼Œä½¿å…¶ä¸º8çš„å€æ•°ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯ä¸ºäº†åœ¨æŸäº›ç¡¬ä»¶ï¼ˆå¦‚GPUï¼‰ä¸Šæé«˜æ•ˆç‡ã€‚
    model.resize_token_embeddings(int(
        8 *
        math.ceil(len(tokenizer) / 8.0)))  # make the vocab size multiple of 8

    return model
```

ç„¶åæ˜¯`load_hf_tokenizer`å‡½æ•°ï¼š


```python
# è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºload_hf_tokenizerçš„å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„ä½œç”¨æ˜¯åŠ è½½ä¸€ä¸ªæŒ‡å®šçš„åˆ†è¯å™¨ã€‚è¿™ä¸ªå‡½æ•°çš„å‚æ•°æœ‰ï¼š
# model_name_or_pathï¼šæ¨¡å‹çš„åç§°æˆ–è€…é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
# fast_tokenizerï¼šæ˜¯å¦ä½¿ç”¨Fast Tokenizerï¼Œè¿™æ˜¯ä¸€ç§æ›´å¿«çš„åˆ†è¯å™¨å®ç°ã€‚
def load_hf_tokenizer(model_name_or_path, fast_tokenizer=True): 
    # æ£€æŸ¥model_name_or_pathæ˜¯å¦å­˜åœ¨ã€‚å¦‚æœå­˜åœ¨ï¼Œå¯èƒ½æ„å‘³ç€è¿™æ˜¯ä¸€ä¸ªæœ¬åœ°çš„æ¨¡å‹è·¯å¾„ã€‚
    if os.path.exists(model_name_or_path):
        # Locally tokenizer loading has some issue, so we need to force download
        # å¦‚æœæ˜¯æœ¬åœ°æ¨¡å‹ï¼Œå†æ£€æŸ¥åœ¨è¯¥è·¯å¾„ä¸‹çš„"config.json"æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚å¦‚æœå­˜åœ¨ï¼Œæ‰“å¼€è¯¥æ–‡ä»¶ï¼Œå¹¶ä»æ–‡ä»¶ä¸­è¯»å–"_name_or_path"å­—æ®µï¼Œå°†å…¶èµ‹å€¼ç»™model_nameï¼Œç„¶åé€šè¿‡AutoTokenizer.from_pretrainedå‡½æ•°ï¼Œä½¿ç”¨è¯¥æ¨¡å‹åç§°å’Œfast_tokenizerå‚æ•°ï¼Œä»HuggingFaceæ¨¡å‹åº“ä¸‹è½½å¹¶åŠ è½½åˆ†è¯å™¨ã€‚
        model_json = os.path.join(model_name_or_path, "config.json")
        if os.path.exists(model_json):
            model_json_file = json.load(open(model_json))
            model_name = model_json_file["_name_or_path"]
            tokenizer = AutoTokenizer.from_pretrained(model_name,
                                                      fast_tokenizer=True)
    else:
        # å¦‚æœmodel_name_or_pathä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨AutoTokenizer.from_pretrainedå‡½æ•°ï¼Œä½¿ç”¨model_name_or_pathå’Œfast_tokenizerå‚æ•°ï¼Œä»HuggingFaceæ¨¡å‹åº“ä¸‹è½½å¹¶åŠ è½½åˆ†è¯å™¨ã€‚
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,
                                                  fast_tokenizer=True)
    return tokenizer
```

2. æ¥ä¸‹æ¥æ˜¯å‚æ•°è§£æéƒ¨åˆ†ï¼Œè§£æå¦‚ä¸‹ï¼š

```python
# è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºparse_argsçš„å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„ä½œç”¨æ˜¯è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚å®ƒä½¿ç”¨Pythonçš„argparseåº“æ¥å®Œæˆè¿™ä¸ªå·¥ä½œã€‚ä¸‹é¢æ˜¯æ¯ä¸ªå‚æ•°çš„è¯¦è§£ï¼š
def parse_args():
    parser = argparse.ArgumentParser(description="Eval the finetued SFT model")
    # model_name_or_path_baselineï¼šåŸºçº¿æ¨¡å‹çš„è·¯å¾„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¿…é¡»æä¾›çš„å‚æ•°ï¼ˆrequired=Trueï¼‰ã€‚
    parser.add_argument(
        "--model_name_or_path_baseline",
        type=str,
        help="Path to baseline model",
        required=True,
    )
    # model_name_or_path_finetuneï¼šå¾®è°ƒåæ¨¡å‹çš„è·¯å¾„ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªå¿…é¡»æä¾›çš„å‚æ•°ã€‚
    parser.add_argument(
        "--model_name_or_path_finetune",
        type=str,
        help="Path to pretrained model",
        required=True,
    )
    # num_beamsï¼šç”¨äºæŒ‡å®šé›†æŸæœç´¢çš„é›†æŸå®½åº¦ï¼Œå…¶é»˜è®¤å€¼ä¸º1ã€‚
    parser.add_argument(
        "--num_beams",
        type=int,
        default=1,
        help='Specify num of beams',
    )
    # num_beam_groupsï¼šç”¨äºæŒ‡å®šé›†æŸæœç´¢çš„ç»„æ•°ï¼Œå…¶é»˜è®¤å€¼ä¸º1ã€‚
    parser.add_argument(
        "--num_beam_groups",
        type=int,
        default=1,
        help='Specify num of beams',
    )
    # top_kï¼šç”¨äºæŒ‡å®šåœ¨Top-Ké‡‡æ ·ä¸­è€ƒè™‘çš„æœ€é«˜å¯èƒ½æ€§è¯æ±‡çš„æ•°é‡ï¼Œå…¶é»˜è®¤å€¼ä¸º4ã€‚
    parser.add_argument(
        "--top_k",
        type=int,
        default=4,
        help='Specify num of beams',
    )
    # penalty_alphaï¼šæƒ©ç½šå› å­ï¼Œå…¶é»˜è®¤å€¼ä¸º0.6ã€‚
    parser.add_argument(
        "--penalty_alpha",
        type=float,
        default=0.6,
        help='Specify num of beams',
    )
    # num_return_sequencesï¼šç”Ÿæˆåºåˆ—çš„æ•°é‡ï¼Œå…¶é»˜è®¤å€¼ä¸º1ã€‚
    parser.add_argument(
        "--num_return_sequences",
        type=int,
        default=1,
        help='Specify num of return sequences',
    )
    # max_new_tokensï¼šç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°ï¼Œå…¶é»˜è®¤å€¼ä¸º100ã€‚
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=100,
        help='Specify num of return sequences',
    )
    # languageï¼šè¯­è¨€ç±»å‹ï¼Œå¯ä»¥æ˜¯"English"ã€"Chinese"æˆ–"Japanese"ï¼Œé»˜è®¤ä¸º"English"ã€‚
    parser.add_argument("--language",
                        type=str,
                        default="English",
                        choices=["English", "Chinese", "Japanese"])
		# parser.parse_args()è¿™ä¸ªå‡½æ•°å°†è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶å°†ç»“æœä¿å­˜åœ¨ä¸€ä¸ªNamespaceå¯¹è±¡ä¸­ã€‚è¿™ä¸ªå¯¹è±¡è¢«è¿”å›ï¼Œå¯ä»¥åœ¨å…¶ä»–åœ°æ–¹ä½¿ç”¨è¿™äº›å‚æ•°ã€‚
    args = parser.parse_args()

    return args

```
3. æ¥ä¸‹æ¥æ˜¯`generate`å‡½æ•°çš„è§£æï¼š

```python
# è¿™ä¸ªå‡½æ•°æ˜¯ç”¨æ¥åˆ©ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„ï¼Œå®ƒæ¥å—ä»¥ä¸‹å‚æ•°
# modelï¼šå·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
# tokenizerï¼šç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„è¾“å…¥çš„å·¥å…·ã€‚
# inputsï¼šæ¨¡å‹çš„è¾“å…¥æ•°æ®ã€‚
# num_beamsï¼šåœ¨ä½¿ç”¨æŸæœç´¢ç®—æ³•æ—¶çš„æŸå®½ï¼Œå…¶é»˜è®¤å€¼ä¸º1ã€‚
# num_beam_groupsï¼šåœ¨ä½¿ç”¨åˆ†ç»„æŸæœç´¢æ—¶çš„ç»„æ•°ï¼Œé»˜è®¤ä¸º1ã€‚
# do_sampleï¼šæ˜¯å¦è¿›è¡Œéšæœºé‡‡æ ·ã€‚å¦‚æœè®¾ä¸ºTrueï¼Œåˆ™åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼šéšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªå•è¯ï¼Œè€Œä¸æ˜¯ä»…é€‰æ‹©æœ€å¯èƒ½çš„å•è¯ã€‚é»˜è®¤ä¸ºFalseã€‚
# num_return_sequencesï¼šæ¨¡å‹è¿”å›çš„åºåˆ—æ•°ï¼Œé»˜è®¤ä¸º1ã€‚
# max_new_tokensï¼šæ¨¡å‹ç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°ï¼Œå³æœ€å¤§ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ï¼Œé»˜è®¤ä¸º100ã€‚
def generate(model,
             tokenizer,
             inputs,
             num_beams=1,
             num_beam_groups=1,
             do_sample=False,
             num_return_sequences=1,
             max_new_tokens=100):
		# å‡½æ•°é¦–å…ˆä½¿ç”¨æ¨¡å‹çš„generateæ–¹æ³•ï¼Œæ ¹æ®æä¾›çš„å‚æ•°ç”Ÿæˆæ–‡æœ¬ã€‚
    generate_ids = model.generate(inputs.input_ids,
                                  num_beams=num_beams,
                                  num_beam_groups=num_beam_groups,
                                  do_sample=do_sample,
                                  num_return_sequences=num_return_sequences,
                                  max_new_tokens=max_new_tokens)
		# ä½¿ç”¨tokenizerçš„batch_decodeæ–¹æ³•å°†ç”Ÿæˆçš„ä»¤ç‰ŒIDè§£ç ä¸ºå¯è¯»çš„æ–‡æœ¬ã€‚æ³¨æ„ï¼Œè¿™é‡Œè·³è¿‡äº†ç‰¹æ®Šçš„ä»¤ç‰Œï¼ˆå¦‚å¡«å……å’Œå¼€å§‹/ç»“æŸä»¤ç‰Œï¼‰ï¼Œå¹¶ä¸”ä¸ä¼šæ¸…ç†tokenizeäº§ç”Ÿçš„é¢å¤–ç©ºæ ¼ã€‚
    result = tokenizer.batch_decode(generate_ids,
                                    skip_special_tokens=True,
                                    clean_up_tokenization_spaces=False)
    return result
```


4. æ¥ä¸‹æ¥æ˜¯`generate_constrastive_search`å‡½æ•°çš„è§£æï¼š

```python

# è¿™ä¸ªå‡½æ•°å«åš generate_constrastive_searchï¼Œå®ƒæ˜¯ç”¨äºåˆ©ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”æœç´¢ç”Ÿæˆæ–‡æœ¬çš„ã€‚è¿™ä¸ªå‡½æ•°æ¥å—ä»¥ä¸‹å‚æ•°ï¼š
# modelï¼šå·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
# tokenizerï¼šç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„è¾“å…¥çš„å·¥å…·ã€‚
# inputsï¼šæ¨¡å‹çš„è¾“å…¥æ•°æ®ã€‚
# top_kï¼šåœ¨æ¯ä¸€æ­¥ç”Ÿæˆæ—¶ï¼Œåªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„top_kä¸ªå€™é€‰é¡¹ï¼Œç„¶åè¿›è¡ŒéšæœºæŠ½æ ·ã€‚é»˜è®¤ä¸º4ã€‚
# penalty_alphaï¼šç”¨äºæƒ©ç½šæ–°ç”Ÿæˆçš„tokenä¸åŸå§‹è¾“å…¥ä¹‹é—´çš„å·®å¼‚ï¼Œé»˜è®¤ä¸º0.6ã€‚
# num_return_sequencesï¼šæ¨¡å‹è¿”å›çš„åºåˆ—æ•°ï¼Œé»˜è®¤ä¸º1ã€‚
# max_new_tokensï¼šæ¨¡å‹ç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°ï¼Œå³æœ€å¤§ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ï¼Œé»˜è®¤ä¸º100ã€‚
def generate_constrastive_search(model,
                                 tokenizer,
                                 inputs,
                                 top_k=4,
                                 penalty_alpha=0.6,
                                 num_return_sequences=1,
                                 max_new_tokens=100):
		# å‡½æ•°é¦–å…ˆä½¿ç”¨æ¨¡å‹çš„generateæ–¹æ³•ï¼Œæ ¹æ®æä¾›çš„å‚æ•°ç”Ÿæˆæ–‡æœ¬ã€‚æ³¨æ„è¿™é‡Œä½¿ç”¨äº†æ¨¡å‹çš„ä¸€ä¸ªç‰¹æ®Šçš„ç”Ÿæˆæ–¹å¼ï¼Œè¿™ç§æ–¹å¼åœ¨æ¯ä¸€æ­¥ç”Ÿæˆæ—¶ï¼Œåªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„top_kä¸ªå€™é€‰é¡¹ï¼Œç„¶åè¿›è¡ŒéšæœºæŠ½æ ·ï¼ŒåŒæ—¶ä½¿ç”¨äº†ä¸€ä¸ªæƒ©ç½šå› å­penalty_alphaæ¥æƒ©ç½šæ–°ç”Ÿæˆçš„tokenä¸åŸå§‹è¾“å…¥ä¹‹é—´çš„å·®å¼‚ã€‚
    generate_ids = model.generate(inputs.input_ids,
                                  top_k=top_k,
                                  penalty_alpha=penalty_alpha,
                                  num_return_sequences=num_return_sequences,
                                  max_new_tokens=max_new_tokens)
		# ç„¶åï¼Œä½¿ç”¨tokenizerçš„batch_decodeæ–¹æ³•å°†ç”Ÿæˆçš„token IDè§£ç ä¸ºå¯è¯»çš„æ–‡æœ¬ã€‚æ³¨æ„ï¼Œè¿™é‡Œè·³è¿‡äº†ç‰¹æ®Šçš„tokenï¼ˆå¦‚å¡«å……å’Œå¼€å§‹/ç»“æŸtokenï¼‰ï¼Œå¹¶ä¸”ä¸ä¼šæ¸…ç†tokenåŒ–äº§ç”Ÿçš„é¢å¤–ç©ºæ ¼ã€‚
    result = tokenizer.batch_decode(generate_ids,
                                    skip_special_tokens=True,
                                    clean_up_tokenization_spaces=False)
    return result
```

5. æ¥ä¸‹æ¥æ˜¯ä¸€ä¸ªç®€å•çš„æ‰“å°å·¥å…·å‡½æ•°ï¼š

```python
# gen_outputï¼šè¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«äº†æˆ‘ä»¬å¸Œæœ›æ‰“å°çš„å†…å®¹ï¼Œæ¯ä¸€é¡¹éƒ½æ˜¯ä¸€æ®µæ–‡æœ¬ã€‚
def print_utils(gen_output):
    # å‡½æ•°ä¼šéå†gen_outputåˆ—è¡¨ä¸­çš„æ¯ä¸€é¡¹ï¼Œç„¶åå°†æ¯ä¸€é¡¹éƒ½æ‰“å°å‡ºæ¥ã€‚ä¸ºäº†åœ¨ä¸åŒé¡¹ä¹‹é—´å¢åŠ ä¸€äº›å¯è§†åŒ–çš„åˆ†éš”ï¼Œå‡½æ•°åœ¨æ¯ä¸€é¡¹å‰åéƒ½é¢å¤–æ‰“å°äº†ä¸€ä¸ªç©ºè¡Œã€‚
    for i in range(len(gen_output)):
        print()
        print(gen_output[i])
        print()
```

6. ç„¶åæ˜¯`prompt_eval`è¿™ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°prompt_evalçš„ç›®çš„æ˜¯è¯„ä¼°å’Œæ¯”è¾ƒåŸºçº¿æ¨¡å‹ï¼ˆmodel_baselineï¼‰å’Œå¾®è°ƒè¿‡çš„æ¨¡å‹ï¼ˆmodel_fintunedï¼‰å¯¹äºä¸€ç»„æç¤ºï¼ˆpromptsï¼‰çš„ç”Ÿæˆæ€§èƒ½ã€‚è®©æˆ‘ä»¬é€è¡Œè¿›è¡Œè§£æï¼š

```python
# è¾“å…¥å‚æ•°åŒ…æ‹¬ï¼šargsï¼ˆå‘½ä»¤è¡Œå‚æ•°ï¼‰ã€model_baselineï¼ˆåŸºçº¿æ¨¡å‹ï¼‰ã€model_fintunedï¼ˆå¾®è°ƒæ¨¡å‹ï¼‰ã€tokenizerï¼ˆç”¨äºç¼–ç å’Œè§£ç çš„åˆ†è¯å™¨ï¼‰ã€deviceï¼ˆæŒ‡å®šè¿è¡Œæ¨¡å‹çš„è®¾å¤‡ï¼‰ã€promptsï¼ˆä¸€ç»„è¦è¯„ä¼°çš„æç¤ºï¼‰ã€‚
def prompt_eval(args, model_baseline, model_fintuned, tokenizer, device,
                prompts):
    # å¯¹äºpromptsä¸­çš„æ¯ä¸€ä¸ªæç¤ºï¼Œæˆ‘ä»¬éƒ½åšä»¥ä¸‹æ“ä½œï¼š
    for prompt in prompts:
        # ä½¿ç”¨åˆ†è¯å™¨å°†æç¤ºè½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„è¾“å…¥æ ¼å¼ï¼Œå¹¶å°†å…¶ç§»è‡³æŒ‡å®šçš„è®¾å¤‡ä¸Šã€‚
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        # æ‰“å°ä¸€æ¡æ¶ˆæ¯è¡¨ç¤ºæˆ‘ä»¬ç°åœ¨æ­£åœ¨è¿›è¡ŒåŸºçº¿æ¨¡å‹çš„ç”Ÿæˆã€‚
        print("==========Baseline: Greedy=========")
        # ç„¶åï¼Œæˆ‘ä»¬è°ƒç”¨ä¹‹å‰å®šä¹‰çš„generateå‡½æ•°ä½¿ç”¨è´ªå©ªæœç´¢æ–¹æ³•ç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨print_utilså‡½æ•°æ‰“å°ç”Ÿæˆçš„ç»“æœã€‚
        r_base = generate(model_baseline,
                          tokenizer,
                          inputs,
                          num_beams=1,
                          num_return_sequences=args.num_return_sequences,
                          max_new_tokens=args.max_new_tokens)
        print_utils(r_base)
        # æ‰“å°ä¸€æ¡æ¶ˆæ¯è¡¨ç¤ºæˆ‘ä»¬ç°åœ¨æ­£åœ¨è¿›è¡Œå¾®è°ƒæ¨¡å‹çš„ç”Ÿæˆã€‚
        print("==========finetune: Greedy=========")
        # åŒæ ·åœ°ï¼Œæˆ‘ä»¬è°ƒç”¨generateå‡½æ•°ä½¿ç”¨è´ªå©ªæœç´¢æ–¹æ³•ç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨print_utilså‡½æ•°æ‰“å°ç”Ÿæˆçš„ç»“æœã€‚
        r_finetune_g = generate(model_fintuned,
                                tokenizer,
                                inputs,
                                num_beams=1,
                                num_return_sequences=args.num_return_sequences,
                                max_new_tokens=args.max_new_tokens)
        print_utils(r_finetune_g)
        # æ³¨æ„ï¼šåœ¨æ­¤å‡½æ•°ä¸­ï¼Œè´ªå©ªæœç´¢è¢«ç”¨ä½œåŸºçº¿æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¯¥å‡½æ•°è¿˜æä¾›äº†å…¶ä»–å‡ ç§æœç´¢ç­–ç•¥çš„ä¾‹å­ï¼ŒåŒ…æ‹¬å¤šé¡¹å¼é‡‡æ ·ã€æŸæœç´¢ã€æŸæœç´¢å¤šé¡¹å¼é‡‡æ ·ã€å¤šæ ·æ€§æŸæœç´¢å’Œå¯¹æ¯”æœç´¢ã€‚è¿™äº›ç­–ç•¥åœ¨æ­¤å‡½æ•°ä¸­éƒ½è¢«æ³¨é‡Šæ‰äº†ï¼Œä½†ä½ å¯ä»¥æ ¹æ®éœ€è¦å»æ‰æ³¨é‡Šï¼Œä½¿ç”¨è¿™äº›ç­–ç•¥ã€‚

        # print("==========finetune: Multinomial sampling=========")
        # r_finetune_m = generate(model_fintuned, tokenizer, inputs,
        #                         num_beams=1,
        #                         do_sample=True,
        #                         num_return_sequences=args.num_return_sequences,
        #                         max_new_tokens=args.max_new_tokens)
        # print_utils(r_finetune_m)
        # print("==========finetune: Beam Search=========")
        # r_finetune_b = generate(model_fintuned, tokenizer, inputs,
        #                         num_beams=args.num_beams,
        #                         num_return_sequences=args.num_return_sequences,
        #                         max_new_tokens=args.max_new_tokens)
        # print_utils(r_finetune_b)
        # print("==========finetune: Beam-search multinomial sampling=========")
        # r_finetune_s = generate(model_fintuned, tokenizer, inputs,
        #                         num_beams=args.num_beams,
        #                         do_sample=True,
        #                         num_return_sequences=args.num_return_sequences,
        #                         max_new_tokens=args.max_new_tokens)
        # print_utils(r_finetune_s)
        # print("==========finetune: Diverse Beam Search=========")
        # r_finetune_d = generate(model_fintuned, tokenizer, inputs,
        #                         num_beams=args.num_beams,
        #                         num_beam_groups=args.num_beam_groups,
        #                         num_return_sequences=args.num_return_sequences,
        #                         max_new_tokens=args.max_new_tokens)
        # print_utils(r_finetune_d)
        # print("==========finetune: Constrastive Search=========")
        # r_finetune_c = generate_constrastive_search(model_fintuned, tokenizer, inputs,
        #                                             top_k=args.top_k,
        #                                             penalty_alpha=args.penalty_alpha,
        #                                             num_return_sequences=args.num_return_sequences,
        #                                             max_new_tokens=args.max_new_tokens)
        # print_utils(r_finetune_c)
        # æœ€åï¼Œæ‰“å°ä¸€æ¡æ¶ˆæ¯è¡¨ç¤ºè¿™ä¸ªæç¤ºçš„å¤„ç†å·²ç»ç»“æŸã€‚ç„¶åæ‰“å°ä¸¤ä¸ªç©ºè¡Œä½œä¸ºåˆ†éš”ã€‚
        print("====================prompt end=============================")
        print()
        print()
```
7. è§£æmainå‡½æ•°ï¼š

```python
# mainå‡½æ•°è´Ÿè´£è§£æå‘½ä»¤è¡Œå‚æ•°ã€å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨ã€å®šä¹‰æç¤ºï¼Œç„¶åä½¿ç”¨è¿™äº›æ¥è¯„ä¼°å’Œæ¯”è¾ƒåŸºçº¿æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ã€‚
def main():
	  # è¿™ä¸ªmainå‡½æ•°æ˜¯æ•´ä¸ªè„šæœ¬çš„å…¥å£ç‚¹ã€‚å®ƒé¦–å…ˆé€šè¿‡parse_argså‡½æ•°è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚ç„¶åå®ƒè®¾ç½®äº†è¿è¡Œæ¨¡å‹çš„è®¾å¤‡ä¸ºç¬¬ä¸€ä¸ªGPUã€‚
    args = parse_args()

    device = torch.device("cuda:0")
    # æ¥ç€ï¼Œå®ƒä½¿ç”¨load_hf_tokenizerå‡½æ•°åŠ è½½åˆ†è¯å™¨ï¼Œç„¶åä½¿ç”¨create_hf_modelå‡½æ•°åˆ›å»ºåŸºçº¿æ¨¡å‹ï¼ˆmodel_baselineï¼‰å’Œå¾®è°ƒæ¨¡å‹ï¼ˆmodel_fintunedï¼‰
    tokenizer = load_hf_tokenizer(args.model_name_or_path_baseline,
                                  fast_tokenizer=True)

    model_baseline = create_hf_model(AutoModelForCausalLM,
                                     args.model_name_or_path_baseline,
                                     tokenizer, None)
    model_fintuned = create_hf_model(AutoModelForCausalLM,
                                     args.model_name_or_path_finetune,
                                     tokenizer, None)
    # ç„¶åï¼Œè¿™äº›æ¨¡å‹è¢«ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Šã€‚
    model_baseline.to(device)
    model_fintuned.to(device)

    # åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ï¼Œå‡½æ•°å®šä¹‰äº†ä¸€ç»„ç”¨äºè¯„ä¼°çš„æç¤ºã€‚æ³¨æ„ï¼Œè¿™é‡Œç‰¹åˆ«æŒ‡å‡ºï¼Œå¦‚æœæç¤ºä»¥ç©ºæ ¼ç»“æŸï¼Œé‚£ä¹ˆæ²¡æœ‰ç»è¿‡å¾®è°ƒçš„åŸå§‹æ¨¡å‹æœ‰å¯èƒ½ä¼šé™·å…¥åœæ»å¹¶æ— æ³•äº§ç”Ÿå“åº”ã€‚å¾®è°ƒè¿‡çš„æ¨¡å‹åœ¨è¿™æ–¹é¢è¡¨ç°å¾—æ›´å¥½ã€‚å› æ­¤ï¼Œè¿™é‡Œæ‰€æœ‰çš„æç¤ºéƒ½ä»¥å†’å·":"ç»“æŸï¼Œä»¥ä½¿å¾—æ¯”è¾ƒæ›´æœ‰æ„ä¹‰ã€‚
    # è¿™ä¸ªè„šæœ¬æ”¯æŒè‹±æ–‡ã€ä¸­æ–‡å’Œæ—¥æ–‡çš„è¯„ä¼°ï¼Œå®ƒé€šè¿‡args.languageå‚æ•°åˆ¤æ–­ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€ï¼Œå¹¶æ ¹æ®æ­¤é€‰æ‹©åŠ è½½å¯¹åº”çš„æç¤ºã€‚
    if args.language == "English":
        prompts = [
            "Human: Please tell me about Microsoft in a few sentence? Assistant:",
            "Human: Explain the moon landing to a 6 year old in a few sentences. Assistant:",
            "Human: Write a short poem about a wise frog. Assistant:",
            "Human: Who was president of the United States in 1955? Assistant:",
            "Human: How does a telescope work? Assistant:",
            "Human: Why do birds migrate south for the winter? Assistant:"
        ]
    elif args.language == "Chinese":
        prompts = [
            "Human: è¯·ç”¨å‡ å¥è¯ä»‹ç»ä¸€ä¸‹å¾®è½¯? Assistant:",
            "Human: ç”¨å‡ å¥è¯å‘6å²çš„å­©å­è§£é‡Šç™»æœˆã€‚ Assistant:",
            "Human: å†™ä¸€é¦–å…³äºä¸€åªèªæ˜çš„é’è›™çš„çŸ­è¯—ã€‚ Assistant:",
            "Human: è°æ˜¯1955å¹´çš„ç¾å›½æ€»ç»Ÿ? Assistant:", "Human: æœ›è¿œé•œæ˜¯å¦‚ä½•å·¥ä½œçš„? Assistant:",
            "Human: é¸Ÿç±»ä¸ºä»€ä¹ˆè¦å—è¿è¿‡å†¬? Assistant:"
        ]
    elif args.language == "Japanese":
        prompts = [
            "Human: ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆã«ã¤ã„ã¦ç°¡å˜ã«æ•™ãˆã¦ãã ã•ã„ã€‚ Assistant:",
            "Human: 6æ­³å…ã«æœˆé¢ç€é™¸ã‚’çŸ­ã„æ–‡ã§èª¬æ˜ã™ã‚‹ã€‚ Assistant:",
            "Human: è³¢ã„ã‚«ã‚¨ãƒ«ã«ã¤ã„ã¦çŸ­ã„è©©ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚ Assistant:",
            "Human: 1955å¹´ã®ã‚¢ãƒ¡ãƒªã‚«åˆè¡†å›½å¤§çµ±é ˜ã¯èª°? Assistant:",
            "Human: æœ›é é¡ã¯ã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã—ã¾ã™ã‹? Assistant:",
            "Human: é³¥ãŒå†¬ã«å—ã«ç§»å‹•ã™ã‚‹ã®ã¯ãªãœã§ã™ã‹? Assistant:"
        ]
    # æœ€åï¼Œå®ƒè°ƒç”¨prompt_evalå‡½æ•°ï¼Œå°†æ‰€æœ‰çš„å‚æ•°å’Œæç¤ºä¼ é€’ç»™å®ƒï¼Œä»¥è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚
    prompt_eval(args, model_baseline, model_fintuned, tokenizer, device,
                prompts)

```

# 0x3. è®­ç»ƒè„šæœ¬è§£è¯»
è®­ç»ƒè¿‡ç¨‹éƒ½é›†ä¸­åœ¨`DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/main.py`è¿™ä¸ªè„šæœ¬ï¼Œæˆ‘ä»¬é€è¡Œè§£æä¸€ä¸‹è¿™ä¸ªè„šæœ¬ã€‚

## 0x3.1 å¤´æ–‡ä»¶ç›¸å…³è§£æ

```python
# DeepSpeed Team
# é¦–å…ˆï¼Œå®ƒå¯¼å…¥äº†Pythonçš„æ ‡å‡†åº“ï¼Œå¦‚argparseï¼ˆç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°ï¼‰ï¼Œoså’Œmathã€‚
# ç„¶åï¼Œå®ƒå¯¼å…¥äº†PyTorchåº“ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ·±åº¦å­¦ä¹ çš„å¼€æºåº“ï¼ŒåŒæ—¶ä¹Ÿå¯¼å…¥äº†ä¸€äº›PyTorchçš„è¾…åŠ©æ¨¡å—ï¼Œå¦‚DataLoaderï¼ˆç”¨äºåŠ è½½æ•°æ®ï¼‰ã€
# RandomSamplerå’ŒSequentialSamplerï¼ˆç”¨äºæ•°æ®æŠ½æ ·ï¼‰ä»¥åŠDistributedSamplerï¼ˆç”¨äºåœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­è¿›è¡Œæ•°æ®æŠ½æ ·ï¼‰ã€‚
import argparse
import os
import math
import sys

import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from torch.utils.data.distributed import DistributedSampler

# æ¥ä¸‹æ¥ï¼Œå®ƒå¯¼å…¥äº†Hugging Faceçš„transformersåº“çš„ä¸€äº›æ¨¡å—ï¼ŒåŒ…æ‹¬ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„æ¨¡å‹ï¼ˆAutoModelForCausalLMï¼‰ï¼Œä¼˜åŒ–å™¨è°ƒåº¦ç±»å‹ï¼ˆSchedulerTypeï¼‰ï¼Œé»˜è®¤çš„æ•°æ®æ•´ç†å‡½æ•°ï¼ˆdefault_data_collatorï¼‰å’Œè·å–ä¼˜åŒ–å™¨è°ƒåº¦å™¨çš„å‡½æ•°ï¼ˆget_schedulerï¼‰ã€‚
from transformers import (
    AutoModelForCausalLM,
    SchedulerType,
    default_data_collator,
    get_scheduler,
)

# ç„¶åï¼Œå®ƒå¯¼å…¥äº†deepspeedåº“ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¼˜åŒ–çš„åº“ã€‚å®ƒä¹Ÿå¯¼å…¥äº†deepspeedåº“ä¸­çš„ä¸€äº›æ¨¡å—ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ç±»ï¼ˆDeepSpeedCPUAdamå’ŒFusedAdamï¼‰
import deepspeed
from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam

# ä¹‹åï¼Œå®ƒå°†å½“å‰è„šæœ¬çš„çˆ¶ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä»è¯¥ç›®å½•ä¸‹çš„utilsç›®å½•å¯¼å…¥ä¸€äº›è‡ªå®šä¹‰å‡½æ•°å’Œæ¨¡å—ã€‚
sys.path.append(
    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
# æœ€åï¼Œå®ƒä»utilsç›®å½•ä¸­å¯¼å…¥äº†ä¸€äº›è‡ªå®šä¹‰æ¨¡å—å’Œå‡½æ•°ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†å‡½æ•°ï¼ˆcreate_prompt_datasetï¼‰ï¼Œæ‰“å°å’Œè®¾å¤‡è½¬æ¢å‡½æ•°ï¼ˆprint_rank_0å’Œto_deviceï¼‰ï¼Œ
# æ¨¡å‹ä¿å­˜å‡½æ•°ï¼ˆsave_hf_formatï¼‰ï¼Œéšæœºç§å­è®¾ç½®å‡½æ•°ï¼ˆset_random_seedï¼‰ï¼Œæ±‚å¹³å‡å‡½æ•°ï¼ˆget_all_reduce_meanï¼‰ï¼Œ
# è·å–ä¼˜åŒ–å™¨å‚æ•°ç»„çš„å‡½æ•°ï¼ˆget_optimizer_grouped_parametersï¼‰ï¼Œä¿å­˜å’ŒåŠ è½½æ¨¡å‹çš„å‡½æ•°ï¼ˆsave_zero_three_modelå’Œload_hf_tokenizerï¼‰ï¼Œ
# ä»¥åŠåˆ›å»ºæ¨¡å‹å’Œå¤„ç†æ¨¡å‹çš„å‡½æ•°ï¼ˆcreate_hf_modelï¼‰ã€‚è¿™äº›å‡½æ•°åœ¨è„šæœ¬ä¸­çš„åç»­éƒ¨åˆ†éƒ½å°†è¢«ä½¿ç”¨ã€‚
from utils.data.data_utils import create_prompt_dataset
from utils.utils import print_rank_0, to_device, save_hf_format, set_random_seed, get_all_reduce_mean, get_optimizer_grouped_parameters, save_zero_three_model, load_hf_tokenizer
from utils.ds_utils import get_train_ds_config
from utils.module.lora import convert_linear_layer_to_lora, convert_lora_to_linear_layer, only_optimize_lora_parameters
from utils.model.model_utils import create_hf_model
```

### create_prompt_datasetè§£æ
`create_prompt_dataset`è¿™ä¸ªå‡½æ•°å®é™…ä¸Šç›´æ¥æˆ–è€…é—´æ¥çš„ç”¨åˆ°äº†`utils/data`ä¸­`raw_dataset.py`å’Œ`data_utils.py`ï¼Œä¸ºäº†ææ¸…æ¥šè¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¿™ä¸¤ä¸ªæ–‡ä»¶åšä¸€ä¸ªè§£æã€‚

é¦–å…ˆè§£æä¸€ä¸‹`raw_dataset.py`ã€‚è¿™é‡Œå…ˆå®šä¹‰äº†ä¸€ä¸ª`PromptRawDataset`ç±»ï¼š

```python
# DeepSpeed Team
from datasets import load_dataset
from torch.utils.data import Subset
import re


# è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºPromptRawDatasetçš„ç±»ï¼Œè¿™ä¸ªç±»æ˜¯ä¸€ä¸ªæ¨¡æ¿ç±»ï¼Œç”¨äºå¤„ç†å’Œç»„ç»‡æ¨¡å‹è¾“å…¥æ•°æ®çš„æ ¼å¼ã€‚
# å¦‚æœæœ‰æ–°çš„æ•°æ®é›†éœ€è¦è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥ç»§æ‰¿è¿™ä¸ªç±»å¹¶å®ç°ç›¸åº”çš„æ–¹æ³•æ¥ç¡®ä¿æ•°æ®çš„ç»Ÿä¸€æ ¼å¼å’Œæ¥å£ã€‚
class PromptRawDataset(object):
		# é¦–å…ˆï¼Œè¿™ä¸ªç±»çš„æ„é€ å‡½æ•°__init__æ¥æ”¶å››ä¸ªå‚æ•°ï¼šoutput_pathï¼ˆè¾“å‡ºè·¯å¾„ï¼‰ï¼Œseedï¼ˆéšæœºç§å­ï¼‰ï¼Œ
		# local_rankï¼ˆæœ¬åœ°ç­‰çº§ï¼‰å’Œdataset_nameï¼ˆæ•°æ®é›†åç§°ï¼‰ã€‚
		# åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œå¦‚æœæ•°æ®é›†åç§°ä¸æ˜¯'local/jsonfile'ï¼Œ
		# é‚£ä¹ˆä¼šä½¿ç”¨Hugging Faceçš„datasetsåº“çš„load_datasetå‡½æ•°æ¥åŠ è½½æ•°æ®é›†ã€‚


    def __init__(self, output_path, seed, local_rank, dataset_name):
        self.output_path = output_path
        self.seed = seed
        self.local_rank = local_rank
        if not dataset_name == 'local/jsonfile':
            self.raw_datasets = load_dataset(dataset_name)
    # ç„¶åï¼Œè¿™ä¸ªç±»å®šä¹‰äº†ä¸€äº›æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨é»˜è®¤æƒ…å†µä¸‹å¹¶æ²¡æœ‰å®ç°ï¼ˆåªæ˜¯è¿”å›Noneæˆ–è€…ç©ºæ“ä½œï¼‰ï¼Œ
    # è¿™æ˜¯å› ä¸ºè¿™ä¸ªç±»åªæ˜¯ä¸€ä¸ªæ¨¡æ¿ç±»ï¼Œè¿™äº›æ–¹æ³•éœ€è¦åœ¨å®é™…ä½¿ç”¨æ—¶åœ¨å­ç±»ä¸­å…·ä½“å®ç°ã€‚
    def get_train_data(self):     # è·å–è®­ç»ƒæ•°æ®
        return

    def get_eval_data(self):     # è·å–è¯„ä¼°æ•°æ®
        return

    # The prompt should be in the format of: " Human: " + actual_prompt_sentence + " Assistant:"
    # get_promptæ–¹æ³•ç”¨äºè·å–æ ·æœ¬ä¸­çš„promptï¼ˆæç¤ºï¼Œè¿™æ˜¯æ¨¡å‹çš„è¾“å…¥ï¼‰ã€‚
    def get_prompt(self, sample):
        return

    # The chosen response should be in the format of: " " + actual_response_sentence
    # get_chosenæ–¹æ³•ç”¨äºè·å–æ ·æœ¬ä¸­çš„chosenï¼ˆå·²é€‰çš„å›åº”ï¼Œè¿™æ˜¯æ¨¡å‹éœ€è¦ç”Ÿæˆçš„ç›®æ ‡è¾“å‡ºï¼‰ã€‚
    def get_chosen(self, sample):
        return

    # The rejected response should be in the format of: " " + actual_response_sentence
    # If the dataset does not have rejected response, return None
    # get_rejectedæ–¹æ³•ç”¨äºè·å–æ ·æœ¬ä¸­çš„rejectedï¼ˆè¢«æ‹’ç»çš„å›åº”ï¼Œè¿™å¯èƒ½ç”¨äºä¸€äº›ç‰¹å®šçš„è®­ç»ƒåœºæ™¯ï¼Œæ¯”å¦‚åœ¨å¯¹æŠ—è®­ç»ƒä¸­ï¼Œä½†å¦‚æœæ•°æ®é›†ä¸­æ²¡æœ‰è¿™æ ·çš„æ•°æ®ï¼Œå¯ä»¥è¿”å›Noneï¼‰ã€‚
    def get_rejected(self, sample):
        return
    # è·å–æ ·æœ¬ä¸­çš„promptå’Œchosen
    def get_prompt_and_chosen(self, sample):
        return
    # è·å–æ ·æœ¬ä¸­çš„promptå’Œrejected
    def get_prompt_and_rejected(self, sample):
        return
```

æ¥ä¸‹æ¥å°±æ˜¯æ¯ä¸ªå…·ä½“æ•°æ®é›†çš„å®šä¹‰ï¼Œæˆ‘è¿™é‡Œä»¥ OpenaiWebgptcomparisonsDataset ä¸ºä¾‹è§£æä¸€ä¸‹ï¼Œå‰©ä¸‹çš„è¯»è€…åˆéœ€è¦å¯ä»¥è‡ªè¡Œç†è§£ï¼š

```python
# English dataset
# è¿™ä¸ªç±»OpenaiWebgptcomparisonsDatasetç»§æ‰¿è‡ªPromptRawDatasetç±»ï¼Œ
# é’ˆå¯¹"openai/webgpt_comparisons"è¿™ä¸ªå…·ä½“çš„æ•°æ®é›†è¿›è¡Œäº†ç‰¹åŒ–ã€‚
class OpenaiWebgptcomparisonsDataset(PromptRawDataset):
    # åœ¨æ„é€ å‡½æ•°__init__ä¸­ï¼Œè°ƒç”¨äº†çˆ¶ç±»çš„æ„é€ å‡½æ•°ï¼Œå¹¶è®¾å®šäº†dataset_nameå’Œdataset_name_cleanä¸¤ä¸ªå±æ€§ï¼Œ
    # åˆ†åˆ«ä¸º"openai/webgpt_comparisons"å’Œ"openai_webgpt_comparisons"ã€‚
    def __init__(self, output_path, seed, local_rank, dataset_name):
        super().__init__(output_path, seed, local_rank, dataset_name)
        self.dataset_name = "openai/webgpt_comparisons"
        self.dataset_name_clean = "openai_webgpt_comparisons"
    # get_train_dataå’Œget_eval_dataæ–¹æ³•åˆ†åˆ«ä»raw_datasetsä¸­è·å–è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚
    # å®ƒä»¬ä¸ä¹‹å‰çš„DahoasRmstaticDatasetç±»ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒä»¬ä½¿ç”¨get_raw_dataset_split_index
    # æ–¹æ³•å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œäº†åˆ’åˆ†ï¼Œå°†å…¶åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¹¶è¿”å›å¯¹åº”çš„æ•°æ®å­é›†ã€‚
    def get_train_data(self):
        from .data_utils import get_raw_dataset_split_index
        dataset = self.raw_datasets["train"]
        index = get_raw_dataset_split_index(self.local_rank, self.output_path,
                                            self.dataset_name_clean,
                                            self.seed, "train_eval", "9,1", 0,
                                            len(dataset))
        dataset = Subset(dataset, index)
        return dataset

    def get_eval_data(self):
        from .data_utils import get_raw_dataset_split_index
        dataset = self.raw_datasets["train"]
        index = get_raw_dataset_split_index(self.local_rank, self.output_path,
                                            self.dataset_name_clean,
                                            self.seed, "train_eval", "9,1", 1,
                                            len(dataset))
        dataset = Subset(dataset, index)
        return dataset
     # get_promptï¼Œget_chosenå’Œget_rejectedæ–¹æ³•åˆ†åˆ«ä»æ ·æœ¬ä¸­è·å–æç¤ºï¼Œå·²é€‰å›åº”å’Œè¢«æ‹’ç»çš„å›åº”ã€‚
     # è¿™é‡Œå‡å®šæ ·æœ¬æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«äº†åä¸º'question'ï¼Œ'score_0'ï¼Œ'score_1'ï¼Œ'answer_0'å’Œ'answer_1'çš„å­—æ®µã€‚
		# å…¶ä¸­ï¼Œ'question'å­—æ®µæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«äº†'full_text'å­—æ®µã€‚è¿™ä¸ªå­—æ®µåŒ…å«äº†äººç±»æå‡ºçš„é—®é¢˜ã€‚
		# 'score_0'å’Œ'score_1'å­—æ®µæ˜¯å­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºå¯¹'answer_0'å’Œ'answer_1'çš„è¯„åˆ†ã€‚
		# å¦‚æœ'score_0'å¤§äºç­‰äº'score_1'ï¼Œé‚£ä¹ˆ'answer_0'å°±æ˜¯å·²é€‰å›åº”ï¼Œ'answer_1'å°±æ˜¯è¢«æ‹’ç»çš„å›åº”ï¼Œåä¹‹äº¦ç„¶ã€‚
		# åœ¨è·å–å·²é€‰å›åº”å’Œè¢«æ‹’ç»çš„å›åº”æ—¶ï¼Œè¿˜å¯¹å›åº”è¿›è¡Œäº†å¤„ç†ï¼Œ
		# å»é™¤äº†æ‰€æœ‰å½¢å¦‚"[...]"æˆ–"(...)"çš„æ–‡æœ¬ï¼Œç„¶ååœ¨å›åº”å‰æ·»åŠ äº†ä¸€ä¸ªç©ºæ ¼ã€‚
    def get_prompt(self, sample):
        return " Human: " + sample['question']['full_text'] + " Assistant:"

    def get_chosen(self, sample):
        if float(sample['score_0']) >= float(sample['score_1']):
            response = sample['answer_0']
        else:
            response = sample['answer_1']
        # This data has citation square brackets and numbers (e.g., "[1]").
        # Right now we are not doing browser-assisted finetuning, thus we
        # remove these citations to avoid confusing the model.
        response = re.sub(r" [\(\[].*?[\)\]]", "", response)
        response = re.sub(r"[\(\[].*?[\)\]]", "", response)
        return " " + response

    def get_rejected(self, sample):
        if float(sample['score_0']) < float(sample['score_1']):
            response = sample['answer_0']
        else:
            response = sample['answer_1']
        response = re.sub(r" [\(\[].*?[\)\]]", "", response)
        response = re.sub(r"[\(\[].*?[\)\]]", "", response)
        return " " + response
    # get_prompt_and_chosenå’Œget_prompt_and_rejectedæ–¹æ³•åˆ™åˆ†åˆ«è¿”å›æ ·æœ¬ä¸­çš„'prompt'å’Œ'chosen'ï¼Œ
    # ä»¥åŠ'prompt'å’Œ'rejected'çš„ç»„åˆã€‚è¿™ä¸¤ä¸ªæ–¹æ³•çš„è¿”å›å€¼å¯ä»¥ç›´æ¥ä½œä¸ºæ¨¡å‹çš„è¾“å…¥å’Œç›®æ ‡è¾“å‡ºã€‚
    # åœ¨è¿”å›è¿™ä¸¤ä¸ªç»„åˆæ—¶ï¼Œä¹Ÿè¿›è¡Œäº†ç±»ä¼¼çš„å¤„ç†ï¼Œå»é™¤äº†æ‰€æœ‰å½¢å¦‚"[...]"æˆ–"(...)"çš„æ–‡æœ¬ã€‚
    def get_prompt_and_chosen(self, sample):
        if float(sample['score_0']) >= float(sample['score_1']):
            response = sample['answer_0']
        else:
            response = sample['answer_1']
        response = re.sub(r" [\(\[].*?[\)\]]", "", response)
        response = re.sub(r"[\(\[].*?[\)\]]", "", response)
        return " Human: " + sample['question'][
            'full_text'] + " Assistant: " + response

    def get_prompt_and_rejected(self, sample):
        if float(sample['score_0']) < float(sample['score_1']):
            response = sample['answer_0']
        else:
            response = sample['answer_1']
        response = re.sub(r" [\(\[].*?[\)\]]", "", response)
        response = re.sub(r"[\(\[].*?[\)\]]", "", response)
        return " Human: " + sample['question'][
            'full_text'] + " Assistant: " + response

```

æ¥ç€è§£æ `data_utils.py`ï¼š

```python
# DeepSpeed Team
"""
Part of the code was adopted from https://github.com/microsoft/Megatron-DeepSpeed/blob/main/megatron/data/dataset_utils.py
"""
import torch
from torch.utils.data import Dataset, Subset, ConcatDataset
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from datasets import load_dataset
import numpy as np
import os
import hashlib # Pythonçš„å†…ç½®åº“ï¼Œæä¾›äº†ä¸€ç³»åˆ—æ•£åˆ—å‡½æ•°ï¼Œå¦‚MD5ã€SHA1ç­‰ã€‚
from itertools import chain # Pythonçš„å†…ç½®åº“ï¼Œæä¾›äº†ä¸€ç³»åˆ—ç”¨äºæ“ä½œè¿­ä»£å™¨çš„å‡½æ•°ã€‚
from . import raw_datasets

# è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºget_raw_datasetçš„å‡½æ•°ï¼Œå…¶ä¸»è¦ä½œç”¨æ˜¯æ ¹æ®ä¼ å…¥çš„æ•°æ®é›†åç§°dataset_name
# è¿”å›ä¸€ä¸ªé€‚å½“çš„PromptRawDatasetå­ç±»çš„å®ä¾‹ã€‚
# è¿™ä¸ªå‡½æ•°æœ‰å››ä¸ªå‚æ•°ï¼šdataset_nameï¼Œoutput_pathï¼Œseedå’Œlocal_rankã€‚
def get_raw_dataset(dataset_name, output_path, seed, local_rank):
    # åœ¨å‡½æ•°ä¸­ï¼Œæ ¹æ®dataset_nameçš„å…·ä½“å€¼æ¥åˆ›å»ºä¸åŒçš„PromptRawDatasetå­ç±»çš„å®ä¾‹ã€‚
    # ä¾‹å¦‚ï¼Œå¦‚æœdataset_nameä¸º"Dahoas/rm-static"ï¼Œé‚£ä¹ˆå°±åˆ›å»ºä¸€ä¸ªDahoasRmstaticDatasetçš„å®ä¾‹ï¼›
    # å¦‚æœdataset_nameä¸º"Dahoas/full-hh-rlhf"ï¼Œé‚£ä¹ˆå°±åˆ›å»ºä¸€ä¸ªDahoasFullhhrlhfDatasetçš„å®ä¾‹ï¼Œä»¥æ­¤ç±»æ¨ã€‚
    if "Dahoas/rm-static" in dataset_name:
        return raw_datasets.DahoasRmstaticDataset(output_path, seed,
                                                  local_rank, dataset_name)
    elif "Dahoas/full-hh-rlhf" in dataset_name:
        return raw_datasets.DahoasFullhhrlhfDataset(output_path, seed,
                                                    local_rank, dataset_name)
    elif "Dahoas/synthetic-instruct-gptj-pairwise" in dataset_name:
        return raw_datasets.DahoasSyntheticinstructgptjpairwiseDataset(
            output_path, seed, local_rank, dataset_name)
    elif "yitingxie/rlhf-reward-datasets" in dataset_name:
        return raw_datasets.YitingxieRlhfrewarddatasetsDataset(
            output_path, seed, local_rank, dataset_name)
    elif "openai/webgpt_comparisons" in dataset_name:
        return raw_datasets.OpenaiWebgptcomparisonsDataset(
            output_path, seed, local_rank, dataset_name)
    elif "stanfordnlp/SHP" in dataset_name:
        return raw_datasets.StanfordnlpSHPDataset(output_path, seed,
                                                  local_rank, dataset_name)
    elif "pvduy/sharegpt_alpaca_oa_vicuna_format" in dataset_name:
        return raw_datasets.PvduySharegptalpacaoavicunaformatDataset(
            output_path, seed, local_rank, dataset_name)
    elif "wangrui6/Zhihu-KOL" in dataset_name:
        return raw_datasets.Wangrui6ZhihuKOLDataset(output_path, seed,
                                                    local_rank, dataset_name)
    elif "Cohere/miracl-zh-queries-22-12" in dataset_name:
        return raw_datasets.CohereMiraclzhqueries2212Dataset(
            output_path, seed, local_rank, dataset_name)
    elif "Hello-SimpleAI/HC3-Chinese" in dataset_name:
        return raw_datasets.HelloSimpleAIHC3ChineseDataset(
            output_path, seed, local_rank, dataset_name)
    elif "mkqa-Chinese" in dataset_name:
        return raw_datasets.MkqaChineseDataset(output_path, seed, local_rank,
                                               "mkqa")
    elif "mkqa-Japanese" in dataset_name:
        return raw_datasets.MkqaJapaneseDataset(output_path, seed, local_rank,
                                                "mkqa")
    elif "Cohere/miracl-ja-queries-22-12" in dataset_name:
        return raw_datasets.CohereMiracljaqueries2212Dataset(
            output_path, seed, local_rank, dataset_name)
    elif "lmqg/qg_jaquad" in dataset_name:
        return raw_datasets.LmqgQgjaquadDataset(output_path, seed, local_rank,
                                                dataset_name)
    elif "lmqg/qag_jaquad" in dataset_name:
        return raw_datasets.LmqgQagjaquadDataset(output_path, seed, local_rank,
                                                 dataset_name)
    # å¦‚æœdataset_nameæ˜¯"local/jsonfile"ï¼Œåˆ™ä¼šæ£€æŸ¥åœ¨è·¯å¾„chat_path + '/data/train.json'
    # å’Œchat_path + '/data/eval.json'ä¸‹æ˜¯å¦å­˜åœ¨æ–‡ä»¶ã€‚å¦‚æœå­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªLocalJsonFileDatasetçš„å®ä¾‹ï¼›
    # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡ºä¸€ä¸ªRuntimeErrorå¼‚å¸¸ã€‚
    elif "local/jsonfile" in dataset_name:
        chat_path = os.path.abspath(
            os.path.join(os.path.dirname(__file__), os.path.pardir,
                         os.path.pardir, os.path.pardir))
        if not (os.path.isfile(chat_path + '/data/train.json')
                and os.path.isfile(chat_path + '/data/eval.json')):
            raise RuntimeError(
                f"Please check both the train.json and eval.json files in your applications/DeepSpeed-Chat/data directory."
            )
        return raw_datasets.LocalJsonFileDataset(output_path, seed, local_rank,
                                                 dataset_name, chat_path)
    else:
        # å¦‚æœdataset_nameæ²¡æœ‰åœ¨ä»¥ä¸Šçš„æ‰€æœ‰æ¡ä»¶ä¸­åŒ¹é…åˆ°ï¼Œé‚£ä¹ˆå‡½æ•°ä¹Ÿä¼šæŠ›å‡ºä¸€ä¸ªRuntimeErrorå¼‚å¸¸ï¼Œè¡¨ç¤ºæ²¡æœ‰ä¸ºè¿™ä¸ªæ•°æ®é›†çš„é…ç½®ã€‚
        raise RuntimeError(
            f"We do not have configs for dataset {dataset_name}, but you can add it by yourself in raw_datasets.py."
        )
```

å†çœ‹ä¸‹ `get_shuffle_idx`å‡½æ•°ï¼š

```python
# è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯ç”Ÿæˆä¸€ä¸ªå¤§å°ä¸ºsizeçš„ä¹±åºç´¢å¼•æ•°ç»„ï¼Œå®ƒæ¥å—ä¸¤ä¸ªå‚æ•°ï¼šseedå’Œsizeã€‚
def get_shuffle_idx(seed, size):
    np_rng = np.random.RandomState(seed=seed) # åˆ›å»ºä¸€ä¸ªNumPyçš„éšæœºçŠ¶æ€ç”Ÿæˆå™¨å¯¹è±¡np_rngï¼Œseedæ˜¯éšæœºç§å­ï¼Œç¡®å®šäº†éšæœºæ•°çš„ç”Ÿæˆåºåˆ—ã€‚
    dtype_ = np.uint32 # è®¾ç½®å…¶ä¸ºNumPyçš„uint32ç±»å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— ç¬¦å·32ä½æ•´æ•°ç±»å‹ã€‚
    if size >= (np.iinfo(np.uint32).max - 1): # å¦‚æœsizeå¤§äºæˆ–ç­‰äºuint32çš„æœ€å¤§å€¼å‡ä¸€ï¼Œè¿™é‡Œå‡ä¸€æ˜¯ä¸ºäº†é˜²æ­¢å¯èƒ½çš„æº¢å‡ºã€‚
        dtype_ = np.int64 # åˆ™å°†dtype_æ”¹ä¸ºint64ï¼Œè¿™æ˜¯ä¸€ä¸ª64ä½çš„æœ‰ç¬¦å·æ•´æ•°ç±»å‹ã€‚
    shuffle_idx = np.arange(start=0, stop=size, step=1, dtype=dtype_) # åˆ›å»ºä¸€ä¸ªç”±0å¼€å§‹ï¼Œæ­¥é•¿ä¸º1ï¼Œåˆ°sizeç»“æŸï¼ˆä¸åŒ…å«sizeï¼‰ï¼Œå¹¶ä¸”æ•°æ®ç±»å‹ä¸ºdtype_çš„ç­‰å·®æ•°åˆ—ï¼Œå°†å…¶èµ‹å€¼ç»™shuffle_idxã€‚
    np_rng.shuffle(shuffle_idx) # ä½¿ç”¨np_rngéšæœºçŠ¶æ€ç”Ÿæˆå™¨å¯¹shuffle_idxè¿›è¡Œéšæœºæ’åˆ—ï¼Œè¿™æ ·å°±æ‰“ä¹±äº†shuffle_idxçš„é¡ºåºã€‚
    return shuffle_idx # è¿”å›ä¹±åºåçš„shuffle_idxã€‚
```

æ¥ç€è§£æ`get_raw_dataset_split_index`å‡½æ•°ï¼š

```python
# è¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯æ ¹æ®æä¾›çš„å‚æ•°åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªåˆ†å‰²ç´¢å¼•ã€‚
# å®ƒé¦–å…ˆæ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ç”Ÿæˆåˆ†å‰²ç´¢å¼•ï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚
# ç„¶åï¼Œå®ƒä»æ–‡ä»¶ä¸­åŠ è½½ç´¢å¼•ï¼Œå¹¶è¿”å›ç´¢å¼•åˆ—è¡¨ã€‚
# å®ƒæ¥å—8ä¸ªå‚æ•°ï¼šlocal_rankã€è¾“å‡ºè·¯å¾„ã€æ•°æ®é›†åç§°ã€ç§å­ã€split_nameã€data_splitã€split_indexå’Œæ•°æ®å¤§å°ã€‚
def get_raw_dataset_split_index(local_rank, output_path, dataset_name, seed,
                                split_name, data_split, split_index,
                                data_size):
    # æ ¹æ®ç»™å®šå‚æ•°ç”Ÿæˆç´¢å¼•æ–‡ä»¶åã€‚
    index_file_name = f"{output_path}/{dataset_name}_seed{seed}_{split_name}_{data_split}_{split_index}.npy"
    # reindex each time when using local jsonfile since it's more likely to get modified
    # å¦‚æœç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…æ•°æ®é›†åä¸º'jsonfile'ï¼Œåˆ™æ‰§è¡Œä¸‹é¢çš„æ“ä½œã€‚
    if (not os.path.isfile(index_file_name)) or (dataset_name == 'jsonfile'):
        splits = [float(s) for s in data_split.split(',')] # å°†data_splitï¼ˆä»¥é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼‰åˆ†å‰²ä¸ºæµ®ç‚¹æ•°åˆ—è¡¨ã€‚
        splits_sum = sum(splits) # è®¡ç®—æ‰€æœ‰åˆ†å‰²çš„æ€»å’Œã€‚
        splits = [split / splits_sum for split in splits] # å°†æ¯ä¸ªåˆ†å‰²å€¼å½’ä¸€åŒ–ï¼Œä½¿å…¶å’Œä¸º1ã€‚
        splits_index = [0] # åˆå§‹åŒ–åˆ†å‰²ç´¢å¼•åˆ—è¡¨ï¼Œèµ·å§‹å€¼ä¸º0
        for index, split in enumerate(splits): # éå†åˆ†å‰²åˆ—è¡¨ã€‚
            # å°†æ–°çš„ç´¢å¼•ï¼ˆå½“å‰ç´¢å¼•åŠ ä¸Šå½’ä¸€åŒ–çš„åˆ†å‰²å€¼ä¸æ•°æ®å¤§å°çš„ä¹˜ç§¯ï¼‰æ·»åŠ åˆ°ç´¢å¼•åˆ—è¡¨ã€‚
            splits_index.append(splits_index[index] +
                                int(round(split * float(data_size))))
        # è®¡ç®—æœ€åä¸€ä¸ªç´¢å¼•ä¸æ•°æ®å¤§å°çš„å·®å€¼ã€‚
        diff = splits_index[-1] - data_size
        # éå†é™¤ç¬¬ä¸€ä¸ªå¤–çš„æ‰€æœ‰ç´¢å¼•ã€‚
        for index in range(1, len(splits_index)):
            # å°†å·®å€¼ä»æ¯ä¸ªç´¢å¼•ä¸­å‡å»ï¼Œä»¥ç¡®ä¿æœ€åä¸€ä¸ªç´¢å¼•ç­‰äºæ•°æ®å¤§å°ã€‚
            splits_index[index] -= diff
        # æ–­è¨€æœ€åä¸€ä¸ªç´¢å¼•ç­‰äºæ•°æ®å¤§å°
        assert splits_index[-1] == data_size
        
        # ç”Ÿæˆä¸€ä¸ªä¹±åºçš„ç´¢å¼•ã€‚
        shuffle_idx = get_shuffle_idx(seed, data_size)
        for split_i in range(len(splits)): # éå†æ¯ä¸ªåˆ†å‰²ã€‚
            # æ ¹æ®ç»™å®šå‚æ•°ç”Ÿæˆä¹±åºç´¢å¼•åˆ†å‰²æ–‡ä»¶åã€‚
            shuffle_idx_split_file_name = f"{output_path}/{dataset_name}_seed{seed}_{split_name}_{data_split}_{split_i}.npy" 
            # æå–ä¹±åºç´¢å¼•çš„ä¸€ä¸ªåˆ†å‰²ã€‚
            shuffle_idx_split = shuffle_idx[
                splits_index[split_i]:splits_index[split_i + 1]]
            # å°†ä¹±åºç´¢å¼•åˆ†å‰²ä¿å­˜åˆ°æ–‡ä»¶ã€‚
            np.save(shuffle_idx_split_file_name,
                    shuffle_idx_split,
                    allow_pickle=True)
    # åŠ è½½ç´¢å¼•æ–‡ä»¶ã€‚
    index = np.load(index_file_name, allow_pickle=True)
    # å°†ç´¢å¼•æ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨å¹¶è¿”å›ã€‚
    return index.tolist()
```

æ¥ä¸‹æ¥è§£æä¸€ä¸‹ç»§æ‰¿è‡ª`Dataset`çš„`PromptDataset`ç±»ï¼š

```python
# è¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„PromptDatasetç±»ï¼Œå®ƒç»§æ‰¿è‡ªtorch.utils.data.Datasetã€‚
# è¿™æ˜¯ä¸€ä¸ªæ•°æ®é›†ç±»ï¼Œé€šå¸¸è¢«ç”¨äºPyTorchä¸­æ•°æ®çš„åŠ è½½å’Œé¢„å¤„ç†ã€‚
class PromptDataset(Dataset):
    # ç±»çš„æ„é€ å‡½æ•°ï¼Œå®ƒæ¥å—äº”ä¸ªå‚æ•°ï¼šprompt_datasetã€chosen_datasetã€reject_datasetã€pad_token_idå’Œtrain_phaseã€‚
    def __init__(self, prompt_dataset, chosen_dataset, reject_dataset,
                 pad_token_id, train_phase) -> None:
        super().__init__() # è°ƒç”¨çˆ¶ç±»torch.utils.data.Datasetçš„æ„é€ å‡½æ•°ã€‚
        self.prompt_dataset = prompt_dataset # å°†ä¼ å…¥çš„å‚æ•°èµ‹å€¼ç»™ç±»çš„æˆå‘˜å˜é‡ã€‚
        self.chosen_dataset = chosen_dataset
        self.reject_dataset = reject_dataset
        self.pad_token_id = pad_token_id
        self.train_phase = train_phase
   
    def __len__(self): # å®šä¹‰ç±»çš„__len__æ–¹æ³•ï¼Œå®ƒè¿”å›æ•°æ®é›†çš„é•¿åº¦ã€‚è¿™æ˜¯PyTorchæ•°æ®é›†çš„å¿…è¦æ–¹æ³•ã€‚
        length = len(self.chosen_dataset) # åˆå§‹è®¾å®šæ•°æ®é›†é•¿åº¦ä¸ºchosen_datasetçš„é•¿åº¦ã€‚
        if self.train_phase == 3: 
            length = len(self.prompt_dataset) # å¦‚æœè®­ç»ƒé˜¶æ®µä¸º3ï¼Œåˆ™æ•°æ®é›†é•¿åº¦è®¾å®šä¸ºprompt_datasetçš„é•¿åº¦ã€‚
        return length # è¿”å›è®¡ç®—å¾—å‡ºçš„æ•°æ®é›†é•¿åº¦ã€‚
    
    # å®šä¹‰ç±»çš„__getitem__æ–¹æ³•ï¼Œå®ƒæ¥å—ä¸€ä¸ªå‚æ•°idxï¼Œè¿”å›ç´¢å¼•idxå¤„çš„æ•°æ®ã€‚è¿™æ˜¯PyTorchæ•°æ®é›†çš„å¿…è¦æ–¹æ³•ã€‚
    def __getitem__(self, idx): 
        # å¦‚æœè®­ç»ƒé˜¶æ®µä¸º1ï¼Œåˆ™è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«input_idsã€attention_maskå’Œlabelsï¼Œå®ƒä»¬éƒ½æ¥è‡ªchosen_datasetçš„ç´¢å¼•idxå¤„ã€‚
        if self.train_phase == 1:
            return {
                "input_ids": self.chosen_dataset[idx]["input_ids"],
                "attention_mask": self.chosen_dataset[idx]["attention_mask"],
                "labels": self.chosen_dataset[idx]["input_ids"]
            }
        # å¦‚æœè®­ç»ƒé˜¶æ®µä¸º2ï¼Œåˆ™è¿”å›æ¥è‡ªchosen_datasetå’Œreject_datasetçš„input_idså’Œattention_maskã€‚
        elif self.train_phase == 2:
            return self.chosen_dataset[idx]["input_ids"], self.chosen_dataset[idx]["attention_mask"], \
                self.reject_dataset[idx]["input_ids"], self.reject_dataset[idx]["attention_mask"]
        # å¦‚æœè®­ç»ƒé˜¶æ®µä¸º3ï¼Œåˆ™è¿”å›æ¥è‡ªprompt_datasetçš„input_idsã€attention_maskå’Œpad_token_id
        elif self.train_phase == 3:
            return self.prompt_dataset[idx]["input_ids"],self.prompt_dataset[idx]["attention_mask"], \
                self.pad_token_id
```
æ¥ç€å†è§£æä¸€ä¸‹`create_dataset_split`å‡½æ•°ï¼š

```python
# è¿™æ˜¯ä¸€ä¸ªåä¸ºcreate_dataset_splitçš„å‡½æ•°ï¼Œå®ƒçš„åŠŸèƒ½æ˜¯æ ¹æ®ç»™å®šçš„è®­ç»ƒé˜¶æ®µï¼ˆtrain_phaseï¼‰ï¼Œåˆ›å»ºå¹¶è¿”å›ç›¸åº”çš„æ•°æ®é›†åˆ†å‰²ã€‚
# å…·ä½“æ¥è¯´ï¼Œå®ƒä¸ºæ¯ä¸ªè®­ç»ƒé˜¶æ®µç”Ÿæˆä¸åŒçš„æ•°æ®é›†åˆ—è¡¨ï¼Œå¹¶å°†å®ƒä»¬æ”¾å…¥PromptDatasetå¯¹è±¡ä¸­ã€‚
# å‡½æ•°æ¥å—6ä¸ªå‚æ•°ï¼šå½“å‰æ•°æ®é›†(current_dataset)ã€åŸå§‹æ•°æ®é›†(raw_dataset)ã€è®­ç»ƒé˜¶æ®µ(train_phase)ã€
# åˆ†è¯å™¨(tokenizer)ã€ä¼šè¯ç»“æŸæ ‡è®°(end_of_conversation_token)å’Œæœ€å¤§åºåˆ—é•¿åº¦(max_seq_len)ã€‚
def create_dataset_split(current_dataset, raw_dataset, train_phase, tokenizer,
                         end_of_conversation_token, max_seq_len):
    # åˆ›å»ºä¸‰ä¸ªç©ºçš„åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨å¯¹è¯æç¤ºï¼ˆprompt_datasetï¼‰ã€é€‰å®šçš„å¯¹è¯ï¼ˆchosen_datasetï¼‰å’Œè¢«æ‹’ç»çš„å¯¹è¯ï¼ˆreject_datasetï¼‰ã€‚
    prompt_dataset = []
    chosen_dataset = []
    reject_dataset = []
    # å¦‚æœè®­ç»ƒé˜¶æ®µä¸º1ï¼Œåˆ™å°†æ¥å—çš„å¯¹è¯è¿›è¡Œåˆ†è¯å¹¶æ·»åŠ åˆ°chosen_datasetä¸­ã€‚
    if train_phase == 1:
        # éå†å½“å‰æ•°æ®é›†ã€‚
        for i, tmp_data in enumerate(current_dataset):
            # tokenize the text
            # ä»åŸå§‹æ•°æ®é›†ä¸­è·å–å¯¹è¯æç¤ºå’Œæ¥å—çš„å¯¹è¯ã€‚
            chosen_sentence = raw_dataset.get_prompt_and_chosen(
                tmp_data)  # the accept response
            # å¦‚æœæ¥å—çš„å¯¹è¯ä¸ä¸ºç©ºï¼Œåˆ™å°†å…¶åˆ†è¯å¹¶æ·»åŠ åˆ°chosen_datasetä¸­ã€‚
            if chosen_sentence is not None:
                chosen_sentence += end_of_conversation_token
                chosen_token = tokenizer(chosen_sentence,
                                         max_length=max_seq_len,
                                         padding="max_length",
                                         truncation=True,
                                         return_tensors="pt")
                chosen_token["input_ids"] = chosen_token["input_ids"].squeeze(
                    0)
                chosen_token["attention_mask"] = chosen_token[
                    "attention_mask"].squeeze(0)
                chosen_dataset.append(chosen_token)
    # å¦‚æœè®­ç»ƒé˜¶æ®µä¸º2ï¼Œåˆ™å°†æ¥å—å’Œè¢«æ‹’ç»çš„å¯¹è¯éƒ½è¿›è¡Œåˆ†è¯å¹¶åˆ†åˆ«æ·»åŠ åˆ°chosen_datasetå’Œreject_datasetä¸­ã€‚
    elif train_phase == 2:
        for i, tmp_data in enumerate(current_dataset):
            # tokenize the text
            chosen_sentence = raw_dataset.get_prompt_and_chosen(
                tmp_data)  # the accept response
            reject_sentence = raw_dataset.get_prompt_and_rejected(
                tmp_data)  # the accept response
            if chosen_sentence is not None and reject_sentence is not None:
                chosen_sentence += end_of_conversation_token  # the accept response
                reject_sentence += end_of_conversation_token
                chosen_token = tokenizer(chosen_sentence,
                                         max_length=max_seq_len,
                                         padding="max_length",
                                         truncation=True,
                                         return_tensors="pt")
                reject_token = tokenizer(reject_sentence,
                                         max_length=max_seq_len,
                                         padding="max_length",
                                         truncation=True,
                                         return_tensors="pt")
                chosen_token["input_ids"] = chosen_token["input_ids"]
                chosen_token["attention_mask"] = chosen_token["attention_mask"]
                chosen_dataset.append(chosen_token)

                reject_token["input_ids"] = reject_token["input_ids"]
                reject_token["attention_mask"] = reject_token["attention_mask"]
                reject_dataset.append(reject_token)
    # å¦‚æœè®­ç»ƒé˜¶æ®µä¸º3ï¼Œåˆ™å°†å¯¹è¯æç¤ºè¿›è¡Œåˆ†è¯å¹¶æ·»åŠ åˆ°prompt_datasetä¸­ã€‚
    elif train_phase == 3:
        for i, tmp_data in enumerate(current_dataset):
            # tokenize the text
            prompt = raw_dataset.get_prompt(tmp_data)
            if prompt is not None:
                prompt_token = tokenizer(prompt, return_tensors="pt")
                prompt_token["input_ids"] = prompt_token["input_ids"]
                prompt_token["attention_mask"] = prompt_token["attention_mask"]
                for key_word in ["input_ids", "attention_mask"]:
                    length = prompt_token[key_word].size()[-1]
                    if length > max_seq_len:
                        y = prompt_token[key_word].squeeze(0)[length -
                                                              (max_seq_len -
                                                               1):].flip(0)
                    else:
                        y = prompt_token[key_word].squeeze(0).flip(0)
                    prompt_token[key_word] = y
                prompt_dataset.append(prompt_token)
    # åˆ›å»ºä¸€ä¸ªæ–°çš„PromptDatasetå¯¹è±¡ï¼Œå¹¶è¿”å›ã€‚è¿™ä¸ªå¯¹è±¡åŒ…å«äº†å¯¹è¯æç¤ºã€æ¥å—çš„å¯¹è¯å’Œè¢«æ‹’ç»çš„å¯¹è¯çš„æ•°æ®é›†ï¼Œä»¥åŠåˆ†è¯å™¨çš„å¡«å……æ ‡è®°IDå’Œè®­ç»ƒé˜¶æ®µã€‚
    return PromptDataset(prompt_dataset, chosen_dataset, reject_dataset,
                         tokenizer.pad_token_id, train_phase)
```

æ¥ä¸‹æ¥å†è§£æä¸€ä¸‹`create_dataset`è¿™ä¸ªå‡½æ•°ï¼š

```python
è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªå‡½æ•° create_datasetï¼Œä¸»è¦è´Ÿè´£åˆ›å»ºè®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†ï¼Œå…·ä½“çš„åŠŸèƒ½ç»†èŠ‚å¦‚ä¸‹ï¼š
def create_dataset(local_rank, dataset_name, data_split, output_path,
                   train_phase, seed, tokenizer, end_of_conversation_token,
                   max_seq_len):
    # è°ƒç”¨ get_raw_dataset å‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®æä¾›çš„æ•°æ®é›†åç§°ã€è¾“å‡ºè·¯å¾„ã€éšæœºç§å­å’Œlocal_rankç­‰å‚æ•°ï¼Œä»å„ç§é¢„å®šä¹‰çš„æ•°æ®é›†ä¸­è·å–æ‰€éœ€çš„åŸå§‹æ•°æ®é›†ã€‚
    raw_dataset = get_raw_dataset(dataset_name, output_path, seed, local_rank)
    train_dataset = raw_dataset.get_train_data() # ä»åŸå§‹æ•°æ®é›†ä¸­è·å–è®­ç»ƒæ•°æ®ã€‚
    #  è·å–è®­ç»ƒæ•°æ®é›†çš„ç´¢å¼•ï¼Œæ¶‰åŠæ•°æ®çš„åˆ†å‰²ã€‚
    train_index = get_raw_dataset_split_index(local_rank, output_path,
                                              raw_dataset.dataset_name_clean,
                                              seed, "train", data_split,
                                              train_phase - 1,
                                              len(train_dataset))
    # æ ¹æ®ä¸Šä¸€æ­¥è·å–çš„ç´¢å¼•ï¼Œåˆ›å»ºè®­ç»ƒæ•°æ®çš„å­é›†ã€‚
    train_dataset = Subset(train_dataset, train_index)
    # è°ƒç”¨ create_dataset_split å‡½æ•°å¯¹ä¸Šä¸€æ­¥è·å¾—çš„æ•°æ®å­é›†è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ï¼Œ
    # è¿™å¯èƒ½åŒ…æ‹¬å¯¹æ–‡æœ¬çš„æ ‡è®°åŒ–(tokenization)ï¼Œå¹¶ä¸”åˆ›å»ºä¸€ä¸ªPromptDataset å¯¹è±¡ã€‚
    train_dataset = create_dataset_split(train_dataset, raw_dataset,
                                         train_phase, tokenizer,
                                         end_of_conversation_token,
                                         max_seq_len)
    # æ˜¯ç”¨äºåˆ›å»ºè¯„ä¼°æ•°æ®é›†çš„ï¼Œæ­¥éª¤ä¸è®­ç»ƒæ•°æ®é›†çš„åˆ›å»ºåŸºæœ¬ç›¸åŒã€‚
    eval_dataset = raw_dataset.get_eval_data()
    eval_index = get_raw_dataset_split_index(local_rank, output_path,
                                             raw_dataset.dataset_name_clean,
                                             seed, "eval",
                                             data_split, train_phase - 1,
                                             len(eval_dataset))
    eval_dataset = Subset(eval_dataset, eval_index)
    eval_dataset = create_dataset_split(eval_dataset, raw_dataset, train_phase,
                                        tokenizer, end_of_conversation_token,
                                        max_seq_len)
    return train_dataset, eval_dataset
```

è¿™é‡Œéœ€è¦é¢å¤–æ³¨æ„ä¸€ç‚¹ï¼Œ`create_dataset_split` è¿™ä¸ªå‡½æ•°ä¼ å…¥çš„å‚æ•°æ˜¯ train_phaseï¼Œè€Œä¸æ˜¯åƒ `get_raw_dataset_split_index` å‡½æ•°é‚£æ ·ä¼ å…¥ train_phase-1ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿè¿™æ˜¯å› ä¸º`train_phase`ç”¨æ¥æ ‡è¯†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å“ªä¸ªé˜¶æ®µï¼Œæ˜¯ä¸€ä¸ªç›´æ¥çš„æŒ‡ç¤ºç¬¦ï¼Œä¸æ•°æ®åˆ†å‰²æˆ–ç´¢å¼•æ— å…³ã€‚å®ƒç”¨æ¥åœ¨`create_dataset_split`å‡½æ•°ä¸­ç¡®å®šå½“å‰å¤„äºå“ªä¸ªè®­ç»ƒé˜¶æ®µï¼Œå¹¶æ ¹æ®ä¸åŒçš„è®­ç»ƒé˜¶æ®µå¯¹æ•°æ®è¿›è¡Œä¸åŒçš„å¤„ç†ã€‚è€Œåœ¨è°ƒç”¨`get_raw_dataset_split_index`å‡½æ•°æ—¶ï¼Œä¼ å…¥çš„æ˜¯`train_phase - 1`ï¼Œè¿™æ˜¯å› ä¸ºåœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®å½“å‰è®­ç»ƒé˜¶æ®µçš„å‰ä¸€ä¸ªé˜¶æ®µï¼ˆç”±`train_phase - 1`è¡¨ç¤ºï¼‰çš„æ•°æ®åˆ†å‰²æˆ–ç´¢å¼•æƒ…å†µï¼Œæ¥å†³å®šå¦‚ä½•å¯¹å½“å‰é˜¶æ®µçš„æ•°æ®è¿›è¡Œåˆ†å‰²æˆ–ç´¢å¼•ã€‚`create_dataset_split`ä¸»è¦å…³æ³¨å¦‚ä½•æ ¹æ®è®­ç»ƒé˜¶æ®µå¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œè€Œ`get_raw_dataset_split_index`ä¸»è¦å…³æ³¨å¦‚ä½•æ ¹æ®å‰ä¸€ä¸ªè®­ç»ƒé˜¶æ®µçš„æ•°æ®åˆ†å‰²æˆ–ç´¢å¼•æƒ…å†µï¼Œå¯¹å½“å‰é˜¶æ®µçš„æ•°æ®è¿›è¡Œåˆ†å‰²æˆ–ç´¢å¼•ã€‚


å›æ”¶æœ¬èŠ‚çš„å¼€å¤´ï¼Œæˆ‘ä»¬è§£æ`create_prompt_dataset`å‡½æ•°ï¼š

```python
# è¿™ä¸ªå‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯åˆ›å»ºä¸€ä¸ªåŒ…å«è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†çš„â€œæç¤ºâ€æ•°æ®é›†ï¼Œå¹¶å°†è¿™ä¸¤ä¸ªæ•°æ®é›†ä¿å­˜åœ¨æŒ‡å®šçš„æ–‡ä»¶ä¸­ã€‚å…·ä½“æ¥è¯´ï¼š
def create_prompt_dataset(local_rank,
                          data_path,
                          data_split,
                          output_path,
                          train_phase,
                          seed,
                          tokenizer,
                          max_seq_len,
                          end_of_conversation_token="<|endoftext|>",
                          sft_only_data_path=[],
                          reload=False):
    """
    Creates the prompt dataset
    """
    # os.makedirs(output_path, exist_ok=True): åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²ç»å­˜åœ¨åˆ™ä¸ä¼šå¼•å‘å¼‚å¸¸ã€‚
    os.makedirs(output_path, exist_ok=True)
    # æ„é€ æ–‡ä»¶åï¼Œè¿™ä¸ªæ–‡ä»¶ååŒ…å«äº†å¾ˆå¤šæœ‰å…³æ•°æ®é›†å’Œæ¨¡å‹çš„ä¿¡æ¯ï¼Œå¦‚æ•°æ®è·¯å¾„ã€æ•°æ®åˆ†å‰²ã€è®­ç»ƒé˜¶æ®µã€
    # éšæœºç§å­ã€tokenizerçš„åç§°ã€æœ€å¤§åºåˆ—é•¿åº¦ç­‰ã€‚ç„¶åå°†è¿™ä¸ªæ–‡ä»¶åå“ˆå¸ŒåŒ–ï¼Œä»¥é¿å…æ–‡ä»¶åè¿‡é•¿ã€‚
    fname = "_".join(data_path)
    sft_cache_key = "_".join(sft_only_data_path)
    tokenizer_name = tokenizer.init_kwargs["name_or_path"].replace("/", "_")
    fname = f"{fname}_split{data_split}_phase{train_phase}_seed{seed}_tokenizer{tokenizer_name}_seqlen{max_seq_len}_sft{sft_cache_key}"
    fname = "_".join(fname.split("/"))
    fname = hashlib.sha256(fname.encode()).hexdigest(
    )  # hash the file name to avoid too long file name
    # æ„é€ è®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†çš„æ–‡ä»¶è·¯å¾„ã€‚
    train_fname = f"{output_path}/traindata_{fname}.pt"
    eval_fname = f"{output_path}/evaldata_{fname}.pt"

    # æ£€æŸ¥è®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†çš„æ–‡ä»¶æ˜¯å¦éƒ½å·²ç»å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨ï¼Œåˆ™è¡¨ç¤ºç¼“å­˜å·²ç»æ‰¾åˆ°ï¼Œå¦åˆ™è¡¨ç¤ºéœ€è¦åˆ›å»ºç¼“å­˜ã€‚
    cache_found = os.path.isfile(train_fname) and os.path.isfile(eval_fname)
    buf_create_cache = torch.ByteTensor([not cache_found]).cuda()
    torch.distributed.all_reduce(buf_create_cache)
    
    # å¦‚æœå½“å‰è¿›ç¨‹æ˜¯ä¸»è¿›ç¨‹ï¼ˆlocal_rank <= 0ï¼‰å¹¶ä¸”éœ€è¦åˆ›å»ºç¼“å­˜ï¼Œå°±æ‰§è¡Œä»¥ä¸‹æ“ä½œã€‚
    if local_rank <= 0 and (buf_create_cache.item() != 0 or reload):
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œç›´æ¥è°ƒç”¨create_datasetå‡½æ•°åˆ›å»ºè®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†ã€‚
        if len(data_path) == 1:  # Single dataset.
            train_dataset, eval_dataset = create_dataset(
                local_rank, data_path[0], data_split, output_path, train_phase,
                seed, tokenizer, end_of_conversation_token, max_seq_len)
        else:  # Blending datasets.
            # å¦‚æœæœ‰å¤šä¸ªæ•°æ®é›†ï¼Œå¯¹æ¯ä¸ªæ•°æ®é›†éƒ½è°ƒç”¨create_datasetå‡½æ•°ï¼Œå¹¶æŠŠå¾—åˆ°çš„è®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†æ·»åŠ åˆ°å¯¹åº”çš„åˆ—è¡¨ä¸­ï¼Œ

            train_datasets = []
            eval_datasets = []
            train_size = 0
            eval_size = 0
            for d_path in data_path:
                train_dataset, eval_dataset = create_dataset(
                    local_rank, d_path, data_split, output_path, train_phase,
                    seed, tokenizer, end_of_conversation_token, max_seq_len)
                train_datasets.append(train_dataset)
                eval_datasets.append(eval_dataset)
                train_size += len(train_dataset)
                eval_size += len(eval_dataset)
            # ç„¶åä½¿ç”¨ConcatDatasetå’ŒSubsetå‡½æ•°åˆå¹¶æ•°æ®é›†ã€‚
            train_dataset = ConcatDataset(train_datasets)
            shuffle_idx = get_shuffle_idx(seed, train_size)
            train_dataset = Subset(train_dataset, shuffle_idx.tolist())
            eval_dataset = ConcatDataset(eval_datasets)
            shuffle_idx = get_shuffle_idx(seed, eval_size)
            eval_dataset = Subset(eval_dataset, shuffle_idx.tolist())

        # Append the SFT-only dataset if it exists, and current phase is 1(SFT).
        # å¦‚æœå½“å‰æ˜¯ç¬¬ä¸€é˜¶æ®µçš„è®­ç»ƒï¼ˆSFTï¼‰å¹¶ä¸”æŒ‡å®šäº†ä»…ç”¨äºSFTçš„æ•°æ®é›†ï¼Œé‚£ä¹ˆå¯¹è¿™äº›æ•°æ®é›†æ‰§è¡Œç±»ä¼¼çš„æ“ä½œï¼Œ
        # ç„¶åæŠŠå¾—åˆ°çš„è®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†æ·»åŠ åˆ°åŸæœ‰çš„æ•°æ®é›†ä¸­ã€‚
        if train_phase == 1 and sft_only_data_path:
            sft_train_datasets = []
            sft_eval_datasets = []
            sft_train_size = 0
            sft_eval_size = 0
            for sft_path in sft_only_data_path:
                sft_train_dataset, sft_eval_dataset = create_dataset(
                    local_rank,
                    sft_path,
                    "10,0,0",
                    output_path,
                    train_phase,
                    seed,
                    tokenizer,
                    end_of_conversation_token,
                    max_seq_len,
                )
                sft_train_datasets.append(sft_train_dataset)
                sft_eval_datasets.append(sft_eval_dataset)
                sft_train_size += len(sft_train_dataset)
                sft_eval_size += len(sft_eval_dataset)
            if sft_train_datasets:  # Check if sft_train_datasets is not empty
                sft_train_dataset = ConcatDataset(sft_train_datasets)
                train_dataset = ConcatDataset(
                    [train_dataset, sft_train_dataset])
                shuffle_idx = get_shuffle_idx(seed, len(train_dataset))
                train_dataset = Subset(train_dataset, shuffle_idx.tolist())
            if sft_eval_datasets:  # Check if sft_eval_datasets is not empty
                sft_eval_dataset = ConcatDataset(sft_eval_datasets)
                eval_dataset = ConcatDataset([eval_dataset, sft_eval_dataset])
                shuffle_idx = get_shuffle_idx(seed, len(eval_dataset))
                eval_dataset = Subset(eval_dataset, shuffle_idx.tolist())
        # æŠŠè®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°æ•°æ®é›†ä¿å­˜åˆ°å¯¹åº”çš„æ–‡ä»¶ä¸­ã€‚
        torch.save(train_dataset, train_fname)
        torch.save(eval_dataset, eval_fname)
    # åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆäº†æ•°æ®é›†çš„ä¿å­˜æ“ä½œã€‚
    torch.distributed.barrier()
    return torch.load(train_fname), torch.load(eval_fname)

```

### å·¥å…·å‡½æ•°è§£æ
è¿™é‡Œè§£æä¸€ä¸‹ä¸‹é¢å¯¼å…¥çš„ä»£ç ï¼Œåœ¨`DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/utils.py`è¿™ä¸ªè·¯å¾„ä¸‹ï¼š

```python
from utils.utils import print_rank_0, to_device, save_hf_format, set_random_seed, get_all_reduce_mean, get_optimizer_grouped_parameters, save_zero_three_model, load_hf_tokenizer
```


#### 	print_rank_0

```python
# åœ¨rank0ä¹Ÿå°±æ˜¯master rankæ‰“å°ä¿¡æ¯ï¼Œé˜²æ­¢æ¯ä¸ªæœºå™¨æˆ–GPUéƒ½æ‰“å°æ¶ˆæ¯é€ æˆå¤§é‡é‡å¤ä¿¡æ¯
def print_rank_0(msg, rank=0):
    if rank <= 0:
        print(msg)
```

#### to_device

```python
# è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯æŠŠä¸€ä¸ªæ•°æ®æ‰¹æ¬¡ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Šã€‚åœ¨PyTorchä¸­ï¼Œæ•°æ®é»˜è®¤æ˜¯åœ¨CPUä¸Šçš„ï¼Œ
# å¦‚æœè¦åœ¨GPUä¸Šè¿›è¡Œè¿ç®—ï¼Œå°±éœ€è¦å…ˆæŠŠæ•°æ®ç§»åˆ°GPUä¸Šã€‚è¿™ä¸ªå‡½æ•°é€šè¿‡éå†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰å…ƒç´ å¹¶
# è°ƒç”¨to(device)æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å¦‚æœæŸä¸ªå…ƒç´ ä¸èƒ½è¢«ç§»åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Š
#ï¼ˆä¾‹å¦‚ï¼Œè¿™ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–è€…å…¶ä»–ä¸èƒ½åœ¨GPUä¸Šä½¿ç”¨çš„ç±»å‹ï¼‰ï¼Œé‚£ä¹ˆå°±ç›´æ¥ä¿ç•™è¿™ä¸ªå…ƒç´ ï¼Œä¸è¿›è¡Œä»»ä½•æ“ä½œã€‚
def to_device(batch, device):
    output = {}
    for k, v in batch.items():
        try:
            output[k] = v.to(device)
        except:
            output[k] = v
    return output
```

#### save_hf_format

```python
# è¿™æ®µä»£ç çš„åŠŸèƒ½æ˜¯å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä»¥Hugging Faceæ ¼å¼ä¿å­˜ï¼Œè¿™æ ·ä¹‹åå°±å¯ä»¥ä½¿ç”¨Hugging Faceåº“çš„from_pretrainedæ–¹æ³•åŠ è½½äº†ã€‚
def save_hf_format(model, tokenizer, args, sub_folder=""):
    # used to save huggingface format, so we can use it for hf.from_pretrained
    # é¦–å…ˆï¼Œè¿™è¡Œä»£ç æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰'module'è¿™ä¸ªå±æ€§ã€‚è¿™åœ¨PyTorchä¸­æ˜¯ä¸€ä¸ªå¸¸è§çš„æ¨¡å¼ï¼Œ
    # å½“æ¨¡å‹è¢«å°è£…åœ¨torch.nn.DataParallelæˆ–torch.nn.parallel.DistributedDataParallelæ—¶ï¼Œ
    # æ¨¡å‹çš„æ‰€æœ‰å±æ€§éƒ½ä¼šè¢«å­˜å‚¨åœ¨'module'å±æ€§ä¸­ã€‚æ‰€ä»¥è¿™è¡Œä»£ç çš„ç›®çš„æ˜¯ç¡®ä¿æˆ‘ä»¬æ€»æ˜¯åœ¨åŸå§‹æ¨¡å‹ä¸Šè¿›è¡Œæ“ä½œï¼Œè€Œä¸æ˜¯å¹¶è¡ŒåŒ–çš„åŒ…è£…å™¨ã€‚
    model_to_save = model.module if hasattr(model, 'module') else model
    # å®šä¹‰æ¨¡å‹çš„é…ç½®å’Œæƒé‡çš„æ–‡ä»¶åã€‚
    CONFIG_NAME = "config.json"
    WEIGHTS_NAME = "pytorch_model.bin"
    # å®šä¹‰æ¨¡å‹ä¿å­˜çš„ç›®å½•è·¯å¾„ã€‚
    output_dir = os.path.join(args.output_dir, sub_folder)
    # å¦‚æœæ¨¡å‹ä¿å­˜çš„ç›®å½•ä¸å­˜åœ¨ï¼Œé‚£ä¹ˆåˆ›å»ºè¿™ä¸ªç›®å½•ã€‚
    os.makedirs(output_dir, exist_ok=True)
    # å®šä¹‰æ¨¡å‹æƒé‡å’Œé…ç½®çš„ä¿å­˜è·¯å¾„ã€‚
    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)
    output_config_file = os.path.join(output_dir, CONFIG_NAME)
    # è·å–æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼Œè¿™ä¸ªå­—å…¸åŒ…å«äº†æ¨¡å‹çš„æ‰€æœ‰æƒé‡ã€‚
    save_dict = model_to_save.state_dict()
    # éå†çŠ¶æ€å­—å…¸ä¸­çš„æ‰€æœ‰é”®ã€‚è¿™é‡Œä½¿ç”¨list(save_dict.keys())è€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨
    # save_dict.keys()æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨éå†è¿‡ç¨‹ä¸­ä¼šä¿®æ”¹å­—å…¸ï¼Œç›´æ¥éå†å­—å…¸çš„é”®åœ¨Pythonä¸­ä¼šå¯¼è‡´é”™è¯¯ã€‚
    for key in list(save_dict.keys()):
        # if "lora" in key:ï¼šå¦‚æœé”®ä¸­åŒ…å«"lora"ï¼Œé‚£ä¹ˆåˆ é™¤è¿™ä¸ªé”®å€¼å¯¹ã€‚
        # "lora"å¯èƒ½æ˜¯æŸç§ç‰¹å®šçš„æ¨¡å‹ç»„ä»¶æˆ–å‚æ•°ï¼Œè¿™é‡Œå°†å…¶æ’é™¤åœ¨ä¿å­˜çš„æ¨¡å‹æƒé‡ä¹‹å¤–ã€‚
        if "lora" in key:
            del save_dict[key]
    # å°†å¤„ç†è¿‡çš„çŠ¶æ€å­—å…¸ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚
    torch.save(save_dict, output_model_file)
    # å°†æ¨¡å‹çš„é…ç½®ä¿å­˜ä¸ºJSONæ–‡ä»¶ã€‚
    model_to_save.config.to_json_file(output_config_file)
    # å°†åˆ†è¯å™¨çš„è¯æ±‡è¡¨ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ã€‚
    tokenizer.save_vocabulary(output_dir)

```
#### set_random_seed

```python
# è¿™æ®µä»£ç çš„ç›®çš„æ˜¯ä¸ºæ‰€æœ‰å¯èƒ½ç”¨åˆ°çš„éšæœºæ•°ç”Ÿæˆå™¨è®¾å®šç§å­ï¼Œä»¥ç¡®ä¿å®éªŒç»“æœçš„å¯å¤ç°æ€§ã€‚
# éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­æ˜¯ç”Ÿæˆéšæœºæ•°åºåˆ—çš„åˆå§‹å€¼ï¼Œå½“æˆ‘ä»¬ç”¨åŒä¸€ä¸ªç§å­åˆå§‹åŒ–éšæœºæ•°ç”Ÿæˆå™¨æ—¶ï¼Œå®ƒä¼šç”Ÿæˆå®Œå…¨ä¸€æ ·çš„éšæœºæ•°åºåˆ—ã€‚
def set_random_seed(seed):
    # é¦–å…ˆæ£€æŸ¥ç§å­æ˜¯å¦æ˜¯Noneã€‚å¦‚æœæ˜¯Noneï¼Œé‚£ä¹ˆå°±è·³è¿‡è¿™æ®µä»£ç ï¼Œéšæœºæ•°ç”Ÿæˆå™¨å°†ä¼šç”¨ä¸€ä¸ªéšæœºçš„ç§å­ã€‚
    if seed is not None: 
        set_seed(seed)# è¿™ä¸€è¡Œè°ƒç”¨transformersåº“çš„set_seedçš„å‡½æ•°ï¼Œå°†ç§å­è®¾å®šä¸ºæŒ‡å®šçš„å€¼ã€‚
        random.seed(seed) # å°†Pythonå†…ç½®çš„éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­è®¾å®šä¸ºæŒ‡å®šçš„å€¼ã€‚
        np.random.seed(seed) # å°†NumPyçš„éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­è®¾å®šä¸ºæŒ‡å®šçš„å€¼ã€‚
        torch.manual_seed(seed) # å°†PyTorchçš„éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­è®¾å®šä¸ºæŒ‡å®šçš„å€¼ã€‚
        torch.cuda.manual_seed_all(seed) # å°†PyTorchçš„æ‰€æœ‰GPUéšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­è®¾å®šä¸ºæŒ‡å®šçš„å€¼ã€‚
```

#### get_all_reduce_mean

```python
# è¿™æ®µä»£ç æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸­è¿›è¡Œå¹³å‡å€¼è®¡ç®—çš„å‡½æ•°ï¼Œé€šè¿‡è¿™æ®µä»£ç ï¼Œ
# æ‰€æœ‰çš„å¤„ç†å™¨ï¼ˆæˆ–è€…å«èŠ‚ç‚¹ï¼‰ä¸Šçš„åŒä¸€ä¸ªtensoréƒ½ä¼šè¢«åŠ å’Œèµ·æ¥ï¼Œç„¶åé™¤ä»¥æ€»çš„å¤„ç†å™¨æ•°ï¼Œå¾—åˆ°å¹³å‡å€¼ã€‚
def get_all_reduce_mean(tensor):
    # è¿™è¡Œä»£ç æ‰§è¡Œä¸€ä¸ªåˆ†å¸ƒå¼çš„reduceæ“ä½œã€‚reduceæ“ä½œæ˜¯æŒ‡æ‰€æœ‰å¤„ç†å™¨ä¸­çš„åŒä¸€ä¸ªtensoréƒ½è¢«æŸç§æ–¹å¼ç»“åˆèµ·æ¥ã€‚
    # åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œtorch.distributed.ReduceOp.SUMè¡¨ç¤ºæ‰€æœ‰å¤„ç†å™¨ä¸Šçš„tensorå°†è¢«åŠ å’Œèµ·æ¥ã€‚
    # åŠ å’Œçš„ç»“æœä¼šåœ¨æ‰€æœ‰å¤„ç†å™¨ä¸Šéƒ½å¯ç”¨ã€‚
    torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)
    # è¿™è¡Œä»£ç å°†å‰ä¸€æ­¥å¾—åˆ°çš„åŠ å’Œç»“æœé™¤ä»¥å¤„ç†å™¨çš„æ•°é‡ï¼ˆä¹Ÿå«ä½œ world sizeï¼‰ã€‚
    # è¿™æ ·ï¼Œtensorå°±å˜æˆäº†æ‰€æœ‰å¤„ç†å™¨ä¸ŠåŸå§‹tensorçš„å¹³å‡å€¼ã€‚
    tensor = tensor / torch.distributed.get_world_size()
    # æœ€åï¼Œè¿™ä¸ªå¹³å‡å€¼tensorè¢«è¿”å›ã€‚åœ¨æ‰€æœ‰å¤„ç†å™¨ä¸Šï¼Œè¿™ä¸ªå‡½æ•°è¿”å›çš„tensoréƒ½æ˜¯ç›¸åŒçš„ï¼Œ
    # ç­‰äºæ‰€æœ‰å¤„ç†å™¨ä¸ŠåŸå§‹tensorçš„å¹³å‡å€¼ã€‚
    return tensor
```

#### get_optimizer_grouped_parameters

```python
# è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯å°†æ¨¡å‹ä¸­çš„å‚æ•°åˆ†ç»„ä»¥ä¾¿äºåœ¨ä¼˜åŒ–å™¨ä¸­ä½¿ç”¨ã€‚å®ƒå°†æ¨¡å‹å‚æ•°åˆ†ä¸ºä¸¤ç»„ï¼š
# ä¸€ç»„éœ€è¦è¿›è¡Œæƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰çš„å‚æ•°ï¼Œå¦ä¸€ç»„ä¸éœ€è¦è¿›è¡Œæƒé‡è¡°å‡çš„å‚æ•°ã€‚
def get_optimizer_grouped_parameters(model,
                                     weight_decay,
                                     no_decay_name_list=[
                                         "bias", "LayerNorm.weight"
                                     ]):
    # å®ƒå®šä¹‰äº†ä¸€ä¸ªåˆ—è¡¨ optimizer_grouped_parametersï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªå­—å…¸ã€‚æ¯ä¸ªå­—å…¸éƒ½å¯¹åº”ä¸€ä¸ªå‚æ•°ç»„ï¼ŒåŒ…å« "params" å’Œ "weight_decay" è¿™ä¸¤ä¸ªå…³é”®å­—ã€‚
    optimizer_grouped_parameters = [
        # åœ¨ç¬¬ä¸€ä¸ªå­—å…¸ä¸­ï¼Œå®ƒä»æ¨¡å‹å‚æ•°ä¸­é€‰å‡ºé‚£äº›åç§°ä¸åŒ…å« "bias" æˆ– "LayerNorm.weight" 
        # ä¸”éœ€è¦æ±‚æ¢¯åº¦çš„å‚æ•°ã€‚è¿™äº›å‚æ•°åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼šåº”ç”¨ weight_decay ä½œä¸ºæƒé‡è¡°å‡é¡¹ã€‚
        {
            "params": [
                p for n, p in model.named_parameters()
                if (not any(nd in n
                            for nd in no_decay_name_list) and p.requires_grad)
            ],
            "weight_decay":
            weight_decay,
        },
        # åœ¨ç¬¬äºŒä¸ªå­—å…¸ä¸­ï¼Œå®ƒé€‰å‡ºé‚£äº›åç§°åŒ…å« "bias" æˆ– "LayerNorm.weight" ä¸”éœ€è¦æ±‚æ¢¯åº¦çš„å‚æ•°ã€‚
        # è¿™äº›å‚æ•°åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¸ä¼šåº”ç”¨æƒé‡è¡°å‡ï¼Œå³å…¶ "weight_decay" å€¼ä¸º0ã€‚

        {
            "params": [
                p for n, p in model.named_parameters()
                if (any(nd in n
                        for nd in no_decay_name_list) and p.requires_grad)
            ],
            "weight_decay":
            0.0,
        },
    ]
    return optimizer_grouped_parameters
```

è¿™ç§å‚æ•°çš„åˆ†ç»„ç­–ç•¥æ˜¯å¾ˆå¸¸è§çš„ã€‚æ¯”å¦‚åœ¨è®­ç»ƒTransformeræ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¼šä¸ºæƒé‡å’Œåç½®é¡¹è®¾å®šä¸åŒçš„å­¦ä¹ ç­–ç•¥ã€‚è¿™æ˜¯å› ä¸ºæƒé‡è¡°å‡å¯¹äºé˜²æ­¢è¿‡æ‹Ÿåˆå¾ˆæœ‰å¸®åŠ©ï¼Œä½†å¯¹äºæŸäº›å‚æ•°ï¼ˆå¦‚åç½®é¡¹æˆ–è€…å±‚å½’ä¸€åŒ–çš„æƒé‡ï¼‰å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå› æ­¤å¸¸å¸¸ä¼šæ’é™¤è¿™äº›å‚æ•°ä¸è¿›è¡Œæƒé‡è¡°å‡ã€‚
#### save_zero_three_model

```python
# è¿™ä¸ªå‡½æ•°çš„ä¸»è¦åŠŸèƒ½æ˜¯ç­›é€‰å‡ºé‚£äº›åœ¨DeepSpeed Zero 3ä¼˜åŒ–ä¸­è¢«ç¦»çº¿å­˜å‚¨ï¼Œä½†åœ¨å½“å‰è¿˜æœªè·å–çš„å‚æ•°ã€‚
# åœ¨DeepSpeed Zero 3ä¼˜åŒ–ä¸­ï¼Œä¸€äº›æ¨¡å‹å‚æ•°åœ¨ä½¿ç”¨è¿‡åä¼šè¢«ç¦»çº¿å­˜å‚¨ï¼Œä»¥æ­¤é‡Šæ”¾GPUæ˜¾å­˜ã€‚
# å½“è¿™äº›å‚æ•°éœ€è¦å†æ¬¡è¢«ä½¿ç”¨æ—¶ï¼Œéœ€è¦å…ˆè·å–åˆ°æœ¬åœ°ã€‚
def _z3_params_to_fetch(param_list):
    # è¿™ä¸ªæ¡ä»¶è¯­å¥åˆ¤æ–­ä¸€ä¸ªå‚æ•°æ˜¯å¦æ˜¯è¢«DeepSpeed Zero 3ä¼˜åŒ–è¿‡çš„ï¼Œä¸”å…¶çŠ¶æ€ä¸º"æœªè·å–"ï¼ˆNOT_AVAILABLEï¼‰ã€‚
    # å¯¹äºè¢«DeepSpeed Zero 3ä¼˜åŒ–è¿‡çš„å‚æ•°ï¼Œå®ƒä»¬æœ‰ä¸€ä¸ªds_idå±æ€§å’Œä¸€ä¸ªds_statuså±æ€§ï¼Œå…¶ä¸­ds_statusè¡¨ç¤ºå‚æ•°çš„å½“å‰çŠ¶æ€ã€‚
    return [
        p for p in param_list
        if hasattr(p, 'ds_id') and p.ds_status == ZeroParamStatus.NOT_AVAILABLE
    ]

# è¿™ä¸ªå‡½æ•°çš„ä¸»è¦ä½œç”¨æ˜¯ä¿å­˜ä¸€ä¸ªä½¿ç”¨äº†DeepSpeed Zeroä¼˜åŒ–ï¼ˆå¯èƒ½ä¸ºstage 3ï¼‰çš„æ¨¡å‹ã€‚
# DeepSpeedçš„Zeroä¼˜åŒ–æŠ€æœ¯æ˜¯ä¸ºäº†è§£å†³æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ç­‰å†…å­˜å ç”¨é—®é¢˜ï¼Œ
# é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥è®­ç»ƒæ¯”å½“å‰GPUå†…å­˜æ›´å¤§çš„æ¨¡å‹ã€‚
def save_zero_three_model(model_ema, global_rank, save_dir, zero_stage=0):
    # é¦–å…ˆï¼Œæ£€æŸ¥è¾“å…¥çš„zero_stageæ˜¯å¦ä¸º3ï¼Œç¡®å®šæ˜¯å¦ä½¿ç”¨äº†DeepSpeed Zeroé˜¶æ®µ3ä¼˜åŒ–ã€‚
    zero_stage_3 = (zero_stage == 3)
    # ç„¶åï¼Œç¡®ä¿ä¿å­˜æ¨¡å‹çš„ç›®å½•å­˜åœ¨ã€‚
    os.makedirs(save_dir, exist_ok=True)
    # å®šä¹‰æ¨¡å‹æƒé‡ä¿å­˜çš„å®Œæ•´è·¯å¾„ã€‚
    WEIGHTS_NAME = "pytorch_model.bin"
    output_model_file = os.path.join(save_dir, WEIGHTS_NAME)

    # å¦‚æœæ¨¡å‹æ˜¯è¢«åŒ…è£¹åœ¨å…¶å®ƒç»“æ„ï¼ˆå¦‚DataParallelæˆ–DistributedDataParallelï¼‰ä¸­çš„ï¼Œæˆ‘ä»¬éœ€è¦å–å‡ºçœŸå®çš„æ¨¡å‹å®ä¾‹ã€‚
    model_to_save = model_ema.module if hasattr(model_ema,
                                                'module') else model_ema
    if not zero_stage_3:
    	 # å¦‚æœæ²¡æœ‰ä½¿ç”¨Zeroé˜¶æ®µ3ä¼˜åŒ–ï¼Œç›´æ¥ä½¿ç”¨PyTorchçš„torch.saveå‡½æ•°ä¿å­˜æ¨¡å‹çŠ¶æ€ã€‚ 
        if global_rank == 0:
            torch.save(model_to_save.state_dict(), output_model_file)
    else:
        # å¦‚æœä½¿ç”¨äº†Zeroé˜¶æ®µ3ä¼˜åŒ–ï¼Œå› ä¸ºæ¨¡å‹çš„éƒ¨åˆ†å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šï¼Œæ‰€ä»¥éœ€è¦å…ˆå°†å®ƒä»¬æ”¶é›†èµ·æ¥ã€‚
        output_state_dict = {}
        for k, v in model_to_save.named_parameters():

            if hasattr(v, 'ds_id'):
                # deepspeed.zero.GatheredParametersæ˜¯DeepSpeedæä¾›çš„ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œ
                # å®ƒå¯ä»¥å°†åˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šçš„å‚æ•°æ”¶é›†åˆ°ä¸€èµ·ã€‚è¿™éƒ¨åˆ†å‚æ•°ä¿å­˜åœ¨CPUä¸Šã€‚
                with deepspeed.zero.GatheredParameters(_z3_params_to_fetch([v
                                                                            ]),
                                                       enabled=zero_stage_3):
                    v_p = v.data.cpu()
            else:
                v_p = v.cpu()
            # ç„¶åï¼Œå°†æ”¶é›†å¥½çš„å‚æ•°ï¼ˆå¹¶ä¸”ä¸åŒ…å«â€œloraâ€å…³é”®å­—çš„å‚æ•°ï¼‰æ·»åŠ åˆ°è¾“å‡ºçŠ¶æ€å­—å…¸ä¸­ã€‚
            if global_rank == 0 and "lora" not in k:
                output_state_dict[k] = v_p
        # æœ€åï¼Œå†ä½¿ç”¨torch.saveå‡½æ•°ä¿å­˜æ¨¡å‹çŠ¶æ€ã€‚
        if global_rank == 0:
            torch.save(output_state_dict, output_model_file)
        # åŒæ—¶ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œä½¿ç”¨delå…³é”®å­—åˆ é™¤äº†å­˜å‚¨å‚æ•°çš„å­—å…¸ã€‚
        del output_state_dict

```

#### load_hf_tokenizer

```python
# è¿™ä¸ªå‡½æ•°çš„ç›®æ ‡æ˜¯åŠ è½½ä¸€ä¸ªHugging Face tokenizerï¼Œè¿™ä¸ªtokenizeræ˜¯ç”¨æ¥å°†æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºæ¨¡å‹å¯ä»¥æ¥å—çš„å½¢å¼çš„ã€‚
# model_name_or_path: è¿™ä¸ªå‚æ•°å¯ä»¥æ˜¯ä¸€ä¸ªæ¨¡å‹åå­—æˆ–è€…ä¸€ä¸ªæœ¬åœ°è·¯å¾„ï¼ŒæŒ‡ç¤ºä»å“ªé‡Œè·å–tokenizerã€‚
# fast_tokenizer=True: è¿™ä¸ªå‚æ•°æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨Hugging Faceçš„"fast" tokenizerã€‚"fast" tokenizeræ˜¯ç”¨Rustç¼–å†™çš„ï¼Œè¿è¡Œé€Ÿåº¦æ›´å¿«ã€‚
def load_hf_tokenizer(model_name_or_path, fast_tokenizer=True):
    # è¿™è¡Œä»£ç æ£€æŸ¥model_name_or_pathæ˜¯å¦æ˜¯ä¸€ä¸ªå­˜åœ¨çš„è·¯å¾„ã€‚å¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå°±å°è¯•ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­åŠ è½½tokenizerã€‚
    if os.path.exists(model_name_or_path):
        # Locally tokenizer loading has some issue, so we need to force download
        # è¿™è¡Œä»£ç æ‹¼æ¥è·¯å¾„ï¼Œå°è¯•æ‰¾åˆ°æ¨¡å‹çš„configæ–‡ä»¶ã€‚
        model_json = os.path.join(model_name_or_path, "config.json")
        # è¿™è¡Œä»£ç æ£€æŸ¥configæ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚å¦‚æœå­˜åœ¨ï¼Œé‚£ä¹ˆå°±å°è¯•ä»configæ–‡ä»¶ä¸­è·å–æ¨¡å‹åå­—ã€‚
        if os.path.exists(model_json):
            model_json_file = json.load(open(model_json)) # è¿™è¡Œä»£ç åŠ è½½configæ–‡ä»¶ã€‚
            model_name = model_json_file["_name_or_path"] # è¿™è¡Œä»£ç ä»configæ–‡ä»¶ä¸­è·å–æ¨¡å‹åå­—ã€‚
            tokenizer = AutoTokenizer.from_pretrained(model_name,
                                                      fast_tokenizer=True) # è¿™è¡Œä»£ç ä½¿ç”¨æ¨¡å‹åå­—ä»Hugging Faceçš„æ¨¡å‹åº“ä¸­ä¸‹è½½å¹¶åŠ è½½tokenizerã€‚
    else: # å¦‚æœmodel_name_or_pathä¸æ˜¯ä¸€ä¸ªå­˜åœ¨çš„è·¯å¾„ï¼Œé‚£ä¹ˆå°±è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªæ¨¡å‹åå­—ã€‚
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,
                                                  fast_tokenizer=True) # è¿™è¡Œä»£ç ä½¿ç”¨æ¨¡å‹åå­—ä»Hugging Faceçš„æ¨¡å‹åº“ä¸­ä¸‹è½½å¹¶åŠ è½½tokenizerã€‚
    return tokenizer
```

å†è§£æä¸€ä¸‹`DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/ds_utils.py`å¯¼å…¥çš„`get_train_ds_config`å‡½æ•°ï¼š

```python
# DeepSpeed Team
GLOBAL_BATCH_SIZE = 32
MICRO_BATCH_SIZE = 4

# è¿™ä¸ªå‡½æ•°çš„ä¸»è¦ç›®æ ‡æ˜¯ç”Ÿæˆä¸€ä¸ªå­—å…¸ï¼Œç”¨æ¥é…ç½®DeepSpeedè®­ç»ƒæ¨¡å‹çš„å„ç§å‚æ•°ã€‚
# DeepSpeedæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„åº“ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ç§å«åšZeROï¼ˆZero Redundancy Optimizerï¼‰çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œ
# å¯ä»¥å¤§å¤§å‡å°‘æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å­˜å’Œå†…å­˜å ç”¨ã€‚
def get_train_ds_config(offload,
                        stage=2,
                        enable_hybrid_engine=False,
                        inference_tp_size=1,
                        release_inference_cache=False,
                        pin_parameters=True,
                        tp_gather_partition_size=8,
                        max_out_tokens=512):
    # è¿™è¡Œä»£ç é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªå˜é‡deviceã€‚å¦‚æœå‚æ•°offloadä¸ºTrueï¼Œé‚£ä¹ˆå°±å°†deviceè®¾ç½®ä¸º"cpu"ï¼Œ
    # è¡¨ç¤ºæ¨¡å‹çš„å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€å°†è¢«"offload"åˆ°CPUå†…å­˜ä¸­ï¼Œä»¥å‡å°‘GPUæ˜¾å­˜çš„å ç”¨ã€‚
    # å¦‚æœoffloadä¸ºFalseï¼Œé‚£ä¹ˆdeviceå°±è¢«è®¾ç½®ä¸º"none"ï¼Œè¡¨ç¤ºä¸è¿›è¡Œoffloadã€‚
    device = "cpu" if offload else "none"
    # è¿™è¡Œä»£ç å®šä¹‰äº†ä¸€ä¸ªå­—å…¸zero_opt_dictï¼ŒåŒ…å«äº†ZeROä¼˜åŒ–å™¨çš„å„ç§é…ç½®å‚æ•°ã€‚
    zero_opt_dict = {
        "stage": stage, # ZeROçš„é˜¶æ®µï¼Œå¯ä»¥ä¸º0ã€1ã€2ã€3ï¼Œé˜¶æ®µè¶Šé«˜ï¼Œæ˜¾å­˜ä¼˜åŒ–æ•ˆæœè¶Šå¥½ï¼Œä½†è®¡ç®—å¤æ‚åº¦ä¹Ÿè¶Šé«˜ã€‚
        "offload_param": { # offload_paramå’Œoffload_optimizerè¿™ä¸¤ä¸ªå­—æ®µæ§åˆ¶æ˜¯å¦å°†æ¨¡å‹çš„å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€offloadåˆ°CPUå†…å­˜ã€‚
            "device": device
        },
        "offload_optimizer": {
            "device": device
        }, 
        # è¿™ä¸‰ä¸ªå­—æ®µåªæœ‰åœ¨ZeRO stage 3æ—¶æœ‰æ•ˆï¼Œç”¨äºæ§åˆ¶å‚æ•°offloadingçš„ç»†èŠ‚ã€‚
        "stage3_param_persistence_threshold": 1e4,
        "stage3_max_live_parameters": 3e7,
        "stage3_prefetch_bucket_size": 3e7,
        # è¿™ä¸ªå­—æ®µæ§åˆ¶æ˜¯å¦ä½¿ç”¨æ˜¾å­˜æ›´é«˜æ•ˆçš„çº¿æ€§å±‚å®ç°ã€‚ç”±äºå½“å‰çš„æ˜¾å­˜é«˜æ•ˆå®ç°å¯èƒ½ä¼šå¯¼è‡´æ•°å€¼ä¸ç¨³å®šï¼Œæ‰€ä»¥è¿™é‡Œé»˜è®¤è®¾ä¸ºFalseã€‚
        "memory_efficient_linear": False
    }
    # è¿”å›ä¸€ä¸ªé…ç½®å­—å…¸ï¼Œè¿™ä¸ªå­—å…¸åŒ…å«äº†è®­ç»ƒæ¨¡å‹çš„å„ç§å‚æ•°ã€‚
    return {
        "train_batch_size": GLOBAL_BATCH_SIZE, # å…¨å±€çš„æ‰¹æ¬¡å¤§å°
        "train_micro_batch_size_per_gpu": MICRO_BATCH_SIZE, # æ¯ä¸ªGPUçš„å¾®æ‰¹æ¬¡å¤§å°ã€‚
        "steps_per_print": 10, # æ¯éš”10æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒä¿¡æ¯ã€‚
        "zero_optimization": zero_opt_dict, # ä¹‹å‰å®šä¹‰çš„ZeROé…ç½®å­—å…¸ã€‚
        "fp16": { # æ§åˆ¶æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œä»¥åŠloss scalingçš„çª—å£å¤§å°ã€‚
            "enabled": True,
            "loss_scale_window": 100
        },
        "gradient_clipping": 1.0, # æ¢¯åº¦è£å‰ªçš„é˜ˆå€¼ã€‚
        "prescale_gradients": False, # æ˜¯å¦åœ¨æ¢¯åº¦ç´¯è®¡ä¹‹å‰å°±è¿›è¡Œæ¢¯åº¦ç¼©æ”¾ï¼Œé€šå¸¸ç”¨äºé˜²æ­¢æ¢¯åº¦ä¸‹æº¢ã€‚
        "wall_clock_breakdown": False, # æ˜¯å¦è¿›è¡Œæ¯æ­¥è®­ç»ƒæ—¶é—´çš„è¯¦ç»†åˆ†æã€‚
        "hybrid_engine": {
            "enabled": enable_hybrid_engine,
            "max_out_tokens": max_out_tokens,
            "inference_tp_size": inference_tp_size,
            "release_inference_cache": release_inference_cache,
            "pin_parameters": pin_parameters,
            "tp_gather_partition_size": tp_gather_partition_size,
        }
    }
```

å†è§£æä¸€ä¸‹`DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/module/lora.py`å¯¼å…¥çš„`convert_linear_layer_to_lora, convert_lora_to_linear_layer, only_optimize_lora_parameters` è¿™å‡ ä¸ªå‡½æ•°ã€‚

åœ¨è§£æè¿™å‡ ä¸ªå‡½æ•°ä¹‹å‰éœ€è¦å…ˆè§£æä¸€ä¸‹ `LinearLayer_LoRA` è¿™ä¸ªç±»ï¼š

```python
# LinearLayer_LoRA æ˜¯ä¸€ä¸ªå®ç°äº† LoRAï¼ˆLow-Rank Adaptationï¼‰ç­–ç•¥çš„ PyTorch æ¨¡å—ã€‚
# LoRA æ˜¯ä¸€ç§åœ¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ GPT-3 æˆ–è€… BERTï¼‰å¾®è°ƒä¸­ä½¿ç”¨çš„ç­–ç•¥ï¼Œå¯ä»¥æé«˜å¾®è°ƒçš„æ•ˆæœã€‚
# è¿™ä¸ªç±»æ˜¯ä¸€ä¸ªç®€å•çš„ LoRA å®ç°ï¼Œåªæ”¯æŒçº¿æ€§å±‚ã€‚
class LinearLayer_LoRA(nn.Module):
    # an simple implementation of LoRA
    # for now only support Linear Layer
    # åˆå§‹åŒ–å‡½æ•°ï¼Œå®šä¹‰äº†ä¸€äº›å‚æ•°ï¼Œå¦‚æƒé‡ï¼ˆweightï¼‰ã€LoRA ç»´åº¦ï¼ˆlora_dimï¼‰ã€
    # LoRA ç¼©æ”¾å› å­ï¼ˆlora_scalingï¼‰ã€LoRA dropout ï¼ˆlora_droppoutï¼‰ä»¥åŠåç½®ï¼ˆbiasï¼‰ã€‚
    def __init__(self,
                 weight,
                 lora_dim=0,
                 lora_scaling=1,
                 lora_droppout=0,
                 bias=None):
        super(LinearLayer_LoRA, self).__init__()
        self.weight = weight
        self.bias = bias
        # åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆæ£€æŸ¥ lora_dim æ˜¯å¦å¤§äº0ï¼Œå¦‚æœä¸æ˜¯ï¼Œå°±æŠ›å‡ºå¼‚å¸¸ã€‚
        if lora_dim <= 0:
            raise ValueError(
                "You are training to use LoRA, whose reduced dim should be larger than 1"
            )
        # ç„¶åå°è¯•è·å–æƒé‡çš„å½¢çŠ¶ï¼Œå¦‚æœæƒé‡æ˜¯åœ¨ ZeRO é˜¶æ®µ3ä¸­çš„å‚æ•°ï¼Œé‚£ä¹ˆé€šè¿‡ ds_shape å±æ€§è·å–å½¢çŠ¶ï¼›
        # å¦åˆ™ç›´æ¥ä½¿ç”¨ shape å±æ€§ã€‚
        try:
            # for zero stage 3
            rows, columns = weight.ds_shape
        except:
            rows, columns = weight.shape
        # ç„¶ååˆå§‹åŒ– LoRA çš„å·¦å³ä¸¤ä¸ªæƒé‡çŸ©é˜µï¼Œå¹¶è®¾ç½®å®ƒä»¬ä¸ºå¯å­¦ä¹ çš„å‚æ•°ã€‚
        self.lora_right_weight = nn.Parameter(torch.zeros(
            columns,
            lora_dim))  # apply transpose so in forward we do not need to
        self.lora_left_weight = nn.Parameter(torch.zeros(lora_dim, rows))
        self.lora_scaling = lora_scaling / lora_dim
        # å¦‚æœ lora_droppout å¤§äº0ï¼Œåˆ™åˆ›å»ºä¸€ä¸ª Dropout å±‚ï¼›å¦åˆ™åˆ›å»ºä¸€ä¸ª Identity å±‚ã€‚
        if lora_droppout > 0:
            self.lora_dropout = nn.Dropout(lora_droppout)
        else:
            self.lora_dropout = nn.Identity()
        # ç„¶åè°ƒç”¨ reset_parameters æ–¹æ³•å¯¹ LoRA æƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚
        self.reset_parameters()
        # disable the original weight gradient
        # æœ€åï¼Œå…³é—­åŸå§‹æƒé‡çš„æ¢¯åº¦ï¼Œè®¾ç½® LoRA èåˆæ ‡å¿—ä½ä¸º Falseã€‚
        self.weight.requires_grad = False
        # fuse LoRA to the original weight
        self.fuse_lora = False
    
    def eval(self):
        self.lora_dropout.eval() # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œè¿™æ—¶å€™ Dropout å±‚ä¼šåœæ­¢å·¥ä½œã€‚

    #   self.fuse_lora_weight()

    def train(self, mode=True):
        self.lora_dropout.train(mode) # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œè¿™æ—¶å€™ Dropout å±‚ä¼šå¼€å§‹å·¥ä½œã€‚
        # self.unfuse_lora_weight()

    def reset_parameters(self): # åˆå§‹åŒ– LoRA æƒé‡çš„æ–¹æ³•ã€‚å³æƒé‡ä½¿ç”¨ kaiming å‡åŒ€åˆ†å¸ƒè¿›è¡Œåˆå§‹åŒ–ï¼Œå·¦æƒé‡åˆå§‹åŒ–ä¸ºå…¨0ã€‚
        nn.init.kaiming_uniform_(self.lora_right_weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_left_weight)
    
    # fuse_lora_weight(self) å’Œ unfuse_lora_weight(self)ï¼š
    # è¿™ä¸¤ä¸ªæ–¹æ³•ç”¨äºå°† LoRA æƒé‡èåˆåˆ°åŸå§‹æƒé‡ä¸­ï¼Œæˆ–è€…ä»åŸå§‹æƒé‡ä¸­è§£èåˆã€‚
    # èåˆæ“ä½œå®è´¨ä¸Šæ˜¯å°†åŸå§‹æƒé‡ä¸ LoRA æƒé‡çš„ä¹˜ç§¯ï¼ˆç¼©æ”¾åï¼‰ç›¸åŠ ã€‚
    def fuse_lora_weight(self):
        if not self.fuse_lora:
            self.weight.data += self.lora_scaling * torch.matmul(
                self.lora_left_weight.t(), self.lora_right_weight.t())
        self.fuse_lora = True

    def unfuse_lora_weight(self):
        if self.fuse_lora:
            self.weight.data -= self.lora_scaling * torch.matmul(
                self.lora_left_weight.t(), self.lora_right_weight.t())
        self.fuse_lora = False
    # å‰å‘ä¼ æ’­å‡½æ•°ã€‚å¦‚æœ LoRA æƒé‡å·²èåˆï¼Œåˆ™ç›´æ¥å¯¹è¾“å…¥è¿›è¡Œçº¿æ€§å˜æ¢ï¼›
    # å¦åˆ™ï¼Œä¼šé¢å¤–è®¡ç®—ä¸€ä¸ª LoRA é¡¹ï¼Œè¯¥é¡¹æ˜¯è¾“å…¥é€šè¿‡ Dropout å±‚ï¼Œç„¶åä¸ LoRA æƒé‡ç›¸ä¹˜å¾—åˆ°çš„ã€‚
    def forward(self, input):
        if self.fuse_lora:
            return F.linear(input, self.weight, self.bias)
        else:
            return F.linear(
                input, self.weight,
                self.bias) + (self.lora_dropout(input) @ self.lora_right_weight
                              @ self.lora_left_weight) * self.lora_scaling
```

#### convert_linear_layer_to_lora

```python
# è¿™ä¸ªå‡½æ•° convert_linear_layer_to_lora æ˜¯ç”¨æ¥å°†æ¨¡å‹ä¸­çš„çº¿æ€§å±‚è½¬æ¢ä¸º LoRA å±‚çš„ã€‚
# åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä¸å˜çš„åŒæ—¶ï¼Œé€šè¿‡æ·»åŠ é¢å¤–çš„å‚æ•°æ¥å¾®è°ƒæ¨¡å‹ã€‚
# convert the linear layer to LoRA
def convert_linear_layer_to_lora(model,
                                 part_module_name,
                                 lora_dim=0,
                                 lora_scaling=1,
                                 lora_droppout=0):
    repalce_name = []
    # å‡½æ•°é¦–å…ˆéå†æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ˆmodel.named_modules()ï¼‰ï¼Œæ‰¾å‡ºåç§°ä¸­åŒ…å« part_module_name çš„çº¿æ€§å±‚ï¼ˆnn.Linearï¼‰ï¼Œ
    # å¹¶å°†è¿™äº›å±‚çš„åç§°æ·»åŠ åˆ° repalce_name åˆ—è¡¨ä¸­ã€‚
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and part_module_name in name:
            repalce_name.append(name)
    # ç„¶åï¼Œå‡½æ•°éå† repalce_name åˆ—è¡¨ï¼Œä½¿ç”¨ recursive_getattr å‡½æ•°è·å–æ¨¡å‹ä¸­å¯¹åº”åç§°çš„æ¨¡å—ã€‚
    # è¿™äº›æ¨¡å—æ˜¯éœ€è¦è¢«æ›¿æ¢æˆ LoRA å±‚çš„çº¿æ€§å±‚ã€‚
    for name in repalce_name:
        module = recursive_getattr(model, name)
        # å¯¹äºæ¯ä¸€ä¸ªéœ€è¦è¢«æ›¿æ¢çš„æ¨¡å—ï¼Œå‡½æ•°åˆ›å»ºä¸€ä¸ª LinearLayer_LoRA å®ä¾‹ tmpï¼Œ
        # å¹¶å°†å…¶ä¼ è¾“åˆ°ä¸åŸå§‹æ¨¡å—ç›¸åŒçš„è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸Šã€‚åˆ›å»º LinearLayer_LoRA å®ä¾‹æ—¶ï¼Œ
        # éœ€è¦ä¼ å…¥åŸå§‹æ¨¡å—çš„æƒé‡ã€åç½®ä»¥åŠ LoRA å±‚çš„ä¸€äº›å‚æ•°ï¼Œå¦‚ lora_dimã€lora_scaling å’Œ lora_droppoutã€‚
        tmp = LinearLayer_LoRA(
            module.weight, lora_dim, lora_scaling, lora_droppout,
            module.bias).to(module.weight.device).to(module.weight.dtype)
        # åˆ›å»ºå®Œ LinearLayer_LoRA å®ä¾‹åï¼Œå‡½æ•°ä½¿ç”¨ recursive_setattr å‡½æ•°å°†åŸå§‹æ¨¡å—æ›¿æ¢ä¸º LinearLayer_LoRA å®ä¾‹ã€‚
        recursive_setattr(model, name, tmp)
    return model
```

#### convert_lora_to_linear_layer

```python
# è¿™ä¸ªå‡½æ•°çš„ä¸»è¦åŠŸèƒ½æ˜¯ç­›é€‰å‡ºé‚£äº›åœ¨DeepSpeed Zero 3ä¼˜åŒ–ä¸­è¢«ç¦»çº¿å­˜å‚¨ï¼Œä½†åœ¨å½“å‰è¿˜æœªè·å–çš„å‚æ•°ã€‚
# åœ¨DeepSpeed Zero 3ä¼˜åŒ–ä¸­ï¼Œä¸€äº›æ¨¡å‹å‚æ•°åœ¨ä½¿ç”¨è¿‡åä¼šè¢«ç¦»çº¿å­˜å‚¨ï¼Œä»¥æ­¤é‡Šæ”¾GPUæ˜¾å­˜ã€‚
# å½“è¿™äº›å‚æ•°éœ€è¦å†æ¬¡è¢«ä½¿ç”¨æ—¶ï¼Œéœ€è¦å…ˆè·å–åˆ°æœ¬åœ°ã€‚
def _z3_params_to_fetch(param_list):
    # è¿™ä¸ªæ¡ä»¶è¯­å¥åˆ¤æ–­ä¸€ä¸ªå‚æ•°æ˜¯å¦æ˜¯è¢«DeepSpeed Zero 3ä¼˜åŒ–è¿‡çš„ï¼Œä¸”å…¶çŠ¶æ€ä¸º"æœªè·å–"ï¼ˆNOT_AVAILABLEï¼‰ã€‚
    # å¯¹äºè¢«DeepSpeed Zero 3ä¼˜åŒ–è¿‡çš„å‚æ•°ï¼Œå®ƒä»¬æœ‰ä¸€ä¸ªds_idå±æ€§å’Œä¸€ä¸ªds_statuså±æ€§ï¼Œå…¶ä¸­ds_statusè¡¨ç¤ºå‚æ•°çš„å½“å‰çŠ¶æ€ã€‚
    return [
        p for p in param_list
        if hasattr(p, 'ds_id') and p.ds_status == ZeroParamStatus.NOT_AVAILABLE
    ]

# è¿™ä¸ªå‡½æ•° convert_lora_to_linear_layer æ˜¯ç”¨æ¥å°†æ¨¡å‹ä¸­çš„ LoRA å±‚è½¬æ¢å›çº¿æ€§å±‚çš„ã€‚
# åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œè¿™ä¸ªæ“ä½œå¯ä»¥ç”¨äºåœ¨è®­ç»ƒå®Œ LoRA å±‚åï¼Œå°†æ¨¡å‹æ¢å¤åˆ°åŸå§‹çš„çŠ¶æ€ï¼Œ
# ä»¥ä¾¿è¿›è¡Œä¸‹ä¸€æ­¥çš„æ“ä½œï¼Œå¦‚æ¨¡å‹çš„ä¿å­˜ã€åŠ è½½ç­‰ã€‚
# convert the LoRA layer to linear layer
def convert_lora_to_linear_layer(model):
    repalce_name = []
    # å‡½æ•°é¦–å…ˆéå†æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨¡å—ï¼ˆmodel.named_modules()ï¼‰ï¼Œæ‰¾å‡ºæ‰€æœ‰çš„ LoRA å±‚ï¼ˆLinearLayer_LoRAï¼‰ï¼Œ
    # å¹¶å°†è¿™äº›å±‚çš„åç§°æ·»åŠ åˆ° repalce_name åˆ—è¡¨ä¸­ã€‚
    for name, module in model.named_modules():
        if isinstance(module, LinearLayer_LoRA):
            repalce_name.append(name)
    # ç„¶åï¼Œå‡½æ•°éå† repalce_name åˆ—è¡¨ï¼Œä½¿ç”¨ recursive_getattr å‡½æ•°è·å–æ¨¡å‹ä¸­å¯¹åº”åç§°çš„ LoRA å±‚ã€‚
    for name in repalce_name:
        module = recursive_getattr(model, name)
        # å¯¹äºæ¯ä¸€ä¸ª LoRA å±‚ï¼Œå‡½æ•°é¦–å…ˆæ£€æŸ¥æ˜¯å¦å¤„äº zero stage 3ï¼ˆDeepSpeed çš„ä¸€ä¸ªç‰¹æ€§ï¼Œç”¨äºåœ¨å¤šGPUè®­ç»ƒä¸­å‡å°‘å†…å­˜å ç”¨ï¼‰ã€‚
        # å¦‚æœæ˜¯ï¼Œåˆ™è®¾ç½® zero_stage_3 ä¸º Trueã€‚
        zero_stage_3 = hasattr(module.weight, 'ds_id')
        with deepspeed.zero.GatheredParameters(_z3_params_to_fetch([
                module.weight, module.bias, module.lora_left_weight,
                module.lora_right_weight
        ]),
                                               modifier_rank=0,
                                               enabled=zero_stage_3):
            module.fuse_lora_weight()
    return model
```

#### only_optimize_lora_parameters

```python
# è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯å…³é—­æ¨¡å‹ä¸­é™¤LoRAå‚æ•°ä¹‹å¤–çš„æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚è¿™æ„å‘³ç€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªæœ‰LoRAå‚æ•°ä¼šè¢«ä¼˜åŒ–ï¼Œå…¶ä»–å‚æ•°ä¿æŒä¸å˜ã€‚
def only_optimize_lora_parameters(model):
    # turn off the gradient of all the parameters except the LoRA parameters
    # éå†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚æ¯ä¸ªå‚æ•°éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„åç§°nameå’Œå¯¹åº”çš„å‚æ•°å€¼paramã€‚
    for name, param in model.named_parameters():
        # æŸ¥å½“å‰å‚æ•°çš„åç§°æ˜¯å¦åŒ…å«lora_right_weightæˆ–lora_left_weightã€‚
        # è¿™æ˜¯å› ä¸ºåœ¨LoRAï¼ˆLow-Rank Adaptationï¼‰ä¸­ï¼Œåªæœ‰è¿™ä¸¤ç§å‚æ•°æ˜¯éœ€è¦ä¼˜åŒ–çš„ã€‚
        if "lora_right_weight" in name or "lora_left_weight" in name:
            # param.requires_grad = True å¦‚æœå‚æ•°ååŒ…å«lora_right_weightæˆ–lora_left_weightï¼Œ
            # åˆ™è®¾ç½®å‚æ•°çš„requires_gradå±æ€§ä¸ºTrueï¼Œè¡¨ç¤ºéœ€è¦å¯¹æ­¤å‚æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚
            param.requires_grad = True
        else:
            param.requires_grad = False
    return model
```

#### create_hf_model

```python
# è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸ºcreate_hf_modelçš„å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„ä½œç”¨æ˜¯åˆ›å»ºæˆ–åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥å‡½æ•°çš„ä¸»è¦å‚æ•°åŒ…æ‹¬ï¼š
# model_classï¼šæ¨¡å‹çš„ç±»åˆ«ï¼Œä¾‹å¦‚GPT-2ã€BERTç­‰ã€‚
# tokenizerï¼šç”¨äºæ¨¡å‹çš„åˆ†è¯å™¨ã€‚
# ds_config: DeepSpeedçš„é…ç½®å‚æ•°ã€‚
# rlhf_trainingï¼šä¸€ä¸ªæ ‡å¿—ï¼Œç”¨æ¥è¡¨ç¤ºæ˜¯å¦æ­£åœ¨è¿›è¡ŒRLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰è®­ç»ƒã€‚
# disable_dropoutï¼šä¸€ä¸ªæ ‡å¿—ï¼Œç”¨æ¥è¡¨ç¤ºæ˜¯å¦ç¦ç”¨dropoutã€‚Dropoutæ˜¯ä¸€ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ã€‚
def create_hf_model(model_class,
                    model_name_or_path,
                    tokenizer,
                    ds_config=None,
                    rlhf_training=False,
                    disable_dropout=False):
    # æ ¹æ®model_name_or_pathä»é¢„è®­ç»ƒæ¨¡å‹è·å–æ¨¡å‹é…ç½®model_configã€‚
    model_config = AutoConfig.from_pretrained(model_name_or_path)
    # å¦‚æœdisable_dropoutä¸ºçœŸï¼Œåˆ™å°†æ¨¡å‹é…ç½®ä¸­çš„dropoutè®¾ä¸º0.0ã€‚
    if disable_dropout:
        model_config.dropout = 0.0
    # Note: dschf is defined in function scope to avoid global effects
    # https://huggingface.co/docs/transformers/main_classes/deepspeed#nontrainer-deepspeed-integration
    # æ ¹æ®ds_configä¸­çš„è®¾ç½®ï¼Œåˆ›å»ºDeepSpeedçš„é…ç½®å¯¹è±¡dschfï¼Œä»¥ä¾¿è¿›è¡ŒDeepSpeedä¼˜åŒ–ã€‚
    if ds_config is not None and ds_config["zero_optimization"]["stage"] == 3:
        dschf = HfDeepSpeedConfig(ds_config)
    else:
        dschf = None
    # æ ¹æ®rlhf_trainingçš„å€¼ï¼Œç¡®å®šæ˜¯ä»é…ç½®ä¸­åˆ›å»ºæ¨¡å‹è¿˜æ˜¯ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½æ¨¡å‹ã€‚å¦‚æœrlhf_trainingä¸ºçœŸï¼Œåˆ™æ ¹æ®æ¨¡å‹é…ç½®åˆ›å»ºæ¨¡å‹ï¼›å¦åˆ™ï¼Œä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æ¨¡å‹ã€‚
    if rlhf_training:
        # the weight loading is handled by create critic model
        model = model_class.from_config(model_config)
    else:
        model = model_class.from_pretrained(
            model_name_or_path,
            from_tf=bool(".ckpt" in model_name_or_path),
            config=model_config)
    # å°†æ¨¡å‹çš„ç»“æŸæ ‡è®°å’Œå¡«å……æ ‡è®°è®¾ä¸ºåˆ†è¯å™¨çš„ç»“æŸæ ‡è®°idã€‚
    model.config.end_token_id = tokenizer.eos_token_id
    model.config.pad_token_id = model.config.eos_token_id
    # è°ƒæ•´æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ï¼Œä½¿å…¶ä¸º8çš„å€æ•°ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯ä¸ºäº†åœ¨æŸäº›ç¡¬ä»¶ï¼ˆå¦‚GPUï¼‰ä¸Šæé«˜æ•ˆç‡ã€‚
    model.resize_token_embeddings(int(
        8 *
        math.ceil(len(tokenizer) / 8.0)))  # make the vocab size multiple of 8

    return model
```

## 0x3.2 main.pyä¸»ä½“è§£æ
### parse_argsè§£æ

```python
def parse_args():
    # åˆ›å»ºä¸€ä¸ªargparseçš„è§£æå™¨å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡å¯ä»¥æ·»åŠ å‘½ä»¤è¡Œå‚æ•°å’Œå¤„ç†å®ƒä»¬ã€‚descriptionå‚æ•°æä¾›äº†ä¸€ä¸ªå¯¹ç¨‹åºçš„ç®€å•æè¿°ã€‚
    parser = argparse.ArgumentParser(
        description=
        "Finetune a transformers model on a causal language modeling task")
    parser.add_argument('--data_path',
                        nargs='*',
                        default=['Dahoas/rm-static'],
                        help='Path to the training dataset. Accepted format:'
                        '1) a single data path, 2) multiple datasets in the'
                        'form: dataset1-path dataset2-path ...')
    parser.add_argument('--data_split',
                        type=str,
                        default='2,4,4',
                        help='Comma-separated list of proportions for training'
                        'phase 1, 2, and 3 data. For example the split `6,2,2`'
                        'will use 60% of data for phase 1, 20% for phase 2'
                        'and 20% for phase 3.')
    parser.add_argument(
        '--sft_only_data_path',
        nargs='*',
        default=[],
        help='Path to the dataset for only using in SFT phase.')
    parser.add_argument(
        '--data_output_path',
        type=str,
        default='/data_turbo/home/zhangxiaoyu/data_files/',
        help=
        'Where to store the data-related files such as shuffle index. This needs to be on a local storage of a node (not on a shared storage)'
    )
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        help=
        "Path to pretrained model or model identifier from huggingface.co/models.",
        required=True,
    )
    parser.add_argument(
        "--per_device_train_batch_size",
        type=int,
        default=16,
        help="Batch size (per device) for the training dataloader.",
    )
    parser.add_argument(
        "--per_device_eval_batch_size",
        type=int,
        default=16,
        help="Batch size (per device) for the evaluation dataloader.",
    )
    parser.add_argument(
        "--max_seq_len",
        type=int,
        default=512,
        help="The maximum sequence length.",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-3,
        help=
        "Initial learning rate (after the potential warmup period) to use.",
    )
    parser.add_argument("--weight_decay",
                        type=float,
                        default=0.,
                        help="Weight decay to use.")
    parser.add_argument("--num_train_epochs",
                        type=int,
                        default=1,
                        help="Total number of training epochs to perform.")
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help=
        "Number of updates steps to accumulate before performing a backward/update pass.",
    )
    parser.add_argument(
        "--lr_scheduler_type",
        type=SchedulerType,
        default="cosine",
        help="The scheduler type to use.",
        choices=[
            "linear", "cosine", "cosine_with_restarts", "polynomial",
            "constant", "constant_with_warmup"
        ],
    )
    parser.add_argument(
        "--num_warmup_steps",
        type=int,
        default=0,
        help="Number of steps for the warmup in the lr scheduler.")
    parser.add_argument("--output_dir",
                        type=str,
                        default=None,
                        help="Where to store the model.")
    parser.add_argument("--seed",
                        type=int,
                        default=1234,
                        help="A seed for reproducible training.")
    parser.add_argument("--local_rank",
                        type=int,
                        default=-1,
                        help="local_rank for distributed training on gpus")
    parser.add_argument('--gradient_checkpointing',
                        action='store_true',
                        help='Enable HF gradient checkpointing for model.')
    parser.add_argument('--disable_dropout',
                        action='store_true',
                        help='Disable the dropout of the model.')
    # deepspeed features
    parser.add_argument('--offload',
                        action='store_true',
                        help='Enable ZeRO Offload techniques.')
    parser.add_argument(
        '--zero_stage',
        type=int,
        default=0,
        help='ZeRO optimization stage for Actor model (and clones).')
    ## LoRA for efficient training setting
    parser.add_argument("--lora_dim",
                        type=int,
                        default=0,
                        help="If > 0, use LoRA for efficient training.")
    parser.add_argument("--lora_module_name",
                        type=str,
                        default="decoder.layers.",
                        help="The scope of LoRA.")
    parser.add_argument('--only_optimize_lora',
                        action='store_true',
                        help='Only optimize the LoRA parameters.')
    # è¿™ä¸€è¡Œå°†DeepSpeedçš„é…ç½®å‚æ•°æ·»åŠ åˆ°è§£æå™¨ä¸­ã€‚
    parser = deepspeed.add_config_arguments(parser)
    # è¿™ä¸€è¡Œè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨argså¯¹è±¡ä¸­
    args = parser.parse_args()

    # Validate settings
    # åœ¨è¿™ä¸ªä»£ç å—ä¸­ï¼ŒéªŒè¯ä¸€äº›ç‰¹å®šçš„å‚æ•°è®¾ç½®æ˜¯å¦åˆæ³•ã€‚
    # ä¾‹å¦‚ï¼Œå¦‚æœåŒæ—¶å¯ç”¨äº†gradient_checkpointingå’Œä»…ä¼˜åŒ–LoRAå‚æ•°ï¼Œé‚£ä¹ˆå°†ä¼šæŠ›å‡ºä¸€ä¸ªé”™è¯¯ã€‚
    if args.gradient_checkpointing and args.lora_dim > 0:
        assert (
            not args.only_optimize_lora
        ), "--gradient_checkpointing and --only_optimize_lora cannot be enabled at the same time."

    return args
```

### mainå‡½æ•°è§£æ
æ¥ä¸‹æ¥æ˜¯è®­ç»ƒéƒ¨åˆ†çš„æ ¸å¿ƒå‡½æ•°ï¼Œä¹Ÿæ˜¯å…¨æ–‡æœ€åä¸€ä¸ªå‡½æ•°`main`çš„è§£æã€‚

```python
# è¿™ä¸ªå‡½æ•°æ˜¯ä¸»å‡½æ•°ï¼Œæ˜¯è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ä¸»æµç¨‹ï¼Œä¸»è¦æ­¥éª¤åŒ…æ‹¬è§£æå‘½ä»¤è¡Œå‚æ•°ã€
# è®¾ç½®è®¾å¤‡ã€å‡†å¤‡æ•°æ®ã€å®šä¹‰æ¨¡å‹ã€é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ç­‰ã€‚
def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚
    args = parse_args()
    
    # å¦‚æœæœ¬åœ°æ’åä¸º-1ï¼Œè¯´æ˜ä¸åœ¨åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸‹ï¼Œè®¾å¤‡è®¾ç½®ä¸º"cuda"ï¼›
    # å¦åˆ™æ ¹æ®args.local_rankè®¾ç½®å¯¹åº”çš„cudaè®¾å¤‡ï¼Œå¹¶åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒã€‚
    if args.local_rank == -1:
        device = torch.device("cuda")
    else:
        torch.cuda.set_device(args.local_rank)
        device = torch.device("cuda", args.local_rank)
        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
        # torch.distributed.init_process_group(backend='nccl')
        deepspeed.init_distributed()
    
    # è·å–å…¨å±€rankã€‚
    args.global_rank = torch.distributed.get_rank()
    # è·å–deepspeedçš„è®­ç»ƒé…ç½®ã€‚
    ds_config = get_train_ds_config(offload=args.offload,
                                    stage=args.zero_stage)
    # åœ¨é…ç½®ä¸­è®¾ç½®è®­ç»ƒæ—¶æ¯ä¸ªGPUçš„å¾®æ‰¹æ¬¡å¤§å°å’Œæ€»çš„æ‰¹æ¬¡å¤§å°ã€‚
    ds_config[
        'train_micro_batch_size_per_gpu'] = args.per_device_train_batch_size
    ds_config[
        'train_batch_size'] = args.per_device_train_batch_size * torch.distributed.get_world_size(
        ) * args.gradient_accumulation_steps

    # If passed along, set the training seed now.
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœçš„å¯å¤ç°æ€§ã€‚
    set_random_seed(args.seed)
    
    # åœ¨æ­¤å¤„æ·»åŠ ä¸€ä¸ªbarrieræ“ä½œï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½æ‰§è¡Œåˆ°è¿™ä¸€ç‚¹åå†ç»§ç»­æ‰§è¡Œåç»­æ“ä½œã€‚
    torch.distributed.barrier()
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„åˆ†è¯å™¨ã€‚
    tokenizer = load_hf_tokenizer(args.model_name_or_path, fast_tokenizer=True)
    tokenizer.pad_token = tokenizer.eos_token
    # make sure tokenizer is right pad in our logic
    tokenizer.padding_side = 'right'
    # åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ã€‚
    model = create_hf_model(AutoModelForCausalLM,
                            args.model_name_or_path,
                            tokenizer,
                            ds_config,
                            disable_dropout=args.disable_dropout)

    # å¦‚æœå‚æ•°lora_dimå¤§äº0ï¼Œå°†æ¨¡å‹çš„çº¿æ€§å±‚è½¬æ¢ä¸ºLoRaå±‚ï¼›å¦‚æœåªä¼˜åŒ–LoRaå‚æ•°ï¼Œå…³é—­å…¶ä»–å‚æ•°çš„æ¢¯åº¦ã€‚
    if args.lora_dim > 0:
        model = convert_linear_layer_to_lora(model, args.lora_module_name,
                                             args.lora_dim)
        if args.only_optimize_lora:
            model = only_optimize_lora_parameters(model)

    # Prepare the data
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ï¼šåŒ…æ‹¬è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œä»¥åŠå¯¹åº”çš„é‡‡æ ·å™¨å’Œæ•°æ®åŠ è½½å™¨ã€‚
    train_phase = 1
    train_dataset, eval_dataset = create_prompt_dataset(
        args.local_rank,
        args.data_path,
        args.data_split,
        args.data_output_path,
        train_phase,
        args.seed,
        tokenizer,
        args.max_seq_len,
        sft_only_data_path=args.sft_only_data_path)
    # DataLoaders creation:
    if args.local_rank == -1:
        train_sampler = RandomSampler(train_dataset)
        eval_sampler = SequentialSampler(eval_dataset)
    else:
        train_sampler = DistributedSampler(train_dataset)
        eval_sampler = DistributedSampler(eval_dataset)
    train_dataloader = DataLoader(train_dataset,
                                  collate_fn=default_data_collator,
                                  sampler=train_sampler,
                                  batch_size=args.per_device_train_batch_size)
    eval_dataloader = DataLoader(eval_dataset,
                                 collate_fn=default_data_collator,
                                 sampler=eval_sampler,
                                 batch_size=args.per_device_eval_batch_size)
    # å®šä¹‰æ¨¡å‹è¯„ä¼°å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å›°æƒ‘åº¦ã€‚
    def evaluation(model, eval_dataloader):
        model.eval() # å°†æ¨¡å‹åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼ã€‚
        losses = 0 # åˆå§‹åŒ–lossã€‚
        for step, batch in enumerate(eval_dataloader): # å¯¹äºè¯„ä¼°æ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ªbatchã€‚
            batch = to_device(batch, device) # å°†batchæ•°æ®ç§»åˆ°å¯¹åº”çš„è®¾å¤‡ä¸Šã€‚
            with torch.no_grad(): # åœ¨æ­¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ï¼Œä¸è®¡ç®—æ¢¯åº¦ï¼Œè¿™æ ·å¯ä»¥èŠ‚çœå­˜å‚¨å’Œè®¡ç®—èµ„æºã€‚
                # å°†batchæ•°æ®è¾“å…¥æ¨¡å‹ï¼Œè¿›è¡Œå‰å‘è®¡ç®—ã€‚
                outputs = model(**batch)

            loss = outputs.loss # å–å‡ºæ¨¡å‹çš„è¾“å‡ºä¸­çš„lossã€‚
            losses += loss.float() # å°†å½“å‰çš„lossç´¯åŠ åˆ°æ€»çš„lossesä¸­ã€‚
        losses = losses / (step + 1) # è®¡ç®—å¹³å‡çš„lossã€‚
        try:
            perplexity = torch.exp(losses) # å°è¯•è®¡ç®—æ¨¡å‹çš„å›°æƒ‘åº¦ï¼Œå¦‚æœæ•æ‰åˆ°æº¢å‡ºé”™è¯¯ï¼Œå°†å›°æƒ‘åº¦è®¾ç½®ä¸ºæ— ç©·å¤§ã€‚
        except OverflowError:
            perplexity = float("inf")
        # å°è¯•åœ¨æ‰€æœ‰è®¾å¤‡ä¸Šè®¡ç®—å›°æƒ‘åº¦çš„å¹³å‡å€¼ï¼Œå¦‚æœå‘ç”Ÿä»»ä½•é”™è¯¯ï¼Œå°±å¿½ç•¥ã€‚
        try:
            perplexity = get_all_reduce_mean(perplexity).item()
        except:
            pass
        return perplexity
    
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨æƒé‡è¡°å‡å°†æ¨¡å‹å‚æ•°åˆ†ä¸ºä¸¤ç»„ã€‚
    # Split weights in two groups, one with weight decay and the other not.
    optimizer_grouped_parameters = get_optimizer_grouped_parameters(
        model, args.weight_decay)
    # é€‰æ‹©ä¼˜åŒ–å™¨ç±»å‹ï¼Œå¦‚æœå¯ç”¨äº†æ¢¯åº¦Offloadï¼Œä½¿ç”¨DeepSpeedCPUAdamï¼Œå¦åˆ™ä½¿ç”¨FusedAdamã€‚
    AdamOptimizer = DeepSpeedCPUAdam if args.offload else FusedAdam
    # åˆ›å»ºä¼˜åŒ–å™¨ã€‚
    optimizer = AdamOptimizer(optimizer_grouped_parameters,
                              lr=args.learning_rate,
                              betas=(0.9, 0.95))
    # è®¡ç®—æ¯ä¸ªepochçš„æ›´æ–°æ­¥æ•°ã€‚
    num_update_steps_per_epoch = math.ceil(
        len(train_dataloader) / args.gradient_accumulation_steps)
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=args.num_warmup_steps,
        num_training_steps=args.num_train_epochs * num_update_steps_per_epoch,
    )
    # ä½¿ç”¨deepspeedåˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
    model, optimizer, _, lr_scheduler = deepspeed.initialize(
        model=model,
        optimizer=optimizer,
        args=args,
        config=ds_config,
        lr_scheduler=lr_scheduler,
        dist_init_required=True)
    # å¦‚æœå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œé‚£ä¹ˆåœ¨æ¨¡å‹ä¸­ä¹Ÿå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚
    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()

    # Train!
    # ä½¿ç”¨ print_rank_0 å‡½æ•°åœ¨ä¸»èŠ‚ç‚¹ï¼ˆglobal_rankä¸º0çš„èŠ‚ç‚¹ï¼‰æ‰“å°å¼€å§‹è®­ç»ƒçš„ä¿¡æ¯ã€‚
    print_rank_0("***** Running training *****", args.global_rank)
    # åœ¨ä¸»èŠ‚ç‚¹æ‰“å°åœ¨ç¬¬0ä¸ªepochï¼ˆè®­ç»ƒå¼€å§‹å‰ï¼‰è¿›è¡Œæ¨¡å‹è¯„ä¼°çš„ä¿¡æ¯ã€‚
    print_rank_0(
        f"***** Evaluating perplexity, Epoch {0}/{args.num_train_epochs} *****",
        args.global_rank)
    # è°ƒç”¨ evaluation å‡½æ•°å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¾—åˆ°æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚
    perplexity = evaluation(model, eval_dataloader)
    # åœ¨ä¸»èŠ‚ç‚¹æ‰“å°æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚
    print_rank_0(f"ppl: {perplexity}", args.global_rank)
    
    # å¾ªç¯ args.num_train_epochs è½®è¿›è¡Œè®­ç»ƒã€‚
    for epoch in range(args.num_train_epochs):
        print_rank_0(
            f"Beginning of Epoch {epoch+1}/{args.num_train_epochs}, Total Micro Batches {len(train_dataloader)}",
            args.global_rank) # åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶ï¼Œåœ¨ä¸»èŠ‚ç‚¹æ‰“å°å¼€å§‹æ–°çš„è®­ç»ƒå‘¨æœŸçš„ä¿¡æ¯ã€‚
        model.train() # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ã€‚
        for step, batch in enumerate(train_dataloader): # å¯¹äºè®­ç»ƒæ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ªbatchã€‚
            batch = to_device(batch, device) # å°†batchæ•°æ®ç§»åˆ°å¯¹åº”çš„è®¾å¤‡ä¸Šã€‚
            outputs = model(**batch, use_cache=False) # å°†batchæ•°æ®è¾“å…¥æ¨¡å‹ï¼Œè¿›è¡Œå‰å‘è®¡ç®—ã€‚
            loss = outputs.loss # å–å‡ºæ¨¡å‹çš„è¾“å‡ºä¸­çš„lossã€‚
            model.backward(loss) # è¿›è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦ã€‚
            model.step() # æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚

        # Evaluate perplexity on the validation set.
        # åœ¨æ¯ä¸ªepochç»“æŸåï¼Œåœ¨ä¸»èŠ‚ç‚¹æ‰“å°å¼€å§‹è¯„ä¼°çš„ä¿¡æ¯ã€‚
        print_rank_0(
            f"***** Evaluating perplexity, Epoch {epoch+1}/{args.num_train_epochs} *****",
            args.global_rank)
        # å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¾—åˆ°æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚
        perplexity = evaluation(model, eval_dataloader)
        # åœ¨ä¸»èŠ‚ç‚¹æ‰“å°æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚
        print_rank_0(f"ppl: {perplexity}", args.global_rank)
        # æ›´æ–°æ¨¡å‹ä¸­çš„epochè®¡æ•°ã€‚
        model.tput_timer.update_epoch_count()
    
    # å¦‚æœè®¾ç½®äº†è¾“å‡ºç›®å½•ï¼Œè¿›è¡Œä»¥ä¸‹æ“ä½œã€‚
    if args.output_dir is not None:
    	 # åœ¨ä¸»èŠ‚ç‚¹æ‰“å°å¼€å§‹ä¿å­˜æ¨¡å‹çš„ä¿¡æ¯ã€‚
        print_rank_0('saving the final model ...', args.global_rank)
        # å°†æ¨¡å‹ä¸­çš„LoRAå±‚è½¬æ¢ä¸ºçº¿æ€§å±‚ã€‚
        model = convert_lora_to_linear_layer(model)
        
        # å¦‚æœæ˜¯ä¸»èŠ‚ç‚¹ï¼Œè¿›è¡Œä»¥ä¸‹æ“ä½œã€‚
        if args.global_rank == 0:
            # ä»¥Hugging Faceçš„æ¨¡å‹æ ¼å¼ä¿å­˜æ¨¡å‹ã€‚
            save_hf_format(model, tokenizer, args)
        # å¦‚æœä½¿ç”¨äº†Zero Redundancy Optimizerï¼ˆZeroï¼‰çš„ç¬¬ä¸‰é˜¶æ®µï¼Œè¿›è¡Œä»¥ä¸‹æ“ä½œã€‚
        if args.zero_stage == 3:
            # For zero stage 3, each gpu only has a part of the model, so we need a special save function
            # ä½¿ç”¨ç‰¹æ®Šçš„ä¿å­˜å‡½æ•°ä¿å­˜æ¨¡å‹ã€‚åœ¨Zeroçš„ç¬¬ä¸‰é˜¶æ®µï¼Œæ¯ä¸ªGPUåªæœ‰æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥éœ€è¦ç‰¹æ®Šçš„ä¿å­˜å‡½æ•°ã€‚
            save_zero_three_model(model,
                                  args.global_rank,
                                  args.output_dir,
                                  zero_stage=args.zero_stage)
```

# 0x4. hybrid_engineçš„ç»†èŠ‚ & log
ä»è®­ç»ƒè¿‡ç¨‹çš„è¾“å‡ºæ—¥å¿—æ¥çœ‹hybrid_engineæ˜¯é»˜è®¤å…³é—­çš„ï¼Œ[DeepSpeed-Chat æ‰“é€ ç±»ChatGPTå…¨æµç¨‹ ç¬”è®°ä¸€](https://mp.weixin.qq.com/s/QQC5ZxlyqqnK52TthqZdjw) é‡Œé¢æåˆ°DeepSpeed Hybrid Engineæ˜¯ç”¨åœ¨åŠ é€Ÿ RLHF æµç¨‹ä¸­æœ€è€—æ—¶çš„éƒ¨åˆ†ä¹Ÿå°±æ˜¯ç¬¬ä¸‰æ­¥ï¼Œè€Œæœ¬æ–‡ä»‹ç»çš„ç›‘ç£æŒ‡ä»¤å¾®è°ƒæ˜¯ç¬¬ä¸€æ­¥ï¼Œæ‰€ä»¥å³ä½¿å¼€å¯hybrid_engineåŠ é€Ÿæ•ˆæœåº”è¯¥ä¹Ÿæ¯”è¾ƒæœ‰é™ï¼Œæ‰€ä»¥è¿™é‡Œé»˜è®¤å…³é—­ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/71c495aee7644403b5f94ed19d796837.png)
hybrid_engineçš„ä¼˜åŒ–æ–¹æ³•å’ŒåŸç†åœ¨åç»­æ–‡ç« ä¸­ç»§ç»­æ¢ç´¢ã€‚

è¿™é‡Œåˆ†äº«ä¸€ä¸‹æˆ‘å¤ç°å®˜æ–¹sampleè®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µçš„logï¼š**https://paste.ubuntu.com/p/vcG49hQmCW/**

# 0x5. æ€»ç»“
è¿™ç¯‡æ–‡ç« è§£æäº†DeepSpeed Chatä¸­ç›‘ç£æŒ‡ä»¤å¾®è°ƒè¿™ä¸ªè¿‡ç¨‹çš„æºç ï¼Œè¿™ä¸ªè¿‡ç¨‹å’Œä¸€èˆ¬çš„PyTorch DDPåˆ†å¸ƒå¼è®­ç»ƒåŒºåˆ«ä¸æ˜¯ç‰¹åˆ«å¤§ï¼Œä¸»è¦æ˜¯è‡ªå®šä¹‰promptæ•°æ®é›†ä»¥åŠå°†æ™®é€šçš„è®­ç»ƒæµç¨‹ä¸­çš„ç»„ä»¶å¦‚æ¨¡å‹ï¼Œä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡è°ƒåº¦å™¨ç­‰ç­‰ï¼Œä½¿ç”¨DeepSpeedæ¥warpä¸€ä¸‹ï¼Œæ¥ç”¨ä¸ŠDeepSpeedæä¾›çš„Zeroï¼ŒGradient Checkpointï¼ˆæ³¨æ„è¿™ä¸ªå…¶å®å°±æ˜¯activation checkpointï¼‰ç­‰ç‰¹æ€§ã€‚æœ¬æ–‡æ˜¯å®Œå…¨æŒ‰ç…§è®­ç»ƒæµç¨‹é¡ºåºé˜…è¯»ä»£ç ï¼Œå¹¶è¡¥å…¨äº†è®­ç»ƒè¿‡ç¨‹ä¸­æ‰€æœ‰æ¶‰åŠåˆ°çš„å·¥å…·å‡½æ•°æˆ–è€…æ–°çš„ç‰¹æ€§å¦‚LoRAå¾®è°ƒçš„ä»£ç è§£æã€‚DeepSpeed Chatè¿™éƒ¨åˆ†ä»£ç å†™å¾—æ¯”è¾ƒæ¸…æ™°æ˜“æ‡‚ï¼Œå› ä¸ºæ˜¯åœ¨æ¥å£å±‚é¢æ¥ä½¿ç”¨DeepSpeedï¼Œç›¸å½“äºåŸºäºDeepSpeedåšåº”ç”¨æ‰€ä»¥ä»£ç ä¸­ä¸ä¼šæ¶‰åŠåˆ°DeepSpeedçš„åº•å±‚ä»£ç ï¼Œåªéœ€è¦å…³æ³¨ç®—æ³•æµç¨‹ã€‚ä½†è¿™ä¸ªä»£ç åœ¨LoRAå¾®è°ƒè¿™éƒ¨åˆ†æ„Ÿè§‰è®¾è®¡çš„è€¦åˆæ€§æœ‰ä¸€ç‚¹é«˜ï¼Œå¦‚æœè¦æ–°å¢æ–°çš„å¾®è°ƒæ–¹å¼æ¯”å¦‚QLoRAå¯èƒ½å†™æ³•å°±ä¸å¤ªä¼˜é›…äº†ã€‚




