## å¦‚ä½•ä½¿ç”¨â€œLoRaâ€çš„æ–¹å¼åŠ è½½Onnxæ¨¡å‹ï¼šStableDiffusionç›¸å…³æ¨¡å‹çš„C++æ¨ç†

æœ¬æ–‡ä¸»è¦å¹²äº†ä»¥ä¸‹å‡ ä¸ªäº‹ï¼š

1.åŸºäº onnxruntimeï¼Œå°† StableDiffusionInpaintPipelineã€StableDiffusionControlNetImg2ImgPipeline(stablediffusion + controlnet + LoRa) C++å·¥ç¨‹åŒ–ï¼›

2.è¾“å‡ºä¸€ä¸ª C++ç‰ˆæœ¬çš„ ddim-schduler åº“ï¼›

3.æä¾›ä¸€ç§â€œLoRaâ€çš„ onnx æ¨¡å‹åŠ è½½æ–¹å¼ï¼›

4.æ‰€æœ‰ç›¸å…³ä»£ç ã€æ¨¡å‹å¼€æº

> é¡¹ç›®åœ°å€: https://github.com/TalkUHulk/ai.deploy.box

> æ¨¡å‹åœ°å€: https://huggingface.co/TalkUHulk/AiDB 

## StableDiffusionInpaint

### æ¨¡å‹å¯¼å‡º

StableDiffusionInpaint çš„ onnx å¯¼å‡ºéå¸¸ç®€å•ï¼Œoptimum å·²ç»åšå¥½äº†é›†æˆï¼Œæ”¯æŒå‘½ä»¤è¡Œç›´æ¥å¯¼å‡ºï¼Œå…·ä½“å‚è€ƒå¯å‚è€ƒoptimum-cliï¼š

```shell
optimum-cli export onnx â€”task stable-diffusion  â€”model stable-diffusion-inpainting stable-diffusion-inpainting-onnx
```

è¿™æ ·å¾—åˆ°äº†å››ä¸ª onnx æ¨¡å‹ï¼ˆunetã€ vae encoderã€decoder å’Œ text encoderï¼‰ã€‚

### tokenizer&scheduler

ä¸æ£€æµ‹ã€åˆ†ç±»ç­‰ä¼ ç»Ÿ cv æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å¦‚æœæƒ³åœ¨ c++ä¸­ä¸²èµ·æ•´ä¸ª pipelineï¼Œè¿˜ç¼ºå°‘ c++ç‰ˆæœ¬çš„ tokenizer å’Œ schedulerã€‚æœ‰å¾ˆå¤šä¼˜ç§€çš„å¼€æº C++ç‰ˆæœ¬çš„ tokenizerï¼Œè¿™é‡Œæˆ‘é€‰ç”¨äº†tokenizers_cppï¼Œåœ°å€ï¼šhttps://github.com/mlc-ai/tokenizers-cppã€‚tokenizers-cpp æ¥å£ç®€å•ï¼Œå¹¶ä¸”å¯ç›´æ¥ä½¿ç”¨ ğŸ¤—hugging face ä¸­å¼€æºçš„çš„ tokenizer.json é…ç½®æ–‡ä»¶ã€‚

```cpp
auto tokenizer = Tokenizer::FromBlobJSON(
            LoadBytesFromFile("./tokenizers/tokenizer.json"));
std::string startoftext = "<|startoftext|>";
std::string endoftext = "<|endoftext|>";
std::string prompt = startoftext + "a lovely girl" + endoftext;
std::vector<int> text_input_ids = tokenizer->Encode(prompt);
```

è€Œå¯¹äº schedulerï¼Œç›®å‰æ²¡æ‰¾åˆ°å¾ˆå¥½ç”¨çš„ c++ç‰ˆæœ¬ï¼Œæ‰€ä»¥ä½œè€…å®ç°äº†ä¸€ä¸ª C++ç‰ˆæœ¬çš„ ddim_schedulerï¼Œå¹¶åšäº†å¼€æºddim_scheduler_cppï¼Œrepåœ°å€ï¼šhttps://github.com/TalkUHulk/ddim_scheduler_cppã€‚ddim_scheduler_cpp åº•å±‚åŸºäº Eigen å®ç°ï¼Œä¸ diffusers æ¥å£ä¿æŒä¸€è‡´ï¼Œå¯ç›´æ¥æ›¿æ¢ã€‚

```cpp
// init from json
auto scheduler = DDIMScheduler("scheduler_config.json");

// set num_inference_steps
scheduler.set_timesteps(10);

// get timesteps
std::vector<int> timesteps;
scheduler.get_timesteps(timesteps);

// random init for example
std::vector<float> sample(1 * 4 * 64 * 64);
std::vector<float> model_output(1 * 4 * 64 * 64);

for(int i = 0; i < 4 * 64 * 64; i++){
    sample[i] = distribution(generator);
    model_output[i] = distribution(generator);
}

// step
std::vector<float> pred_sample;
for(auto t: timesteps){
    scheduler.step(model_output, {1, 4, 3, 3}, sample, {1, 4, 3, 3}, pred_sample, t);
}

```

### C++æ¨ç†

ç›®å‰ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å¿…é¡»çš„ C++ç‰©æ–™éƒ½é›†é½äº†ã€‚å€ŸåŠ©ä½œè€…ä¹‹å‰å¼€æºçš„ä¸€ä¸ªå¼€æºå·¥å…·AiDBï¼ˆ[repåœ°å€](https://mp.weixin.qq.com/s/D3mj9Dj2nmqeUIJMy8BIag)ï¼‰ï¼Œåªéœ€è¦ç®€å•é…ç½®ï¼Œç›´æ¥å¯ä»¥ä½¿ç”¨ C++åŠ è½½å¹¶æ¨ç† onnx æ¨¡å‹ã€‚

```cpp
auto scheduler = Scheduler::DDIMScheduler("scheduler_config.json");
auto tokenizer = Tokenizer::FromBlobJSON(
        LoadBytesFromFile("tokenizer.json"));
std::string startoftext = "<|startoftext|>";
std::string endoftext = "<|endoftext|>";
std::string prompt = startoftext + "A cute cat" + endoftext;

std::vector<int> text_input_ids = tokenizer->Encode(prompt);

std::string uncond_tokens = startoftext + "" + endoftext;

std::vector<int> uncond_input = tokenizer->Encode(uncond_tokens);

auto text_enc = AIDB::Interpreter::createInstance("text_encoder", "onnx");

std::vector<std::vector<float>>  prompt_embeds;
std::vector<std::vector<int>>  prompt_embeds_shape;

text_enc->forward(text_input_ids.data(), 77, 0, 0,  prompt_embeds, prompt_embeds_shape);

std::vector<std::vector<float>>  negative_prompt_embeds;
std::vector<std::vector<int>>  negative_prompt_embeds_shape;
text_enc->forward(uncond_input.data(), 77, 0, 0,  negative_prompt_embeds, negative_prompt_embeds_shape);

std::vector<float> prompt_embeds_cat(2 * 77 * 768, 0);
memcpy(prompt_embeds_cat.data(), negative_prompt_embeds[0].data(), 77 * 768 * sizeof(float));
memcpy(prompt_embeds_cat.data() + 77 * 768, prompt_embeds[0].data(), 77 * 768 * sizeof(float));

auto num_inference_steps = 10;
scheduler.set_timesteps(num_inference_steps);
std::vector<int> timesteps;
scheduler.get_timesteps(timesteps);

auto vae_enc = AIDB::Interpreter::createInstance("sd_inpaint_vae_encoder", "onnx");
auto vae_dec = AIDB::Interpreter::createInstance("sd_inpaint_vae_decoder", "onnx");
auto unet = AIDB::Interpreter::createInstance("sd_inpaint_unet", "onnx");

std::vector<float> latents(1 * 4 * 64 * 64);

AIDB::Utility::randn(latents.data(), latents.size());

auto image = cv::imread("dog.png");
auto mask = cv::imread("dog_mask.png", 0);

// å›¾åƒé¢„å¤„ç†
int target = 512;
float src_ratio = float(image.cols) / float(image.rows);
float target_ratio = 1.0f;

int n_w, n_h, pad_w = 0, pad_h = 0;
float _scale_h, _scale_w;

if(src_ratio > target_ratio){
    n_w = target;

    n_h = floor(float(n_w) / float(image.cols) * float(image.rows) + 0.5f);
    pad_h = target - n_h;
    _scale_h = _scale_w = float(n_w) / float(image.cols);
} else if(src_ratio < target_ratio){
    n_h = target;
    n_w = floor(float(n_h) / float(image.rows) * float(image.cols) + 0.5f);
    pad_w = target - n_w;
    _scale_h = _scale_w = float(n_h) / float(image.rows);
} else{
    n_w = target;
    n_h = target;
    _scale_h = _scale_w = float(n_w) / float(image.cols);
}

cv::resize(image, image, cv::Size(n_w, n_h));
cv::copyMakeBorder(image, image, 0, pad_h, 0, pad_w, cv::BORDER_CONSTANT, cv::Scalar(255, 255, 255));

cv::resize(mask, mask, cv::Size(n_w, n_h));
cv::copyMakeBorder(mask, mask, 0, pad_h, 0, pad_w, cv::BORDER_CONSTANT, cv::Scalar(255, 255, 255));

cv::threshold(mask, mask, 127.5, 1, cv::THRESH_BINARY);

cv::cvtColor(image, image, cv::COLOR_BGR2RGB);
image.convertTo(image, CV_32F);
image = image / 127.5 - 1.0;
cv::Mat mask_image = cv::Mat::zeros(image.rows, image.cols, CV_32FC3);
image.copyTo(mask_image, 1 - mask);

cv::Mat blob;
cv::Mat blob_mask;
cv::dnn::blobFromImage(mask_image, blob);

cv::dnn::blobFromImage(mask, blob_mask, 1.0f, cv::Size(64, 64));

std::vector<std::vector<float>>  masked_image_latents;
std::vector<std::vector<int>>  masked_image_latents_shape;

vae_enc->forward(blob.data, 512, 512, 3, masked_image_latents, masked_image_latents_shape);

auto scaling_factor = 0.18215f;
std::for_each(masked_image_latents.begin(), masked_image_latents.end(),
              [=](std::vector<float>& masked_image_latent) {
                  std::for_each(masked_image_latent.begin(), masked_image_latent.end(), [=](float &item){ item *= scaling_factor;});
              }
);

auto init_noise_sigma = scheduler.get_init_noise_sigma();
std::for_each(latents.begin(), latents.end(), [=](float &item){item*=init_noise_sigma;});
auto guidance_scale = 7.5f;
int step = 0;
// å¾ªç¯å¤„ç†
for(auto t: timesteps){
    auto tic = std::chrono::system_clock::now();

    std::vector<float> latent_model_input(2 * 9 * 64 * 64, 0);
    memcpy(latent_model_input.data(), latents.data(), 4 * 64 * 64 * sizeof(float));
    memcpy(latent_model_input.data() + 4 * 64 * 64, blob_mask.data, 1 * 64 * 64 * sizeof(float));
    memcpy(latent_model_input.data() + 5 * 64 * 64, masked_image_latents[0].data(), 4 * 64 * 64 * sizeof(float));
    memcpy(latent_model_input.data() + 9 * 64 * 64, latent_model_input.data(), 9 * 64 * 64 * sizeof(float));

    std::vector<std::vector<float>> noise_preds;
    std::vector<std::vector<int>>  noise_preds_shape;
    std::vector<void *> input;
    std::vector<std::vector<int>> input_shape;

    input.push_back(latent_model_input.data());
    input_shape.push_back({2, 9, 64, 64});

    std::vector<long long> timestep = {(long long)t};
    input.push_back(timestep.data());
    input_shape.push_back({1});

    input.push_back(prompt_embeds_cat.data());
    input_shape.push_back({2, 77, 768});

    unet->forward(input, input_shape, noise_preds, noise_preds_shape);

    // noise_preds [2,4,64,64] noise_pred_uncond | noise_pred_text
    std::vector<float> noise_pred(1 * 4 * 64 * 64, 0);
    for(int i = 0; i < noise_pred.size(); i++){
        noise_pred[i] = noise_preds[0][i] + guidance_scale * (noise_preds[0][i + 4 * 64 * 64] - noise_preds[0][i]);
    }
    std::vector<float> pred_sample;
    scheduler.step(noise_pred, {1, 4, 64, 64}, latents, {1, 4, 64, 64}, pred_sample, t);
    latents.clear();
    latents.assign(pred_sample.begin(), pred_sample.end());

    auto toc = std::chrono::system_clock::now();
    std::chrono::duration<double> elapsed = toc - tic;
    // è®¡ç®—å¹¶è¾“å‡ºè¿›åº¦ç™¾åˆ†æ¯”
    std::cout << "\rStep " << step++ << " " <<std::fixed << std::setprecision(1) << float(step) / timesteps.size() * 100
              << "% " << std::fixed << std::setprecision(3) << 1.0f / elapsed.count() << "b/s"
              << " [" << std::setw(float(step) / (float)timesteps.size() * 30) << std::setfill('=') << '>' << ']';
    std::flush(std::cout);
    }

std::for_each(latents.begin(), latents.end(), [=](float &item){item /= scaling_factor;});

std::vector<std::vector<float>>  sample;
std::vector<std::vector<int>>  sample_shape;
vae_dec->forward(latents.data(), 64, 64, 4, sample, sample_shape);
cv::Mat sd_image;
AIDB::Utility::stable_diffusion_process(sample[0].data(), sample_shape[0][2], sample_shape[0][3], sample_shape[0][1], sd_image);

cv::imwrite("stable_diffusion_inpainting.jpg", sd_image);

```

![](https://files.mdnice.com/user/48619/93f36c70-2854-4f6a-ba16-057668aef724.jpg)

è‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸæ­å»ºèµ· stablediffusioninpaint çš„ C++ pipelineï¼Œä½†æ›´å¸¸ç”¨ã€æ›´æœ‰è¶£çš„æ˜¯ controlnet å’Œ lora ä¸ stablediffusion çš„ç»“åˆã€‚ä¸‹é¢æˆ‘ä»¬å°è¯•æ­å»º StableDiffusionControlNetImg2ImgPipeline çš„ C++æ¨ç†ä»£ç ï¼Œå¹¶æ”¯æŒ LoRa åŠ è½½ã€‚

## StableDiffusionControlNetImg2ImgPipeline

### æ¨¡å‹å¯¼å‡º

ç›®å‰ optimum è¿˜æœªæä¾› stablediffusion + controlnet +LoRa çš„ onnx æ¨¡å‹å¯¼å‡ºé€‰é¡¹ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬å…ˆå°†æ¨¡å‹å¯¼å‡ºã€‚
è¿™é‡Œæˆ‘ä»¬æœ‰ä¸¤ç§å¯¼å‡ºæ–¹æ¡ˆï¼Œåˆ†åˆ«å¯¼å‡º controlNet å’Œ Unetï¼Œä»¥åŠå°†äºŒè€…åˆå¹¶ä¸ºä¸€ä¸ªæ¨¡å‹ã€‚

![](https://files.mdnice.com/user/48619/b92dc0ac-48fe-45a5-a052-258b5b09c324.png)

å…ˆçœ‹ä¸€ä¸‹ controlNet çš„æ•´ä½“æ¶æ„ï¼ŒcontrolNet å’Œ Unet çš„è€¦åˆæ¯”è¾ƒæ·±ï¼Œå¦‚æœæˆ‘ä»¬åˆ†å¼€å¯¼å‡ºï¼Œä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºå’Œè¾“å…¥æ•°é‡éƒ½ä¼šéå¸¸å¤šï¼Œæ¯”å…¥ Unet éƒ¨åˆ†æœ‰ down_block_res_sample_0 ï½ down_block_res_sample_11ã€mid_block_res_sample ç­‰ 16 ä¸ªè¾“å…¥ï¼Œè¿™æ ·åœ¨å†™ inference ä»£ç çš„æ—¶å€™å°±ä¼šæ¯”è¾ƒç¹çã€‚æ‰€ä»¥æˆ‘ä»¬é€‰æ‹©å°†ä¸¤ä¸ªæ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªã€‚ä½†è¿™æ ·ä¹Ÿæœ‰æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚æˆ‘é¦–å…ˆä½¿ç”¨ controlNet-canny å¯¼å‡ºäº† onnx æ¨¡å‹ï¼ŒåŒæ—¶åˆæƒ³ä½¿ç”¨ controlNet-hedï¼Œé‚£ unet éƒ¨åˆ†æ˜¯ä¸æ˜¯è¦é‡å¤å¯¼å‡ºï¼Ÿè¿™é‡Œæœ‰å‡ ä¸ªæ–¹æ³•è§£å†³ï¼Œæˆ‘ä»¬åé¢å†è¯´æ˜ã€‚

æ­¤å¤„ä½¿ç”¨Yoji Shinkawa Style LoRAï¼ˆğŸ¤— https://civitai.com/models/12324/yoji-shinkawa-style-lora)

å¯¼å‡ºä»£ç ï¼š

```python
from diffusers import (
  ControlNetModel,
  StableDiffusionControlNetImg2ImgPipeline,
)
from diffusers.models.attention_processor import AttnProcessor

is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse("1.11")
is_torch_2_0_1 = version.parse(version.parse(torch.__version__).base_version) == version.parse("2.0.1")

class UNetControlNetModel(torch.nn.Module):
  def __init__(
          self,
          unet,
          controlnet: ControlNetModel,
  ):
      super().__init__()
      self.unet = unet
      self.controlnet = controlnet

  def forward(
          self,
          sample,
          timestep,
          encoder_hidden_states,
          controlnet_cond,
          conditioning_scale,
  ):
      for i, (_controlnet_cond, _conditioning_scale) in enumerate(
              zip(controlnet_cond, conditioning_scale)
      ):
          down_samples, mid_sample = self.controlnet(
              sample,
              timestep,
              encoder_hidden_states=encoder_hidden_states,
              controlnet_cond=_controlnet_cond,
              conditioning_scale=_conditioning_scale,
              return_dict=False,
          )

          # merge samples
          if i == 0:
              down_block_res_samples, mid_block_res_sample = down_samples, mid_sample
          else:
              down_block_res_samples = [
                  samples_prev + samples_curr
                  for samples_prev, samples_curr in zip(down_block_res_samples, down_samples)
              ]
              mid_block_res_sample += mid_sample

      noise_pred = self.unet(
          sample,
          timestep,
          encoder_hidden_states=encoder_hidden_states,
          down_block_additional_residuals=down_block_res_samples,
          mid_block_additional_residual=mid_block_res_sample,
          return_dict=False,
      )[0]
      return noise_pred

def onnx_export(
      model,
      model_args: tuple,
      output_path: Path,
      ordered_input_names,
      output_names,
      dynamic_axes,
      opset,
      use_external_data_format=False,
):
 output_path.parent.mkdir(parents=True, exist_ok=True)
 with torch.inference_mode(), torch.autocast("cuda"):
  if is_torch_less_than_1_11:
      export(
          model,
          model_args,
          f=output_path.as_posix(),
          input_names=ordered_input_names,
          output_names=output_names,
          dynamic_axes=dynamic_axes,
          do_constant_folding=True,
          use_external_data_format=use_external_data_format,
          enable_onnx_checker=True,
          opset_version=opset,
      )
  else:
      export(
          model,
          model_args,
          f=output_path.as_posix(),
          input_names=ordered_input_names,
          output_names=output_names,
          dynamic_axes=dynamic_axes,
          do_constant_folding=True,
          opset_version=opset,
      )

with torch.no_grad():
  dtype = torch.float32
  device = "cpu"
   # init controlnet
  controlnet = ControlNetModel.from_pretrained("sd-controlnet-canny", torch_dtype=dtype).to(device)
  if is_torch_2_0_1:
      controlnet.set_attn_processor(AttnProcessor())

  pipeline = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
      "stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=dtype, safety_checker=None
  ).to(device)

  pipeline.load_lora_weights("stable-diffusion-v1-5/LoRa/", "shinkawa_youji_offset")
  output_path = Path("exp_lora")
  if is_torch_2_0_1:
      pipeline.unet.set_attn_processor(AttnProcessor())
      pipeline.vae.set_attn_processor(AttnProcessor())

  # # TEXT ENCODER
  num_tokens = pipeline.text_encoder.config.max_position_embeddings
  text_hidden_size = pipeline.text_encoder.config.hidden_size
  text_input = pipeline.tokenizer(
      "A sample prompt",
      padding="max_length",
      max_length=pipeline.tokenizer.model_max_length,
      truncation=True,
      return_tensors="pt",
  )
  onnx_export(
      pipeline.text_encoder,
      # casting to torch.int32 until the CLIP fix is released: https://github.com/huggingface/transformers/pull/18515/files
      model_args=(text_input.input_ids.to(device=device, dtype=torch.int32)),
      output_path=output_path / "text_encoder" / "model.onnx",
      ordered_input_names=["input_ids"],
      output_names=["last_hidden_state", "pooler_output"],
      dynamic_axes={
          "input_ids": {0: "batch", 1: "sequence"},
      },
      opset=14,
  )
  del pipeline.text_encoder

  ## VAE ENCODER
  vae_encoder = pipeline.vae
  vae_in_channels = vae_encoder.config.in_channels
  vae_sample_size = vae_encoder.config.sample_size
  # need to get the raw tensor output (sample) from the encoder
  vae_encoder.forward = lambda sample: vae_encoder.encode(sample).latent_dist.sample()

  onnx_export(
      vae_encoder,
      model_args=(torch.randn(1, vae_in_channels, vae_sample_size, vae_sample_size).to(device=device, dtype=dtype),),
      output_path=output_path / "vae_encoder" / "model.onnx",
      ordered_input_names=["sample"],
      output_names=["latent_sample"],
      dynamic_axes={
          "sample": {0: "batch", 1: "channels", 2: "height", 3: "width"},
      },
      opset=14,
  )

  # # UNET
  unet_controlnet = UNet2DConditionControlNetModel(pipeline.unet, controlnet)
  unet_in_channels = pipeline.unet.config.in_channels
  unet_sample_size = pipeline.unet.config.sample_size
  num_tokens = pipeline.text_encoder.config.max_position_embeddings
  text_hidden_size = pipeline.text_encoder.config.hidden_size

  img_size = 8 * unet_sample_size
  unet_path = output_path / "unet" / "model.onnx"

  onnx_export(
      unet_controlnet,
      model_args=(
          torch.randn(2, unet_in_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype),
          torch.tensor([1.0]).to(device=device, dtype=dtype),
          torch.randn(2, num_tokens, text_hidden_size).to(device=device, dtype=dtype),
          torch.randn(2, 3, img_size, img_size).to(device=device, dtype=dtype),
          torch.tensor([1.0]).to(device=device, dtype=dtype),
      ),
      output_path=unet_path,
      ordered_input_names=[
          "sample",
          "timestep",
          "encoder_hidden_states",
          "controlnet_cond",
          "conditioning_scale",
      ],
      output_names=["noise_pred"],  # has to be different from "sample" for correct tracing
      dynamic_axes={
          "sample": {0: "2B", 2: "H", 3: "W"},
          "encoder_hidden_states": {0: "2B"},
          "controlnet_cond": {0: "2B", 2: "8H", 3: "8W"},
      },
      opset=14,
      use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split
  )

  unet_model_path = str(unet_path.absolute().as_posix())
  unet_opt_graph = onnx.load(unet_model_path)
  onnx.save_model(
      unet_opt_graph,
      unet_model_path,
      save_as_external_data=True,
      all_tensors_to_one_file=True,
      location="model.onnx_data",
      convert_attribute=False,
  )
  del pipeline.unet

  # VAE DECODER
  vae_decoder = pipeline.vae
  vae_latent_channels = vae_decoder.config.latent_channels
  # forward only through the decoder part
  vae_decoder.forward = vae_encoder.decode
  onnx_export(
      vae_decoder,
      model_args=(
          torch.randn(1, vae_latent_channels, unet_sample_size, unet_sample_size).to(device=device, dtype=dtype),
      ),
      output_path=output_path / "vae_decoder" / "model.onnx",
      ordered_input_names=["latent_sample"],
      output_names=["sample"],
      dynamic_axes={
          "latent_sample": {0: "batch", 1: "channels", 2: "height", 3: "width"},
      },
      opset=14,
  )

```

è¿™é‡Œæœ‰å‡ ä¸ªç‚¹éœ€è¦æ³¨æ„ã€‚

#### OP é—®é¢˜

pytorch2.0 ä»¥ä¸Šï¼Œéœ€è¦åšä»¥ä¸‹è®¾ç½®æ‰å¯ä»¥æˆåŠŸå¯¼å‡º

```python
pipeline.unet.set_attn_processor(AttnProcessor())
pipeline.vae.set_attn_processor(AttnProcessor())
controlnet.set_attn_processor(AttnProcessor())
```

å…·ä½“å¯ä»¥å‚è€ƒ diffusers->models->attention_processor.py ä¸­çš„ç›¸å…³ä»£ç ã€‚Pytorch2.0 ä»¥ä¸Š scaled dot-product attention è®¡ç®—ä¼šé»˜è®¤ä½¿ç”¨torch.nn.functional.scaled_dot_product_attentionï¼Œè€Œ onnx å¯¼å‡ºæ—¶ä¸æ”¯æŒè¯¥ OPã€‚

```
torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::scaled_dot_product_attention' to ONNX opset version 14 is not supported.
```

å› æ­¤éœ€è¦åšæ›¿æ¢ï¼Œdiffusers å¾ˆè´´å¿ƒçš„æŠŠç›¸å…³ä»£ç å®ç°å¥½ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨å³å¯ã€‚

#### æ¨¡å‹å¤§å°>2GB

ONNX æ¨¡å‹æœ¬è´¨å°±æ˜¯ä¸€ä¸ª Protobuf åºåˆ—åŒ–åçš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œè€Œ Protobuf çš„æ–‡ä»¶å¤§å°é™åˆ¶ä¸º 2GBã€‚å› æ­¤å¯¹äº Unet ç›¸å…³æ¨¡å‹æ¥è¯´ï¼Œå­˜å‚¨å¤§å°å·²ç»è¶…è¿‡äº†é™åˆ¶ã€‚onnx ç»™å‡ºçš„æ–¹æ¡ˆæ˜¯å•ç‹¬å­˜å‚¨ weightsã€bias è¿™äº›æƒé‡ã€‚ è¿™é‡Œåšä¸‹è¯¦ç»†è¯´æ˜ã€‚
å…ˆæ¥çœ‹ä¸‹onnx.proto(æ–‡ä»¶åœ°å€ï¼šhttps://github.com/onnx/onnx/blob/main/onnx/onnx.proto)ä¸­çš„å®šä¹‰ï¼š

```
message TensorProto {
....
repeated int64 dims = 1;
optional int32 data_type = 2;
....
// Data can be stored inside the protobuf file using type-specific fields or raw_data.
// Alternatively, raw bytes data can be stored in an external file, using the external_data field.
// external_data stores key-value pairs describing data location. Recognized keys are:
// - "location" (required) - POSIX filesystem path relative to the directory where the ONNX
//                           protobuf model was stored
// - "offset" (optional) - position of byte at which stored data begins. Integer stored as string.
//                         Offset values SHOULD be multiples 4096 (page size) to enable mmap support.
// - "length" (optional) - number of bytes containing data. Integer stored as string.
// - "checksum" (optional) - SHA1 digest of file specified in under 'location' key.
repeated StringStringEntryProto external_data = 13;

// Location of the data for this tensor. MUST be one of:
// - DEFAULT - data stored inside the protobuf message. Data is stored in raw_data (if set) otherwise in type-specified field.
// - EXTERNAL - data stored in an external location as described by external_data field.
enum DataLocation {
  DEFAULT = 0;
  EXTERNAL = 1;
}
// If value not set, data is stored in raw_data (if set) otherwise in type-specified field.
optional DataLocation data_location = 14;
}
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡ data_location æ¥åˆ¤æ–­æŸä¸ªå‚æ•°çš„ä½ç½®ï¼Œç„¶åè¯»å– external_data å‚æ•°åŠ è½½æƒé‡ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬åœ¨ä»£ç ä¸­æ‰‹åŠ¨åŠ è½½ï¼š

```python
import onnx
from onnx.external_data_helper import  _get_all_tensors

onnx_model = onnx.load("unet/model.onnx", load_external_data=False)
for tensor in _get_all_tensors(onnx_model):
    if tensor.HasField("data_location") and tensor.data_location == onnx.TensorProto.EXTERNAL:
        info = {}
        for item in tensor.external_data:
            info[item.key] = item.value
        with open(info["location"], "rb") as data_file:
            data_file.seek(info["offset"])
            tensor.raw_data = data_file.read(info["length"])

```

æ‘˜å‡ºå…¶ä¸­ä¸€ä¸ª tensor çš„ external_data è¯¦ç»†è¯´æ˜ï¼š

```
[   key: "location"
    value: "model.onnx_data"
    , key: "offset"
    value: "0"
    , key: "length"
    value: "46080"
]
```

location è®°å½•äº†æƒé‡å­˜å‚¨çš„æ–‡ä»¶åï¼Œoffset æ˜¯è¯¥æƒé‡åœ¨æ–‡ä»¶ä¸­çš„åç§»é‡ï¼Œlength æ˜¯æƒé‡çš„é•¿åº¦ã€‚æœ‰äº†ä»¥ä¸Šä¿¡æ¯ï¼Œonnx å†…éƒ¨å°±å¯ä»¥ç›´æ¥ load æƒé‡ï¼Œè§£å†³ 2GB é™åˆ¶é—®é¢˜ã€‚

ä»”ç»†çš„åŒå­¦ä¼šè§‚å¯Ÿåˆ°ï¼Œå¯¼å‡ºçš„ uent ç›®å½•ä¸‹æœ‰ï¼Œé™¤äº†.onnx æ¨¡å‹ï¼Œè¿˜æœ‰éå¸¸éå¸¸å¤šçš„ weight/bias ç­‰æ–‡ä»¶ã€‚è¿™å…¶å®å°±æ˜¯æ¯ä¸€ä¸ªæƒé‡æ•°æ®ã€‚å¦‚æ­¤ç¢ç‰‡åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ–è€…ç‰ˆæœ¬ç®¡ç†èµ·æ¥éå¸¸ä¸æ–¹ä¾¿ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼Œå°†æ‰€æœ‰çš„æƒé‡åˆå¹¶åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼š

```python
unet_opt_graph = onnx.load(unet_model_path)
onnx.save_model(
    unet_opt_graph,
    unet_model_path,
    save_as_external_data=True,
    all_tensors_to_one_file=True,
    location="model.onnx_data",
    convert_attribute=False,
)
```

è¿™æ ·æ‰€æœ‰çš„æƒé‡å°±ä¼šä¿å­˜åˆ°ä¸€ä¸ª model.onnx_data æ–‡ä»¶é‡Œã€‚

### C++æ¨ç†

ä¸ä¸Šæ–‡ç±»ä¼¼ï¼Œå€ŸåŠ© AiDBï¼Œä½¿ç”¨ C++ä¸²èµ·æ•´ä¸ª pipeline

```cpp
std::vector<float> noise(1 * 4 * 64 * 64);
AIDB::Utility::randn(noise.data(), noise.size());

auto strength = 1.0f; // 0~1 ä¹‹é—´é‡ç»˜æ¯”ä¾‹ã€‚è¶Šä½è¶Šæ¥è¿‘è¾“å…¥å›¾ç‰‡ã€‚
auto scheduler = Scheduler::DDIMScheduler("scheduler_config.json");
auto scaling_factor = 0.18215f;
auto tokenizer = Tokenizer::FromBlobJSON(
      LoadBytesFromFile("tokenizer.json"));
std::string startoftext = "<|startoftext|>";
std::string endoftext = "<|endoftext|>";
std::string trigger = argv[1];
std::string prompt = startoftext + trigger + endoftext;
std::vector<int> text_input_ids = tokenizer->Encode(prompt);

std::string uncond_tokens = startoftext + "longbody, lowres, cropped, worst quality, low quality, multiple people" + endoftext;

std::vector<int> uncond_input = tokenizer->Encode(uncond_tokens);
auto text_enc = AIDB::Interpreter::createInstance("text_encoder", "onnx");

std::vector<std::vector<float>>  prompt_embeds;
std::vector<std::vector<int>>  prompt_embeds_shape;

text_enc->forward(text_input_ids.data(), 77, 0, 0,  prompt_embeds, prompt_embeds_shape);

std::vector<std::vector<float>>  negative_prompt_embeds;
std::vector<std::vector<int>>  negative_prompt_embeds_shape;
text_enc->forward(uncond_input.data(), 77, 0, 0,  negative_prompt_embeds, negative_prompt_embeds_shape);

std::vector<float> prompt_embeds_cat(2 * 77 * 768, 0);
memcpy(prompt_embeds_cat.data(), negative_prompt_embeds[0].data(), 77 * 768 * sizeof(float));
memcpy(prompt_embeds_cat.data() + 77 * 768, prompt_embeds[0].data(), 77 * 768 * sizeof(float));

auto num_inference_steps = 10;
scheduler.set_timesteps(num_inference_steps);
std::vector<int> timesteps;
scheduler.get_timesteps(timesteps);

// Figuring initial time step based on strength
auto init_timestep = min(int(num_inference_steps * strength), num_inference_steps);
auto t_start = max(num_inference_steps - init_timestep, 0);

timesteps.assign(timesteps.begin() + t_start, timesteps.end());

num_inference_steps = timesteps.size();

auto image = cv::imread("portrait.png");
int target = 512;
float src_ratio = float(image.cols) / float(image.rows);
float target_ratio = 1.0f;

int n_w, n_h, pad_w = 0, pad_h = 0;
float _scale_h, _scale_w;

if(src_ratio > target_ratio){
  n_w = target;

  n_h = floor(float(n_w) / float(image.cols) * float(image.rows) + 0.5f);
  pad_h = target - n_h;
  _scale_h = _scale_w = float(n_w) / float(image.cols);
} else if(src_ratio < target_ratio){
  n_h = target;
  n_w = floor(float(n_h) / float(image.rows) * float(image.cols) + 0.5f);
  pad_w = target - n_w;
  _scale_h = _scale_w = float(n_h) / float(image.rows);
} else{
  n_w = target;
  n_h = target;
  _scale_h = _scale_w = float(n_w) / float(image.cols);
}

cv::resize(image, image, cv::Size(n_w, n_h));
cv::copyMakeBorder(image, image, 0, pad_h, 0, pad_w, cv::BORDER_CONSTANT, cv::Scalar(255, 255, 255));

auto low_threshold = 150;
auto high_threshold = 200;
cv::Mat canny;
cv::Canny(image, canny, low_threshold, high_threshold);

std::vector<cv::Mat> bgr_channels{canny, canny, canny};
cv::merge(bgr_channels, canny);

image.convertTo(image, CV_32F);
image = image / 127.5 - 1.0;
cv::Mat blob;
cv::dnn::blobFromImage(image, blob);

canny.convertTo(canny, CV_32F);
cv::Mat blob_canny;
cv::dnn::blobFromImage(canny, blob_canny, 1.0f / 255.0f);

auto vae_enc = AIDB::Interpreter::createInstance("sd_vae_encoder_with_controlnet", "onnx");

auto vae_dec = AIDB::Interpreter::createInstance("sd_vae_decoder_with_controlnet", "onnx");

auto unet = AIDB::Interpreter::createInstance2("sd_unet_with_controlnet_with_lora", "shinkawa", "onnx");

// Prepare latent variables
std::vector<std::vector<float>>  image_latents;
std::vector<std::vector<int>>  image_latents_shape;

vae_enc->forward(blob.data, 512, 512, 3, image_latents, image_latents_shape);

auto latents = image_latents[0];


std::for_each(latents.begin(), latents.end(), [=](float &item){item *= scaling_factor;});

auto latent_timestep = timesteps[0];
std::vector<float> init_latents;
scheduler.add_noise(latents, {1, 4, 64, 64}, noise, {1, 4, 64, 64}, latent_timestep, init_latents);

auto guidance_scale = 7.5f;

std::vector<float> controlnet_keep(timesteps.size(), 1.0);
float controlnet_conditioning_scale = 0.5f; // 0~1 ä¹‹é—´çš„ ControlNet çº¦æŸæ¯”ä¾‹ã€‚è¶Šé«˜è¶Šè´´è¿‘çº¦æŸã€‚

int step = 0;
for(auto t: timesteps){
  auto tic = std::chrono::system_clock::now();
  double cond_scale = controlnet_conditioning_scale * controlnet_keep[step];
  std::vector<float> latent_model_input(2 * 4 * 64 * 64, 0);
  memcpy(latent_model_input.data(), init_latents.data(), 4 * 64 * 64 * sizeof(float));
  memcpy(latent_model_input.data() + 4 * 64 * 64, init_latents.data(), 4 * 64 * 64 * sizeof(float));


  std::vector<std::vector<float>> down_and_mid_blok_samples;
  std::vector<std::vector<int>> down_and_mid_blok_samples_shape;
  std::vector<void*> input;
  std::vector<std::vector<int>> input_shape;

  // sample
  input.push_back(latent_model_input.data());
  input_shape.push_back({2, 4, 64, 64});

  // t âœ…
  std::vector<float> timestep = {(float)t};
  input.push_back(timestep.data());
  input_shape.push_back({1});

  // encoder_hidden_states âœ…
  input.push_back(prompt_embeds_cat.data());
  input_shape.push_back({2, 77, 768});

  std::vector<float> controlnet_cond(2 * 3 * 512 * 512, 0);
  memcpy(controlnet_cond.data(), blob_canny.data, 3 * 512 * 512 * sizeof(float));
  memcpy(controlnet_cond.data() + 3 * 512 * 512, blob_canny.data, 3 * 512 * 512 * sizeof(float));

  // controlnet_cond âœ…
  input.push_back(controlnet_cond.data());
  input_shape.push_back({2, 3, 512, 512});

  // conditioning_scale âœ…
  std::vector<float> cond_scales = {(float)(cond_scale)};
  input.push_back(cond_scales.data());
  input_shape.push_back({1});

  std::vector<std::vector<float>> noise_preds;
  std::vector<std::vector<int>> noise_preds_shape;
  unet->forward(input, input_shape, noise_preds, noise_preds_shape);

  // noise_preds [2,4,64,64] noise_pred_uncond | noise_pred_text
  std::vector<float> noise_pred(1 * 4 * 64 * 64, 0);
  for(int i = 0; i < noise_pred.size(); i++){
      noise_pred[i] = noise_preds[0][i] + guidance_scale * (noise_preds[0][i + 4 * 64 * 64] - noise_preds[0][i]);
  }
  std::vector<float> pred_sample;
  scheduler.step(noise_pred, {1, 4, 64, 64}, init_latents, {1, 4, 64, 64}, pred_sample, t);
  init_latents.clear();
  init_latents.assign(pred_sample.begin(), pred_sample.end());
  auto toc = std::chrono::system_clock::now();
  std::chrono::duration<double> elapsed = toc - tic;
  std::cout << "\rStep " << step++ << " " <<std::fixed << std::setprecision(1) << float(step) / timesteps.size() * 100
            << "% " << std::fixed << std::setprecision(3) << 1.0f / elapsed.count() << "b/s"
            << " [" << std::setw(float(step) / (float)timesteps.size() * 30) << std::setfill('=') << '>' << ']';
  std::flush(std::cout);
}
std::for_each(init_latents.begin(), init_latents.end(), [=](float &item){item /= scaling_factor;});

std::vector<std::vector<float>>  sample;
std::vector<std::vector<int>>  sample_shape;
vae_dec->forward(init_latents.data(), 64, 64, 4, sample, sample_shape);

cv::Mat sd_image(sample_shape[0][2], sample_shape[0][3], CV_8UC3);
AIDB::Utility::stable_diffusion_process(sample[0].data(), sample_shape[0][2], sample_shape[0][3], sample_shape[0][1], sd_image);

cv::imwrite("stable_diffusion_controlnet_img2img_" + trigger + ".jpg", sd_image);
```

![](https://files.mdnice.com/user/48619/2eaef51b-02ec-44d5-a255-9116fcc97555.jpeg)

### LoRA æ–¹å¼åŠ è½½

å›åˆ°ä¸Šæ–‡æåˆ°çš„é—®é¢˜ï¼Œä»¥ä¸Šä¾‹å­ä½¿ç”¨ controlNet-canny å¯¼å‡º onnx æ¨¡å‹ï¼Œå¦‚æœæˆ‘ä»¬åˆæƒ³ä½¿ç”¨ controlNet-hedï¼Œæˆ–è€…ä½¿ç”¨æ›´å¤šçš„ LoRa å‘¢ï¼Ÿæ˜¯å¦ä¸€å®šå¿…é¡»é‡æ–°å¯¼å‡ºæ•´ä¸ªæ¨¡å‹ï¼Œ
æ˜¯å¦å¯ä»¥ç”¨â€œLoRaâ€çš„æ–¹å¼åŠ è½½æ¨¡å‹å‘¢ã€‚ç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼ŒæŸ¥çœ‹ onnruntime çš„æ¥å£ï¼Œå®˜æ–¹æä¾›äº†å¦‚ä¸‹æ¥å£:

```cpp
SessionOptions& SessionOptions::AddExternalInitializers(const std::vector<std::string>& names,const std::vector<Value>& ort_values)
```

åˆ©ç”¨æ­¤æ¥å£ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°â€œLoRa æ–¹å¼â€çš„æ¨¡å‹åŠ è½½ã€‚è¿™é‡Œä»¥â€œLoRaâ€ä¸¾ä¾‹ï¼ŒcontrolNet åŒç†ã€‚

å…ˆåšä¸€ç‚¹ç®€å•çš„çŸ¥è¯†å‚¨å¤‡ï¼ŒONNX æ¨¡å‹æœ¬è´¨å°±æ˜¯ä¸€ä¸ª Protobuf åºåˆ—åŒ–åçš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œæ‰€ä»¥ç†è®ºä¸Šæˆ‘ä»¬å¯ä»¥åšä»»æ„åˆç†çš„ä¿®æ”¹ã€‚æ ¹æ® onnx.proto çš„å®šä¹‰ï¼Œé¦–å…ˆæ¥çœ‹ä¸€ä¸‹ onnx æ¨¡å‹çš„ç»“æ„ã€‚ onnx ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªç±»ï¼šModelProtoï¼ŒNodeProtoï¼ŒGraphProtoï¼ŒTensorProto ç­‰ã€‚ModelProto ä½œä¸º top-level ç±»ï¼Œç”¨äºç»‘å®š ML æ¨¡å‹å¹¶å°†å…¶è®¡ç®—å›¾ä¸å…ƒæ•°æ®ç›¸å…³è”ã€‚NodeProto ç”¨æ¥æè¿°äº† graph ä¸­çš„ nodeã€‚TensorProto åˆ™ç”¨æ¥ç»„ç»‡ tensor çš„å…·ä½“ä¿¡æ¯ã€‚æ‰€ä»¥ onnx çš„ç»“æ„å¤§æ¦‚å¯ä»¥ç”¨ä¸‹å›¾è¡¨ç¤ºï¼š

![](https://files.mdnice.com/user/48619/6dfee3a7-5d77-4350-b06f-9e4f5d5e94d0.png)

è¿™æ ·æˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªå¤§æ¦‚çš„æ€è·¯ï¼Œè¯»å– LoRa æ¨¡å‹ï¼Œè§£æ LoRa æ¨¡å‹ä¸­ tensorï¼Œå› ä¸ºç½‘ç»œç»“æ„éƒ½æ˜¯ç›¸åŒçš„ï¼Œæˆ‘ä»¬ç›´æ¥é€šè¿‡ onnxruntime çš„ AddExternalInitializers æ¥å£ï¼Œæ¥æ›¿æ¢åŸå§‹ç½‘ç»œä¸­çš„ LoRa éƒ¨åˆ†ã€‚

#### onnx æ¨¡å‹è¯»å–

ä½¿ç”¨ protobuf è¯»å– onnx æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ ortï¼š

```cpp
std::ifstream fin(lora_path, std::ios::in | std::ios::binary);
onnx::ModelProto onnx_model;
onnx_model.ParseFromIstream(&fin);

auto graph = onnx_model.graph();
const auto& initializer = graph.initializer();
std::vector<std::string> init_names;
std::vector<Ort::Value> initializer_data;
for(auto& tensor: initializer){
    init_names.push_back(tensor.name());
    tensors.emplace_back(tensor);
}

fin.close();
```

#### OPåç§°

åŸå§‹æ¨¡å‹ä¸ onnx å¯¼å‡ºçš„æ¨¡å‹çš„åå­—æ˜¯ä¸ä¸€è‡´çš„ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°æ˜ å°„å…³ç³»ï¼Œæ‰èƒ½æ­£ç¡®åŠ è½½ã€‚
é¦–å…ˆåŠ è½½ ğŸ¤—safetensors æ ¼å¼çš„æ¨¡å‹

```python
from safetensors.torch import load_file
state_dict = load_file("LoRa.safetensors")
```

æ­¤æ—¶ state_dict ä¸­çš„ key å¹¶ä¸æ˜¯æ¨¡å‹ onnx å¯¼å‡ºå‰çš„ keyï¼Œè¿™é‡Œéœ€è¦åšä¸€ä¸ªè½¬æ¢ã€‚ç›´æ¥å‚è€ƒ diffusers çš„ä»£ç ï¼š

```python
from diffusers.loaders.lora_conversion_utils import _convert_kohya_lora_to_diffusers, _maybe_map_sgm_blocks_to_diffusers
from diffusers.utils.state_dict_utils import convert_unet_state_dict_to_peft, convert_state_dict_to_diffusers

indent = "  "
unet_config = None
# Map SDXL blocks correctly.
if unet_config is not None:
    # use unet config to remap block numbers
    state_dict = _maybe_map_sgm_blocks_to_diffusers(state_dict, unet_config)
state_dict, network_alphas = _convert_kohya_lora_to_diffusers(state_dict)

keys = list(state_dict.keys())

unet_name = "unet"
text_encoder_name = "text_encoder"

unet_keys = [k for k in keys if k.startswith(unet_name)]
unet_state_dict = {k: v for k, v in state_dict.items() if k in unet_keys}

unet_state_dict = convert_unet_state_dict_to_peft(unet_state_dict)

text_encoder_lora_state_dict = {}
if any(text_encoder_name in key for key in keys):
    text_encoder_keys = [k for k in keys if k.startswith(text_encoder_name) and k.split(".")[0] == text_encoder_name]
    text_encoder_lora_state_dict = {
        k.replace(f"{text_encoder_name}.", ""): v for k, v in state_dict.items() if k in text_encoder_keys
    }

    text_encoder_lora_state_dict = convert_state_dict_to_diffusers(text_encoder_lora_state_dict)


unet_lora_state_dict = {}
for key, value in unet_state_dict.items():
    name = key
    if "lora_A" in key:
        name = key.replace("lora_A", "lora_layer.down")
    elif "lora_B" in key:
        name = key.replace("lora_B", "lora_layer.up")

    unet_lora_state_dict[name] = value
```

æ‰§è¡Œä»¥ä¸Šä»£ç ï¼Œå¯ä»¥å¾—åˆ° torch.onnx.export å‰æ¨¡å‹çš„ key:valueã€‚æ¥ä¸‹æ¥å°±æ˜¯å’Œ onnx æ¨¡å‹ä¸­çš„ name æ‰¾åˆ°å¯¹åº”å…³ç³»ã€‚

å…¶å® onnx æ¨¡å‹ä¸­å·²ç»å‚¨å­˜äº†å¯¹åº”çš„å¯¹åº”å…³ç³»ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç å…ˆè§‚å¯Ÿä¸‹ onnx æ¨¡å‹ä¸­æ‘äº†ä»€ä¹ˆä¿¡æ¯(è¿™é‡Œåªè¾“å‡ºäº† lora ç›¸å…³çš„):

```python
onnx_model = onnx.load("unet.onnx", load_external_data=False)
for node in onnx_model.graph.node:
    print(onnx.helper.printable_node(node, indent, subgraphs=True))

```

éƒ¨åˆ†è¾“å‡ºï¼š

![](https://files.mdnice.com/user/48619/494336eb-0a38-44a2-b6da-7ed875b64632.png)

å¯ä»¥çœ‹åˆ°æ¯ä¸ªnodeçš„å¯¹åº”å…³ç³»ï¼Œæ ¼å¼å¦‚ä¸‹torch-op-name = OP(param, onnx-tensor-name)ã€‚æŒ‰ç…§ä»¥ä¸Šè§„åˆ™ï¼Œå¯ä»¥æ‰¾åˆ°ä¸¤ç§æ¨¡å‹opnameçš„æ˜ å°„ï¼Œå°†è¿™ç§å…³ç³»ä¿å­˜ä¸‹æ¥ï¼š

```python
unet_mapping = {}
for node in onnx_model.graph.node:
    contents = onnx.helper.printable_node(node, indent, subgraphs=True)[0]
    content = re.findall(r'%.*?(?= |,)|%.*?(?=\))', contents)  # torch-op-name = OP(å½¢å‚ï¼Œ onnx-weight-name)
    if ".".join(content[0].split("/")[1:-1] + ["weight"]) in unet_lora_state_dict:
        unet_mapping[".".join(content[0].split("/")[1:-1] + ["weight"])] = content[-1][1:]
```

#### LoRaä¿å­˜

  æœ€åå°±æ˜¯å¦‚ä½•ç»„ç»‡æ–°çš„LoRaæ¨¡å‹äº†ã€‚è¿™é‡Œä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªâ€œå‡çš„â€onnxæ¨¡å‹ï¼Œä»…ä»…å­˜å‚¨LoRaçš„æƒé‡ï¼Œnameä»¥ä¸Šä¸€èŠ‚æ˜ å°„åä¸ºå‡†ã€‚

```python
initializer = []
for key, value in unet_lora_state_dict.items():
    initializer.append(
        helper.make_tensor(
            name=unet_mapping[key],
            data_type=helper.TensorProto.DataType.FLOAT,
            dims=value.T.shape if value.ndim == 2 else value.shape,
            vals=value.T.float().numpy().astype(np.float32).tobytes()
            if value.ndim == 2 else value.float().numpy().astype(np.float32).tobytes(),
            raw=True
        )
    )

graph = helper.make_graph(
    name=LoRa,
    inputs=[],
    outputs=[],
    nodes=[],
    initializer=initializer
)

opset = [
    helper.make_operatorsetid(LoRa, 14)
]

model = helper.make_model(graph, opset_imports=opset)
onnx.save_model(model, "LoRa.lora")
```

#### LoRaæ ¡éªŒ

  ä»¥ä¸Š3æ­¥å·²ç»å¾—åˆ°äº†æ–°çš„æ¨¡å‹ï¼Œä½†ä¸ºäº†ç¡®è®¤æˆ‘ä»¬çš„æ–¹å¼æ˜¯å¦æ­£ç¡®ï¼Œæˆ‘ä»¬æ‹¿ä¸€ä¸ªå·²ç»å¯¼å‡ºçš„Unetæ¨¡å‹å’Œå¯¹åº”çš„LoRaæƒé‡åšä¸€ä¸‹æ ¡éªŒ

```python
import onnx
from onnx.external_data_helper import load_external_data_for_model, _get_all_tensors
import numpy as np


onnx_model1 = onnx.load("unet-model.onnx", load_external_data=True)
onnx_model2 = onnx.load("lora.lora")

lora_state_dict = {}
for t in _get_all_tensors(onnx_model2):
    lora_state_dict[t.name] = np.frombuffer(t.raw_data, dtype=np.float32)


for t in _get_all_tensors(onnx_model1):
    if t.name in lora_state_dict:
            np.testing.assert_almost_equal(np.frombuffer(t.raw_data, dtype=np.float32),
                                       lora_state_dict[t.name],
                                       decimal=6)
```

ç¡®è®¤æ²¡é—®é¢˜ï¼Œæˆ‘ä»¬çš„å‡†å¤‡å·¥ä½œä¹Ÿç®—å®Œæˆã€‚ä¸‹é¢å®ŒæˆC++ä»£ç éƒ¨åˆ†ã€‚

#### LoRaåŠ è½½

  è¯»å–æ–°çš„LoRaæ¨¡å‹ï¼Œå°†æƒé‡çš„nameå’Œraw_dataè¯»å–å‡ºæ¥ï¼Œç„¶ååˆ›å»ºå¯¹åº”çš„tensorï¼Œæœ€åè°ƒç”¨session_options.AddExternalInitializersä¸€èµ·åˆå§‹åŒ–å³å¯ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œonnxruntimeçš„CreateTensoræ“ä½œæ˜¯æµ…æ‹·è´ï¼Œæ‰€ä»¥åœ¨å†™æ³•ä¸Šæ³¨æ„å±€éƒ¨å˜é‡çš„ç”Ÿå­˜å‘¨æœŸã€‚

```cpp
if(!param._lora_path.empty()){
        std::vector<std::string> init_names;
        std::vector<Ort::Value> initializer_data;
        std::vector<onnx::TensorProto> tensors;
        auto allocator = Ort::AllocatorWithDefaultOptions();

        for(const auto& lora_path: param._lora_path){
            std::ifstream fin(lora_path, std::ios::in | std::ios::binary);
            onnx::ModelProto onnx_model;
            onnx_model.ParseFromIstream(&fin);

            auto graph = onnx_model.graph();
            const auto& initializer = graph.initializer();

            for(auto& tensor: initializer){
                init_names.push_back(tensor.name());
                tensors.emplace_back(tensor);
            }
            fin.close();
        }
        for(const auto& tensor: tensors){
            std::vector<int64_t> shape(tensor.dims_size(), 0);
            for(int i = 0; i < tensor.dims_size(); i++){
                shape[i] = tensor.dims(i);
            }
        Ort::Value input_tensor = Ort::Value::CreateTensor(allocator.GetInfo(),
                                                               (void *)(tensor.raw_data().c_str()),
                                                               tensor.raw_data().length(),
                                                               shape.data(),
                                                               shape.size(),
                                                               ONNXTensorElementDataType(tensor.data_type()));

            initializer_data.push_back(std::move(input_tensor));
        }
        _session_options.AddExternalInitializers(init_names, initializer_data);
        _session = std::make_shared<Ort::Session>(_env, param._model_path.c_str(), _session_options);
}
```

ä½œè€…åœ¨Cç«™æ‰¾äº†å‡ ä¸ªç›¸åŒç»“æ„çš„LoRaï¼Œåˆ†åˆ«ä¸ºblindboxã€mix4å’Œmoxinï¼Œæµ‹è¯•ä¸€ä¸‹æ•ˆæœ 


![](https://files.mdnice.com/user/48619/daa213c9-b9a9-434f-b296-0945e267fedb.jpg)



> ä»¥ä¸Šä»£ç å’Œæ¨¡å‹éƒ½å·²å¼€æºï¼Œæ›´å¤šè¯¦æƒ…ï¼Œæ•¬è¯·ç™»é™†githubï¼Œæ¬¢è¿Starã€‚
