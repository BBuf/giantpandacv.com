# 0x0. å‰è¨€
æœ€è¿‘ç²—ç•¥çš„çœ‹å®Œäº†å¤©å¥‡å¤§ä½¬çš„MLCè¯¾ç¨‹ï¼ˆé¡ºä¾¿ä¿®äº†ä¸€äº›è¯­æ³•å’Œæ‹¼å†™é”™è¯¯ï¼Œä¹Ÿç®—æ˜¯åšäº†å¾®å¼±çš„è´¡çŒ®hhï¼‰ï¼Œå¯¹TVMçš„è¿‘æœŸå‘å±•æœ‰äº†ä¸€äº›æ–°çš„è®¤è¯†ã€‚ä¹‹å‰å¤©å¥‡å¤§ä½¬åœ¨ã€Šæ–°ä¸€ä»£æ·±åº¦å­¦ä¹ ç¼–è¯‘æŠ€æœ¯å˜é©å’Œå±•æœ›ã€‹ä¸€æ–‡ä¸­ï¼ˆé“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/446935289ï¼‰è®²è§£äº†TVM Unifyä¹Ÿå³ç»Ÿä¸€å¤šå±‚æŠ½è±¡çš„æ¦‚å¿µã€‚è¿™é‡Œçš„ç»Ÿä¸€å¤šå±‚æŠ½è±¡å…·ä½“åŒ…æ‹¬**AutoTensorizationç”¨æ¥è§£å†³ç¡¬ä»¶æŒ‡ä»¤å£°æ˜å’Œå¼ é‡ç¨‹åºå¯¹æ¥ï¼ŒTVM FFIï¼ˆPackedFuncï¼‰æœºåˆ¶ä½¿å¾—æˆ‘ä»¬å¯ä»¥çµæ´»åœ°å¼•å…¥ä»»æ„çš„ç®—å­åº“å’Œè¿è¡Œåº“å‡½æ•°å¹¶ä¸”åœ¨å„ä¸ªç¼–è¯‘æ¨¡å—å’Œè‡ªå®šä¹‰æ¨¡å—é‡Œé¢ç›¸äº’è°ƒç”¨ã€‚TensorIRè´Ÿè´£å¼ é‡çº§åˆ«ç¨‹åºå’Œç¡¬ä»¶å¼ é‡æŒ‡ä»¤çš„æ•´åˆã€‚Relax (Relax Next) å¼•å…¥relayçš„è¿›ä¸€æ­¥è¿­ä»£ï¼Œç›´æ¥å¼•å…¥first class symbolic shapeçš„æ”¯æŒ** ï¼ˆæ‘˜æŠ„è‡ªã€Šæ–°ä¸€ä»£æ·±åº¦å­¦ä¹ ç¼–è¯‘æŠ€æœ¯å˜é©å’Œå±•æœ›ã€‹ä¸€æ–‡ï¼‰ã€‚ç„¶åè¿™äº›æŠ½è±¡å¯ä»¥ç›¸äº’äº¤äº’å’Œè”åˆä¼˜åŒ–æ¥æ„é€ æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹åº”çš„æœ€ç»ˆéƒ¨ç½²å½¢å¼ã€‚æˆ‘ä¸ªäººæ„Ÿè§‰TVM Unifyç±»ä¼¼äºMLIRçš„Dialectï¼Œä½†æ˜¯è¿™å‡ ä¸ªæŠ½è±¡çš„ç›´æ¥äº¤äº’èƒ½åŠ›ç›¸æ¯”äºMLIRçš„é€çº§loweræˆ‘æ„Ÿè§‰æ˜¯æ›´ç›´è§‚æ–¹ä¾¿çš„ï¼Œæ¯•ç«Ÿæ˜¯Python Firstï¼ˆè¿™ä¸ªåªæ˜¯æˆ‘æœ€è¿‘çœ‹MLCè¯¾ç¨‹çš„ä¸€ä¸ªæ„Ÿè§‰ï¼‰ã€‚å¯¹è¿™éƒ¨åˆ†å†…å®¹æ„Ÿå…´è¶£çš„è¯»è€…è¯·æŸ¥çœ‹å¤©å¥‡å¤§ä½¬çš„TVM Unifyä»‹ç»åŸæ–‡ä»¥åŠMLCè¯¾ç¨‹ã€‚


è¿™ç¯‡æ–‡ç« æˆ‘å°†ç»“åˆTVM Unifyç›¸å…³çš„æŠ½è±¡ä»¥åŠä¹‹å‰çš„ä¸€äº›ç§¯ç´¯é‡æ–°æ¢³ç†ä¸€ä¸‹TVMçš„æ•´ä½“æµç¨‹ã€‚æˆ‘ä¼šä»å‰ç«¯ï¼Œä¸­ç«¯ï¼ˆå›¾ä¼˜åŒ–Passæœºåˆ¶ï¼‰ï¼Œä»£ç ç”Ÿæˆï¼ˆScheduleï¼‰ï¼ŒRuntimeï¼Œå¼€å‘å·¥å…·å‡ ä¸ªè§’åº¦æ¥ä»‹ç»ä¸€éã€‚æˆ‘å¯¹TVMçš„ä»£ç å¹¶æ²¡æœ‰åšåˆ°ç²¾ç»†çš„é˜…è¯»ï¼Œæ‰€ä»¥æœ¬æ–‡å°†å°½é‡é¿å…æ¶‰åŠåˆ°åº•å±‚C++ä»£ç çš„ç»†ææœ«èŠ‚ï¼Œè€Œæ˜¯ä»è¾ƒä¸ºå®è§‚çš„è§†è§’æ¥è®²æ¸…æ¥šç›®å‰TVMçš„æ¶æ„ã€‚æœ¬ç¯‡æ–‡ç« çš„æ‰€æœ‰å‚è€ƒèµ„æ–™ä»¥åŠideaä¸»è¦æ¥è‡ªæˆ‘ç»´æŠ¤çš„è¿™ä¸ªä»“åº“ï¼ˆhttps://github.com/BBuf/tvm_mlir_learnï¼‰é‡Œé¢æœé›†çš„TVMçš„ç›¸å…³èµ„æ–™ï¼ŒTVMå®˜æ–¹docä»¥åŠæºç ï¼ŒMLCè¯¾ç¨‹ã€‚ä¸Šé¢è¿™ä¸ªä»“åº“åŸºæœ¬æ”¶é›†äº†TVMä¸­æ–‡ç¤¾åŒºé‡Œé¢çš„å¤§éƒ¨åˆ†é«˜è´¨é‡åšå®¢æˆ–è€…ä¸“é¢˜ï¼Œå¯¹TVMæ„Ÿå…´è¶£çš„å°ä¼™ä¼´å¯ä»¥è‡ªè¡Œä¸‹è½½æˆ–è€…æ”¶è—ï¼Œæ›´æ¬¢è¿ç‚¹ä¸ªstarã€‚

å†™ä½œä¸æ˜“ï¼Œè¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰ç”¨çš„è¯ä¹Ÿè¯·ç‚¹ä¸ªèµğŸ‘ã€‚æ–‡ç« æœ‰é”™è¯¯ä¹Ÿè¯·æŒ‡å‡ºï¼Œæˆ‘åŠ¨æ€ä¿®æ”¹ã€‚ä¹‹åçš„è®¡åˆ’åº”è¯¥ä¼šå­¦ä¹ TVMå¦‚ä½•å’Œç¡¬ä»¶çš„æŒ‡ä»¤å¯¹æ¥ã€‚

# 0x1. å‰ç«¯

TVMä¸ºäº†å‘ä¸Šå…¼å®¹æ‰€æœ‰çš„æœºå™¨å­¦ä¹ æ¡†æ¶å¦‚PyTorchï¼ŒTensorFlowï¼ŒONNXç­‰å¼•å…¥äº†Relay IRï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è¿›å…¥TVMä¹‹åé¦–å…ˆä¼šè¢«è½¬æ¢ä¸ºRelay IRã€‚åŒæ—¶TVMä¸ºäº†å‘ä¸‹å…¼å®¹æ‰€æœ‰çš„ç¡¬ä»¶ï¼Œå¼•å…¥äº†Tensor IRç®€ç§°TIRï¼Œæ¨¡å‹åœ¨è¢«ç¼–è¯‘ä¸ºæŒ‡å®šç¡¬ä»¶çš„æºä»£ç ä¹‹å‰éƒ½ä¼šè¢«Lowerä¸ºTIRã€‚å¦å¤–ï¼ŒTVMç¤¾åŒºæ­£åœ¨å¼€å‘æ–°ä¸€ä»£ä¸­é—´è¡¨ç¤ºRelaxï¼ˆä¹Ÿè¢«ç§°ä¸ºä¸‹ä¸€ä»£Relayï¼Œç›®å‰è¿˜æ²¡æœ‰upstreamä¸»åˆ†æ”¯ï¼šhttps://github.com/tlc-pack/relax/tree/relax/python/tvm/relaxï¼‰ï¼ŒRelaxæ˜¯å®ç°å‰è¨€é‡Œé¢æåˆ°çš„TVM Unifyå…³é”®çš„ä¸€ç¯ã€‚TVMå‰ç«¯çš„æ¶æ„å¯ä»¥ç²—ç•¥çš„è¡¨ç¤ºä¸ºï¼š


![TVMå‰ç«¯æ¶æ„å›¾](https://img-blog.csdnimg.cn/6be2049a969a449bb79911739fd42169.png)



æ¥ä¸‹æ¥æˆ‘ä»¬åˆ†åˆ«ä»‹ç»ä¸€ä¸‹ Relayï¼ŒTIRï¼ŒRelaxè¿™å‡ ç§ä¸åŒçš„å‰ç«¯è¡¨ç¤ºã€‚

## 0x1.1 Tensor IR(TIR)
ç”±äºæ— è®ºæ˜¯Relayè¿˜æ˜¯æ–°ä¸€ä»£çš„Relaxä¸­é—´è¡¨ç¤ºï¼Œå®ƒä»¬æœ€åéƒ½ä¼šè¢«Loweråˆ°TIRï¼ˆç¦»ç¡¬ä»¶æœ€è¿‘çš„IRï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œå…ˆä»‹ç»ä¸€ä¸‹TIRã€‚TIRçš„ä»£ç è¢«å°è£…åœ¨`tvm.tir`ä¸­ï¼Œä¸€ä¸ªTIRå¯ä»¥è¢«ç¼–è¯‘æˆç›®æ ‡ç¡¬ä»¶çš„æºä»£ç æˆ–è€…ä¸­é—´è¡¨ç¤ºä¾‹å¦‚C++æºç ï¼ŒCUDAæºç ï¼ŒLLVM IRç­‰ç­‰ã€‚é‚£ä¹ˆTIRæ˜¯å¦‚ä½•è¢«ç¼–è¯‘ä¸ºç›®æ ‡ç¡¬ä»¶çš„ä»£ç å‘¢ï¼Ÿè¿™æ˜¯å› ä¸ºTIRçš„æ•°æ®ç»“æ„å…¶å®æ˜¯ä¸€ä¸ªASTï¼ˆæŠ½è±¡è¯­æ³•æ ‘ï¼‰ï¼Œç„¶åè¿™ä¸ªè¯­æ³•æ ‘å¯ä»¥è¡¨ç¤ºå˜é‡çš„å£°æ˜ï¼Œåˆå§‹åŒ–ï¼Œå˜é‡çš„è®¡ç®—ï¼Œå‡½æ•°è°ƒç”¨ä»¥åŠæ§åˆ¶æµï¼ˆå¦‚if-elseæ¡ä»¶åˆ¤æ–­ï¼Œå¾ªç¯ç­‰ç­‰ï¼‰ç­‰ç­‰ã€‚æ‰€ä»¥åªè¦æˆ‘ä»¬éå†ä¸€ä¸‹TIRå¯¹åº”çš„ASTå°±å¯ä»¥å®ç°ä¸€å¯¹ä¸€çš„å°†å…¶ç¿»è¯‘åˆ°ç›®æ ‡ç¡¬ä»¶äº†ã€‚å¯ä»¥å€ŸåŠ©è¿™ä¸ªå›¾æ¥ç†è§£ï¼š

![åŸå›¾æ¥è‡ªï¼šhttps://zhuanlan.zhihu.com/p/533161438ï¼Œä¾µåˆ ](https://img-blog.csdnimg.cn/c61af990768e471e80e1a067acd41b0b.png)


åœ¨ä¸Šå›¾ä¸­æœ‰å‡ ä¸ªç»†èŠ‚éœ€è¦è§£é‡Šã€‚é¦–å…ˆæ˜¯IRModuleï¼ŒIRModule æ˜¯åœ¨æœºå™¨å­¦ä¹ ç¼–è¯‘ä¸­ä¿å­˜å…ƒå¼ é‡å‡½æ•°ï¼ˆä¹Ÿå³PrimFuncï¼‰é›†åˆçš„å®¹å™¨å¯¹è±¡ï¼Œå®ƒæ˜¯TVMè¿›è¡Œç¼–è¯‘çš„æœ€å°å®Œæ•´å•å…ƒã€‚TVMä¸åŒçš„å‰ç«¯è¡¨ç¤ºæœ€ç»ˆéƒ½ä¼šè¢«å°è£…åˆ°IRModuleä¸­è¿›è¡Œç¼–è¯‘ï¼Œåœ¨Linuxä¸‹IRModuleå°±æ˜¯ä¸€ä¸ª.soåŠ¨æ€é“¾æ¥åº“ã€‚ç„¶åPrimFuncå«ä½œå…ƒå¼ é‡å‡½æ•°ï¼Œå®ƒå†…éƒ¨å°è£…äº†ä¸€ä¸ªå®Œæ•´çš„TIR ASTã€‚å½“IRModuleè¢«ç¼–è¯‘ä¹‹åï¼Œæ¯ä¸ªPrimFuncéƒ½å¯¹åº”äº†è¿™ä¸ªåŠ¨æ€åº“çš„ä¸€ä¸ªå‡½æ•°å…¥å£ï¼Œå› æ­¤ä¸€ä¸ªIRModuleå¯ä»¥æœ‰å¾ˆå¤šä¸ªPrimFuncã€‚ç„¶åä¸Šé¢çš„Codegenå®é™…ä¸Šå°±æ˜¯å¯¹TIR ASTè¿›è¡Œä¸­åºéå†ç„¶åä¸€å¯¹ä¸€çš„å°†AST Nodeç¿»è¯‘ä¸ºç›¸åº”çš„TIR Nodeå¯¹åº”çš„æ•°æ®ç»“æ„å¹¶å‘é€ç»™å›è°ƒå‡½æ•°VisitExpr_ å’Œ VisitStmtã€‚VisitExpr_ ç”¨äºå¤„ç† Expression Nodeï¼Œè€Œ VisitStmt ç”¨äºå¤„ç† Statement Nodeã€‚åç»­åœ¨ä»‹ç»Codegençš„æ—¶å€™æˆ‘ä»¬å†ä»”ç»†æ¢ç´¢ä¸€ä¸‹è¿™ä¸ªè½¬æ¢æµç¨‹ã€‚

è¿™é‡Œè¿˜éœ€è¦è¯´æ˜çš„ä¸€ç‚¹æ˜¯ï¼Œåœ¨0.8ä¹‹å‰çš„TVMè¦å£°æ˜ä¸€ä¸ªTIR ASTä¾èµ–äºå¯¹Tensor Expressionçš„ç¼–è¯‘ã€‚ç°åœ¨TVMåŸºäºPython ASTå®ç°äº†ä¸€ç§æ–°çš„ç‰¹å®šé¢†åŸŸçš„æ–¹è¨€è®©æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨Pythonæ¥ç¼–å†™TIR ASTã€‚æˆ‘ä»¬è¿™é‡Œä¸¾ä¸€ä¸ªä¾‹å­ï¼š

```python
@tvm.script.ir_module
class MyModule:
    @T.prim_func
    def mm_relu(A: T.Buffer[(128, 128), "float32"],
                B: T.Buffer[(128, 128), "float32"],
                C: T.Buffer[(128, 128), "float32"]):
        T.func_attr({"global_symbol": "mm_relu", "tir.noalias": True})
        Y = T.alloc_buffer((128, 128), dtype="float32")
        for i, j, k in T.grid(128, 128, 128):
            with T.block("Y"):
                vi = T.axis.spatial(128, i)
                vj = T.axis.spatial(128, j)
                vk = T.axis.reduce(128, k)
                with T.init():
                    Y[vi, vj] = T.float32(0)
                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]
        for i, j in T.grid(128, 128):
            with T.block("C"):
                vi = T.axis.spatial(128, i)
                vj = T.axis.spatial(128, j)
                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))
```

å®ƒå®ç°çš„åŠŸèƒ½å¯¹åº”çš„numpyä»£ç ä¸ºï¼š

```python
def lnumpy_mm_relu(A: np.ndarray, B: np.ndarray, C: np.ndarray):
    Y = np.empty((128, 128), dtype="float32")
    for i in range(128):
        for j in range(128):
            for k in range(128):
                if k == 0:
                    Y[i, j] = 0
                Y[i, j] = Y[i, j] + A[i, k] * B[k, j]
    for i in range(128):
        for j in range(128):
            C[i, j] = max(Y[i, j], 0)
```

å…¶ä¸­ï¼Œ`@tvm.script.ir_module`è¡¨ç¤ºè¢«ä¿®é¥°çš„MyModuleæ˜¯ä¸€ä¸ªå¾…ç¼–è¯‘çš„IRModuleï¼Œè€Œ`@T.prim_func`è¡¨ç¤ºè¢«ä¿®é¥°çš„mainå‡½æ•°æ˜¯å…ƒå¼ é‡å‡½æ•°ï¼ˆPrimFuncï¼‰ï¼Œè¿™ä¸ªå‡½æ•°å†…éƒ¨å®šä¹‰çš„å°±æ˜¯TIR ASTã€‚


## 0x1.2 äº†è§£tvm.iråŸºç¡€è®¾æ–½
ç»§ç»­è®²Relay IRä»¥åŠRelaxä¹‹å‰æˆ‘ä»¬å…ˆäº†è§£ä¸€ä¸‹`tvm.ir`è¿™ä¸ªæŠ½è±¡ï¼Œæ— è®ºæ˜¯TIRè¿˜æ˜¯Relay/Relax IRå®ƒä»¬éƒ½å¯¹åº”äº†IRModuleè¿™ä¸ªç»Ÿä¸€çš„æœ€å°ç¼–è¯‘å•å…ƒï¼ŒåŒæ—¶å®ƒä»¬ä¹Ÿå¯¹åº”çš„æœ‰ä¸€å¥—å…±ç”¨çš„IRåŸºç¡€è®¾ç½®ï¼Œå…·ä½“å®ç°åœ¨`https://github.com/apache/tvm/tree/main/include/tvm/ir`å’Œ`https://github.com/apache/tvm/tree/main/src/ir`ç›®å½•ä¸‹ã€‚


![tvm.iråŸºç¡€è®¾æ–½æ–‡ä»¶ç»“æ„](https://img-blog.csdnimg.cn/4da3e9acda7d466db75fcd77b0789bac.png)

å¯¹äºIRæ¥è¯´ï¼ŒTypeå’ŒExpræ˜¯å°¤ä¸ºå…³é”®çš„ä¸¤ä¸ªæ¦‚å¿µã€‚TypeåŒ…å«åŸºç¡€çš„æ•°æ®ç±»å‹å¦‚Intï¼ŒFloatï¼ŒDoubleç­‰ç­‰ï¼Œä¹ŸåŒ…å«ä¸€äº›è‡ªå®šä¹‰çš„å¤æ‚ç±»å‹æ¯”å¦‚å‡½æ•°ç±»å‹ï¼ŒTensorç±»å‹ç­‰ã€‚è€Œå¯¹äºExpræ¥è¯´ï¼Œæ—¢åŒ…å«å¯ä»¥ç›´æ¥æ˜ å°„åˆ°Low-level IRçš„PrimExprï¼ŒåˆåŒ…å«RelayExprã€‚

æˆ‘ä»¬å¯ä»¥åœ¨`https://github.com/apache/tvm/blob/main/include/tvm/ir/type.h`ä¸­çœ‹åˆ°å¯¹PrimTypeNodeçš„å®šä¹‰ï¼š

```cpp
/*!
 * \brief Primitive data types used in the low-level IR.
 *
 * PrimType represents POD-values and handles that are
 * not automatically managed by the runtime.
 *
 * \sa PrimType
 */
class PrimTypeNode : public TypeNode {
 public:
  /*!
   * \brief The corresponding dtype field.
   */
  runtime::DataType dtype;
	...
};

```

å¯ä»¥çœ‹åˆ°PrimTypeå¯ä»¥ç›´æ¥å¯¹åº”åˆ°Low-level IRçš„åŸºç¡€æ•°æ®ç±»å‹ã€‚æˆ‘ä»¬è¿˜å¯ä»¥æ‰¾åˆ°FuncTypeNodeçš„å®šä¹‰ï¼š

```cpp
/*!
 * \brief Function type.
 *
 * We support polymorphic function type.
 * This can be roughly viewed as template function in C++.
 *
 * \sa FuncType, TypeVar, TypeConstraint
 */
class FuncTypeNode : public TypeNode {
 public:
  /*! \brief type type of arguments */
  Array<Type> arg_types;
  /*! \brief The type of return value. */
  Type ret_type;
  // The following fields are used in polymorphic(template) functions
  // For normal functions, the following two fields will be empty.
  /*! \brief The type parameters of the function */
  Array<TypeVar> type_params;
  /*!
   * \brief potential constraint the type need to obey
   * \note this field is reserved for futher purposes.
   */
  Array<TypeConstraint> type_constraints;
  ...
};
```

ä»æ³¨é‡Šå¯ä»¥çœ‹åˆ°FuncTypeç±»ä¼¼C++çš„æ¨¡æ¿å‡½æ•°ï¼Œè®°å½•äº†å‡½æ•°çš„å‚æ•°ç±»å‹å’Œè¿”å›å€¼ç±»å‹ä»¥åŠæ¨¡æ¿å‚æ•°ï¼Œçº¦æŸç­‰ä¿¡æ¯ã€‚ç„¶åæˆ‘ä»¬è¿˜å¯ä»¥å…³æ³¨ä¸€ä¸‹å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ç»“åˆå¾—å¾ˆç´§å¯†çš„TensorTypeNodeç±»å‹ã€‚

```cpp
/*!
 * \brief This is the most commonly used type in relay.
 *  TensorType have a fixed dimension, data type.
 *
 *  The elements of shape can be either IntImm(constant integer),
 *  or any symbolic integer expression.
 *  The symbolic integer allows generic shape inference in certain cases.
 * \sa TensorType
 */
class TensorTypeNode : public BaseTensorTypeNode {
 public:
  /*!
   * \brief The shape of the tensor,
   *  represented by PrimExpr(tvm::Expr).
   */
  Array<PrimExpr> shape;
  /*! \brief The content data type */
  DataType dtype;
	...
}
```

æˆ‘ä»¬ä»TensorTypeNodeçš„å®šä¹‰å¯ä»¥çœ‹åˆ°shapeä¹Ÿæ˜¯TensorTypeçš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥TVMåœ¨åšç±»å‹æ¨æ–­çš„æ—¶å€™ä¹ŸåŒ…å«äº†Shapeçš„æ¨æ–­ã€‚ä¹Ÿæ­£æ˜¯å› ä¸ºåœ¨IRä¸­Shapeæ˜¯Typeçš„ä¸€éƒ¨åˆ†ï¼ˆæ¯”å¦‚`Tensor[(m, n)]`å’Œ`Tensor[(m, 4)]`æ˜¯ä¸åŒçš„Typeï¼‰å¯¼è‡´TVMå¯¹åŠ¨æ€Shapeçš„æ”¯æŒéå¸¸å›°éš¾ï¼Œå› ä¸ºExprçš„ç±»å‹æ¨æ–­æ˜¯ä¸æ”¯æŒåŠ¨æ€Shapeçš„ã€‚è¿™é‡Œéœ€è¦æä¸€ä¸‹ï¼ŒRelaxé€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„Typeå«ä½œDynTensorè¾ƒå¥½çš„è§£å†³äº†åŠ¨æ€Shapeçš„è¡¨ç¤ºé—®é¢˜ï¼ŒDynTensoråŒ…å«çš„ä¿¡æ¯æ˜¯Dtypeå’ŒShapeçš„çº¬åº¦ï¼Œä½†Shapeæœ¬èº«çš„è¡¨è¾¾å¼æ˜¯ç‹¬ç«‹å­˜å‚¨çš„ã€‚ä¹Ÿå°±æ˜¯`Tensor[(m, n)]`å’Œ`Tensor[(_, _)]`éƒ½æ˜¯åŒä¸€ä¸ªTypeï¼Œ ä½†æ˜¯`Tensor[(_, _)]`å’Œ`Tensor[(_, _, _)]`æ˜¯ä¸åŒçš„Typeï¼Œè¿™æ ·å°±ä»åŸç”Ÿä¸Šæ”¯æŒäº†åŠ¨æ€Shapeã€‚æˆ‘ä»¬ä»`https://github.com/tlc-pack/relax/blob/95035621177fa0be4adfb55c766f030563e515a5/include/tvm/relax/type.h#L78`è¿™é‡Œå¯ä»¥çœ‹åˆ°DynTensorçš„å®šä¹‰ï¼š

```cpp
class DynTensorTypeNode : public BaseTensorTypeNode {
 public:
  /*!
   * \brief The number of dimensions of the tensor, use -1 to denote tensor with unknwon number of
   * dimensions.
   */
  int ndim; //ç°åœ¨ç›´æ¥å®šä¹‰ndimè€Œä¸æ˜¯shape
  /*! \brief The content data type, use void to denote the dtype is unknown. */
  DataType dtype;
  ...
};
```

æˆ‘ä»¬ç´§æ¥ç€çœ‹ä¸€ä¸‹Exprçš„å®šä¹‰ï¼ˆ`https://github.com/apache/tvm/blob/main/include/tvm/ir/expr.h`ï¼‰ï¼ŒExpråˆ†æˆPrimExprä»¥åŠRelayExprã€‚å…¶ä¸­PrimExprä¿å­˜äº†ä¸€ä¸ªruntimeæ—¶å€™çš„Dtypeï¼Œç„¶å

```cpp
/*!
 * \brief Base node of all primitive expressions.
 *
 *  A primitive expression deals with low-level
 *  POD data types and handles without
 *  doing life-cycle management for objects.
 *
 *  PrimExpr is used in the low-level code
 *  optimizations and integer analysis.
 *
 * \sa PrimExpr
 */
class PrimExprNode : public BaseExprNode {
 public:
  // runtime::DataType(dtype) åœ¨ç¼–è¯‘æ—¶å’Œè¿è¡Œæ—¶æä¾›ç²—ç²’åº¦ç±»å‹ä¿¡æ¯ã€‚ 
  // å®ƒåŠ¨æ€åœ°å†…ç½®åœ¨ PrimExpr è¡¨è¾¾å¼æ„é€ ä¸­ï¼Œå¯ç”¨äºå¿«é€Ÿç±»å‹æ£€æŸ¥ã€‚
  // å½“ PrimExpr å¯¹åº”äº i32 ç­‰ POD å€¼ç±»å‹æ—¶ï¼Œdtype è¶³ä»¥å†³å®š PrimExpr çš„ Typeã€‚
  //  å½“ dtype ä¸º DataType::Handle() æ—¶ï¼Œè¡¨è¾¾å¼å¯ä»¥å¯¹åº”æ›´ç»†ç²’åº¦çš„ Typeï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡lazyç±»å‹æ¨æ–­å¾—åˆ°ç±»å‹ã€‚
  DataType dtype;
  }
```

ä¾‹å¦‚è¡¨ç¤ºä¸€ä¸ªæ•´æ•°çš„Exprå°±å¯ä»¥é€šè¿‡ç»§æ‰¿PrimExprNodeæ¥å®ç°ï¼ŒIntImmè¡¨ç¤ºçš„æ˜¯æ•´æ•°å­—é¢å€¼è¡¨è¾¾å¼ï¼Œæ‰€ä»¥å®ƒè®°å½•äº†ä¸€ä¸ªintç±»å‹çš„valueæˆå‘˜ã€‚

```cpp
// PrimExprs that are useful as runtime containers.
//
/*!
 * \brief Constant integer literals in the program.
 * \sa IntImm
 */
class IntImmNode : public PrimExprNode {
 public:
  /*! \brief the Internal value. */
  int64_t value;
	...
};
```

RelayExprçš„å®šä¹‰å¦‚ä¸‹ï¼š

```cpp
/*!
 * \brief æ‰€æœ‰éPrim Exprçš„åŸºç¡€èŠ‚ç‚¹
 *
 * RelayExpr æ”¯æŒå¼ é‡ç±»å‹ã€å‡½æ•°å’Œ ADT ä½œä¸º
 * ä¸€ç­‰å…¬æ°‘ã€‚ å¯¹è±¡å¯¹åº”çš„ç”Ÿå‘½å‘¨æœŸ
 * ç”±è¯­è¨€éšå¼ç®¡ç†ã€‚
 *
 * \sa RelayExpr
 */
class RelayExprNode : public BaseExprNode {
 public:
  /*!
   * \brief å­˜å‚¨ç±»å‹æ¨æ–­ï¼ˆç±»å‹æ£€æŸ¥ï¼‰çš„ç»“æœã€‚
   *
   * \note è¿™å¯ä»¥åœ¨ç±»å‹æ¨æ–­ä¹‹å‰æœªå®šä¹‰ã€‚ è¯¥å€¼åœ¨åºåˆ—åŒ–æœŸé—´è¢«ä¸¢å¼ƒã€‚
   */
  mutable Type checked_type_ = Type(nullptr);
  /*!
   * \return The checked_type
   */
  inline const Type& checked_type() const;
  /*!
   * \brief æ£€æŸ¥ Expr çš„æ¨æ–­ï¼ˆæ£€æŸ¥ï¼‰ç±»å‹æ˜¯å¦ç”± TTypeNode æ”¯æŒå¹¶è¿”å›ã€‚
   *
   * \note å¦‚æœè¿™ä¸ª Expr çš„èŠ‚ç‚¹ç±»å‹ä¸æ˜¯ TTypeNodeï¼Œè¿™ä¸ªå‡½æ•°ä¼šæŠ›å‡ºä¸€ä¸ªé”™è¯¯ã€‚
   *
   * \return å¯¹åº”çš„ TTypeNode æŒ‡é’ˆã€‚
   * \tparam æˆ‘ä»¬å¯»æ‰¾çš„ç‰¹å®š TypeNodeã€‚
   */
  template <typename TTypeNode>
  inline const TTypeNode* type_as() const;

  ...
};
```

æ€»çš„æ¥è¯´ï¼Œæ— è®ºæ˜¯é«˜çº§åˆ«çš„Relayï¼ŒRelaxè¿˜æ˜¯ä½çº§åˆ«çš„TIRï¼Œå®ƒä»¬æœ€ç»ˆéƒ½æ˜¯ç”±è¿™é‡Œçš„Exprå’ŒTypeä¸ºåŸºç¡€æ¥è¡¨è¾¾çš„ã€‚å› ä¸ºå¯¹äºRelayå’ŒTIRæ¥è®²ï¼Œå®ƒä»¬çš„opå®šä¹‰éƒ½æ˜¯ç»§æ‰¿è‡ªRelayExprNodeï¼š`https://github.com/apache/tvm/blob/main/include/tvm/ir/op.h#L58`ã€‚é™¤äº†å¯¹Opåå­—ï¼Œç±»å‹ä»¥åŠå‚æ•°ï¼Œå±æ€§ç­‰å®šä¹‰å¤–è¿˜æœ‰ä¸€ä¸ªç‰¹æ®Šçš„å‚æ•°`support_level`ï¼Œä»æ³¨é‡Šä¸Šçœ‹åº”è¯¥æ˜¯ç”¨æ¥è§£é‡Šå½“å‰Opçš„ç­‰çº§ï¼Œå€¼è¶Šå°è¡¨ç¤ºè¿™ç§Opç±»å‹ç­‰çº§è¶Šé«˜ï¼ˆæš‚ä¸æ¸…æ¥šå…·ä½“çš„ä½œç”¨ï¼‰ã€‚

```cpp
// TODO(tvm-team): migrate low-level intrinsics to use Op
/*!
 * \brief Primitive Op(builtin intrinsics)
 *
 * This data structure stores the meta-data
 * about primitive operators that can be invoked via Call.
 *
 * Low-level IR intrinsics(such as libc.expf) are also
 * implemented via Op.
 *
 * \sa Op
 */
class OpNode : public RelayExprNode {
 public:
  /*! \brief name of the operator */
  String name;
  /*! \brief the type of the operator */
  mutable FuncType op_type;
  /*!
   * \brief detailed description of the operator
   *  This can be used to generate docstring automatically for the operator.
   */
  String description;
  /* \brief Information of input arguments to the operator */
  Array<AttrFieldInfo> arguments;
  /*!
   * \brief The type key of the attribute field
   *  This can be empty, in which case it defaults to anything.
   */
  String attrs_type_key;
  /*!
   * \brief attribute type index,
   * this field varies in each run and is not exposed to frontend.
   */
  uint32_t attrs_type_index{0};
  /*!
   * \brief number of input arguments to the operator,
   * -1 means it is variable length
   */
  int32_t num_inputs = -1;
  /*!
   * \brief support level of the operator,
   *  The lower the more priority it contains.
   *  This is in analogies to BLAS levels.
   */
  int32_t support_level = 10;
	...
};
```

æœ€åæˆ‘ä»¬çœ‹ä¸€ä¸‹IRModuleçš„å®šä¹‰ï¼Œ`https://github.com/apache/tvm/blob/main/include/tvm/ir/module.h#L56`ã€‚æˆ‘ä»¬è¯´è¿‡IRModuleæ˜¯TVMç¼–è¯‘çš„æœ€å°å•å…ƒï¼Œæˆ‘ä»¬å¯ä»¥ä»å®ƒçš„å®šä¹‰ä¸­å‘ç°å®ƒå°±æ˜¯ä¸€ç³»åˆ—BaseFuncï¼ˆåœ¨ä¸‹ä¸€èŠ‚Relayçš„ä»‹ç»ä¸­æˆ‘ä»¬ä¼šè®²åˆ°å®ƒçš„å®ç°ï¼‰çš„æ˜ å°„ã€‚

```cpp
/*!
 * \brief IRModule that holds functions and type definitions.
 *
 *  IRModule is the basic unit for all IR transformations across the stack.
 *
 *  Many operations require access to the global IRModule.
 *  We pass the IRModule by value in a functional style as an explicit argument,
 *  but we mutate the Module while optimizing programs.
 * \sa IRModule
 */
class IRModuleNode : public Object {
 public:
  /*! \brief A map from ids to all global functions. */
  Map<GlobalVar, BaseFunc> functions;
  /*! \brief A map from global type vars to ADT type data. */
  Map<GlobalTypeVar, TypeData> type_definitions;
  /*! \brief The source map for the module. */
  parser::SourceMap source_map;
  /* \brief Additional attributes storing meta-data about the module. */
  DictAttrs attrs;
  ...
  }
```

å…¶ä¸­type_definitionsæ˜¯å¯¹ADTçš„å®šä¹‰ï¼Œæœ¬æ–‡ä¸å…³æ³¨Relayä¸­å‡½æ•°å¼ç¼–ç¨‹çš„æ¦‚å¿µï¼Œæ‰€ä»¥ä¸å±•å¼€ADTä»¥åŠLet Bindingéƒ¨åˆ†çš„æ¦‚å¿µå’Œæºç ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥å‚è€ƒå¼ ä¼Ÿå¤§ä½¬çš„è¿™ç¯‡æ–‡ç« æˆ–è€…å®˜æ–¹æ–‡æ¡£å¯¹Relayçš„ä»‹ç»å­¦ä¹ ä¸€ä¸‹ï¼šhttps://zhuanlan.zhihu.com/p/446976730 ã€‚åé¢åœ¨ä»‹ç»Relax IRçš„æ—¶å€™æˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œå®é™…ä¸ŠRelaxç›¸æ¯”äºRelayå°±ç±»ä¼¼äºTensorFlowçš„é™æ€å›¾åˆ°PyTorchåŠ¨æ€å›¾çš„è¿‡åº¦ï¼Œæ›´åŠ å¼ºè°ƒæ•°æ®æµå›¾çš„æ¦‚å¿µè€Œéå‡½æ•°å¼ç¼–ç¨‹çš„æ¦‚å¿µï¼Œæˆ‘ä¸ªäººæ„Ÿè§‰ä¹Ÿæ˜¯ä¸ºäº†æ˜“ç”¨æ€§è€ƒè™‘å§ã€‚

## 0x1.3 Relay IR
æ¥ä¸‹æ¥æˆ‘ä»¬ç®€å•ä»‹ç»ä¸€ä¸‹Relay IRã€‚é¦–å…ˆRelay IRç›®å‰ä»ç„¶æ˜¯TVMå’Œå…¶å®ƒæ·±åº¦å­¦ä¹ æ¡†æ¶å¯¹æ¥çš„ä¸»è¦æ–¹å¼ï¼Œæˆ‘ä¹‹å‰åœ¨ã€Šã€ä»é›¶å¼€å§‹å­¦TVMã€‘ä¸‰ï¼ŒåŸºäºONNXæ¨¡å‹ç»“æ„äº†è§£TVMçš„å‰ç«¯ã€‹æ–‡ç« ä¸­ä»¥ONNXä¸ºä¾‹ä»‹ç»äº†æ¨¡å‹æ˜¯å¦‚ä½•è½¬æ¢ä¸ºRelay IRçš„ï¼Œç„¶åè¿™ä¸ªRelay IRä¼šè¢«è¿›ä¸€æ­¥å°è£…ä¸ºIRModuleç»™TVMç¼–è¯‘ã€‚

ä»æºç è§’åº¦æ¥çœ‹ï¼ŒRelayçš„åŸºç±»Exprå°±æ˜¯tvm.iråŸºç¡€è®¾æ–½ä¸­å®šä¹‰çš„RelayIRï¼ˆ`https://github.com/apache/tvm/blob/main/include/tvm/relay/expr.h#L54`ï¼‰ã€‚

```cpp
namespace relay {

using Expr = tvm::RelayExpr;
using ExprNode = tvm::RelayExprNode;
using BaseFunc = tvm::BaseFunc;
using BaseFuncNode = tvm::BaseFuncNode;
using GlobalVar = tvm::GlobalVar;
using GlobalVarNode = tvm::GlobalVarNode;
using tvm::PrettyPrint;
```

ç„¶åRelayè¿˜å®šä¹‰äº†ConstantExprï¼ŒTupleExprï¼ŒVarExprï¼ŒCallNodeExprï¼ŒLetNodeExprï¼ŒIfNodeExprç­‰å¤šç§Exprã€‚æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹ConstantExprNodeçš„å®šä¹‰ï¼Œç±»å®šä¹‰ä¸­å£°æ˜äº†æ•°æ®dataå¹¶å®šä¹‰äº†tensor_typeæ–¹æ³•è¿”å›dataçš„ç±»å‹ï¼Œç„¶åis_scalarå‡½æ•°ç”¨æ¥åˆ¤æ–­è¿™ä¸ªå¸¸é‡æ˜¯å¦ä¸ºæ ‡é‡ã€‚

```cpp
*!
 * \brief Constant tensor type.
 */
class ConstantNode : public ExprNode {
 public:
  /*! \brief The data of the tensor */
  runtime::NDArray data;

  /*! \return The corresponding tensor type of the data */
  TensorType tensor_type() const;

  /*! \return Whether it is scalar(rank-0 tensor) */
  bool is_scalar() const { return data->ndim == 0; }

	...
};
```

ç„¶åæˆ‘ä»¬å†çœ‹ä¸€ä¸‹VarNodeçš„å®šä¹‰ï¼ŒVarå°±æ˜¯Relayé‡Œé¢çš„å˜é‡ï¼Œå®ƒçš„å®šä¹‰å¦‚ä¸‹ï¼š

```cpp
/*! \brief Container for Var */
class VarNode : public ExprNode {
 public:
  /*!
   * \brief The unique identifier of the Var.
   *
   * vid will be preserved for the same Var during type inference
   * and other rewritings, while the VarNode might be recreated
   * to attach additional information.
   * This property can be used to keep track of parameter Var
   * information across passes.
   */
  Id vid;
  /*!
   * \brief type annotaion of the variable.
   * This field records user provided type annotation of the Var.
   * This field is optional and can be None.
   */
  Type type_annotation;

  /*! \return The name hint of the variable */
  const String& name_hint() const { return vid->name_hint; }
};
```

é¦–å…ˆId vidè¡¨ç¤ºçš„å°±æ˜¯å˜é‡çš„åç§°ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ¯”å¦‚æˆ‘ä»¬åœ¨å¯è§†åŒ–Relay IRæ—¶çœ‹åˆ°çš„ä»¥@å¼€å¤´çš„å…¨å±€å˜é‡ä»¥åŠ%å¼€å¤´çš„å±€éƒ¨å˜é‡ã€‚è¿™é‡Œçš„type_annotationè¡¨ç¤ºå˜é‡çš„ç±»å‹æ³¨é‡Šï¼Œè¿™ä¸ªå­—æ®µæ˜¯å¯é€‰çš„ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å†çœ‹ä¸€ä¸ªFunctionNodeçš„å®šä¹‰ï¼ŒFunctionNodeå°±æ˜¯IRModuleä¸­çš„BaseFuncåœ¨Relayé‡Œé¢çš„å…·ä½“å®ç°äº†ï¼š

```cpp
/*!
 * \brief Relay Function container
 * \sa Function
 */
class FunctionNode : public BaseFuncNode {
 public:
  /*! \brief Function parameters */
  tvm::Array<Var> params;
  /*!
   * \brief
   * The expression which represents the computation of the function,
   * the expression may reference the parameters, and the type of it
   * or sub-expressions may reference the type variables.
   */
  Expr body;
  /*! \brief User annotated return type of the function. */
  Type ret_type;
  /*!
   * \brief Type parameters of the function.
   *  Enables the function to vary its type based on these.
   *  This corresponds to template paramaters in c++'s terminology.
   *
   * \note This can be usually empty for non-polymorphic functions.
   */
  tvm::Array<TypeVar> type_params;
}
```

FunctionNodeçš„å®šä¹‰ä¸­æœ‰å‡½æ•°å‚æ•°ï¼Œå‡½æ•°ä½“ä»¥åŠè¿”å›å€¼ç±»å‹å’Œå‚æ•°ç±»å‹ã€‚å…¶å®ƒç±»å‹çš„Relayè¡¨è¾¾å¼å®šä¹‰æˆ‘ä»¬å°±ä¸çœ‹äº†ï¼Œæ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥ç›´æ¥åœ¨`https://github.com/apache/tvm/tree/main/include/tvm/relay`é˜…è¯»ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬è§£æä¸€ä¸‹Relayä¸­çš„Opå®šä¹‰ï¼Œä¸Šä¸€èŠ‚tvm.iråŸºç¡€è®¾æ–½ä¸­æˆ‘ä»¬å·²ç»æåˆ°æ— è®ºæ˜¯Relayè¿˜æ˜¯TIRçš„Opéƒ½å®šä¹‰ä¸ºä¸€ç§RelayExprï¼Œä¹Ÿå°±æ˜¯OpNodeçš„å®šä¹‰ã€‚æˆ‘ä»¬è¿™é‡Œçœ‹ä¸€ä¸ªRelayå®šä¹‰çš„bias_add Opçš„ä¾‹å­æ¥åŠ æ·±ç†è§£ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºBiasAdd Opå®šä¸€ä¸ªå±æ€§ç±»å‹è®°å½•å®ƒæ‰€æœ‰çš„å±æ€§ï¼Œ`https://github.com/apache/tvm/blob/main/include/tvm/relay/attrs/nn.h#L35-L48`ï¼Œå±æ€§å®šä¹‰æ—¶æˆ‘ä»¬è¿˜å¯ä»¥ç»™å±æ€§è®¾ç½®æè¿°å’Œé»˜è®¤å€¼ï¼š

```cpp
/*!
 * \brief Add a 1D Tensor to an axis of a data.
 *
 * \note bias_add is a special add operator that is in nn
 *   and enables automatic derivation of bias's shape.
 *   You can directly use add for more generalized case.
 */
struct BiasAddAttrs : public tvm::AttrsNode<BiasAddAttrs> {
  int axis;

  TVM_DECLARE_ATTRS(BiasAddAttrs, "relay.attrs.BiasAddAttrs") {
    TVM_ATTR_FIELD(axis).describe("The axis to add the bias").set_default(1);
  }
};
```

ç¬¬äºŒæ­¥ï¼Œæˆ‘ä»¬ç»™Biass Add Opå®šä¹‰ç±»å‹æ¨æ–­å‡½æ•°ï¼ˆ`https://github.com/apache/tvm/blob/main/src/relay/op/nn/nn.cc#L52`ï¼‰ï¼š

```cpp
bool BiasAddRel(const Array<Type>& types, int num_inputs, const Attrs& attrs,
                const TypeReporter& reporter) {
  ICHECK_EQ(types.size(), 3);
  const auto* data = types[0].as<TensorTypeNode>();
  if (data == nullptr) return false;

  const BiasAddAttrs* param = attrs.as<BiasAddAttrs>();
  ICHECK(param != nullptr);
  int axis = param->axis;
  if (axis < 0) {
    axis = data->shape.size() + axis;
  }
  if (axis >= static_cast<int>(data->shape.size()) || axis < 0) {
    reporter->GetDiagCtx().EmitFatal(Diagnostic::Error(reporter->GetSpan())
                                     << "The axis in bias_add must be in range for the shape; "
                                     << "attempted to access index " << param->axis << " of "
                                     << PrettyPrint(data->shape));
    return false;
  }

  // assign output type
  reporter->Assign(types[1], TensorType({data->shape[axis]}, data->dtype));
  reporter->Assign(types[2], types[0]);
  return true;
}
```

å‡è®¾è¿™é‡ŒæŒ‡å®šçš„æ“ä½œæ˜¯ c = nn.bias_add(a , b)ï¼Œè¿™é‡Œçš„é€»è¾‘å°±æ˜¯æ ¹æ®è¾“å…¥açš„ç±»å‹æ¨æ–­bå’Œcçš„ç±»å‹å¹¶é‡å†™ï¼ˆAssignï¼‰ã€‚

ç¬¬ä¸‰æ­¥ï¼Œæˆ‘ä»¬æŠŠnn.BiasAdd Opæ³¨å†Œåˆ°å…¨å±€è¡¨ä¸­ï¼ˆ`https://github.com/apache/tvm/blob/main/src/relay/op/nn/nn.cc#L88-L103`ï¼‰ï¼š

```cpp
RELAY_REGISTER_OP("nn.bias_add")
    .describe(R"code(Add bias to an axis of the input.
)code" TVM_ADD_FILELINE)
    .set_attrs_type<BiasAddAttrs>()
    .set_num_inputs(2)
    .add_argument("data", "nD Tensor", "Input data.")
    .add_argument("bias", "1D Tensor", "Bias.")
    .set_support_level(1)
    .add_type_rel("BiasAdd", BiasAddRel)
    .set_attr<TOpPattern>("TOpPattern", kBroadcast)
    .set_attr<FTVMCompute>("FTVMCompute", [](const Attrs& attrs, const Array<te::Tensor>& inputs,
                                             const Type& out_type) {
      const auto* param = attrs.as<BiasAddAttrs>();
      return tvm::Array<tvm::te::Tensor>{topi::nn::bias_add(inputs[0], inputs[1], param->axis)};
    });
```

æ³¨æ„åˆ°è¿™é‡Œçš„op name/describe/num_inputs/arguments/support_levelæ˜¯å¯¹åº”äº†OpNodeç±»çš„æˆå‘˜ï¼Œç„¶åOpNodeè¿˜æœ‰ä¸€ä¸ªattrs_type_keyå’Œattrs_type_indexæˆå‘˜å¯¹åº”çš„å°±æ˜¯BiasAddAttrsäº†ã€‚ç„¶åæˆ‘ä»¬å†çœ‹ä¸€ä¸‹è¿™ä¸ªFTVMComputeè¿™ä¸ªç”¨æ¥æè¿°Opè®¡ç®—é€»è¾‘çš„é¢å¤–å±æ€§ï¼Œå®ƒä½¿ç”¨Opçš„è¾“å…¥ï¼Œå±æ€§å‚æ•°ä»¥åŠè¾“å‡ºç±»å‹æ¥ç¡®å®šè¿™ä¸ªOpçš„è®¡ç®—é€»è¾‘ã€‚

åˆ°è¿™é‡Œå¯èƒ½ä½ è¿˜æœ‰ä¸€ä¸ªç–‘é—®ï¼Œæˆ‘ä»¬çŸ¥é“TVMçš„æ ¸å¿ƒæ˜¯è®¡ç®—å’Œè°ƒåº¦åˆ†ç¦»ï¼Œ**Relay Opçš„è°ƒåº¦é€»è¾‘æ˜¯æ€ä¹ˆæ³¨å†Œçš„å‘¢**ï¼Ÿ

TVMæ²¡æœ‰ä¸ºæ¯ä¸ªRelay OPæ³¨å†Œcomputeå’Œscheduleï¼Œè€Œæ˜¯ä¸ºå…¶æ³¨å†Œfcomputeå’Œfscheduleï¼Œç„¶åæ ¹æ®è¾“å…¥å’Œå±æ€§å‚æ•°ï¼Œè¾“å‡ºç±»å‹ç­‰ç”Ÿæˆå¯¹åº”çš„computeå’Œschedulï¼Œè¿™ç§computeå’Œscheduleçš„ç»„åˆå¯¹åº”äº†OpImplementationï¼ˆ`https://github.com/apache/tvm/blob/main/include/tvm/relay/op_strategy.h#L39`ï¼‰ã€‚

```cpp
/*!
 * \brief Operator implementation that includes compute and schedule function.
 */
class OpImplementationNode : public Object {
 public:
  /*! \brief Compute function */
  FTVMCompute fcompute;
  /*! \brief Schedule function */
  FTVMSchedule fschedule;
  /*! \brief Name of the implementation */
  String name;
  /*! \brief Priority level */
  int plevel;

  void VisitAttrs(tvm::AttrVisitor* v) {
    v->Visit("name", &name);
    v->Visit("plevel", &plevel);
  }

  static constexpr const char* _type_key = "relay.OpImplementation";
  TVM_DECLARE_FINAL_OBJECT_INFO(OpImplementationNode, Object);
};

/*!
 * \brief Operator implementation class.
 */
class OpImplementation : public ObjectRef {
 public:
  /*!
   * \brief Invoke the operator compute function.
   * \param attrs The attribute of the primitive
   * \param inputs The input tensors.
   * \param out_type The output type information.
   * \return The output compute description of the operator.
   */
  TVM_DLL Array<te::Tensor> Compute(const Attrs& attrs, const Array<te::Tensor>& inputs,
                                    const Type& out_type);
  /*!
   * \brief Build the computation schedule.
   * \param attrs The attribute of the node.
   * \param outs The output tensors.
   * \param target The build target.
   * \return The computation schedule.
   */
  TVM_DLL te::Schedule Schedule(const Attrs& attrs, const Array<te::Tensor>& outs,
                                const Target& target);

  TVM_DEFINE_OBJECT_REF_METHODS(OpImplementation, ObjectRef, OpImplementationNode);
};
```

ä»OpImplementationç±»çš„å®ç°æˆ‘ä»¬çœ‹å‡ºï¼Œå®ƒçš„Computeå’ŒScheduleå°±æ˜¯æ ¹æ®fcomputeå’Œfscheduleæ¥ç”Ÿæˆçš„ã€‚

```cpp
Array<te::Tensor> OpImplementation::Compute(const Attrs& attrs, const Array<te::Tensor>& inputs,
                                            const Type& out_type) {
  return (*this)->fcompute(attrs, inputs, out_type);
}

te::Schedule OpImplementation::Schedule(const Attrs& attrs, const Array<te::Tensor>& outs,
                                        const Target& target) {
  return (*this)->fschedule(attrs, outs, target);
}
```

ç„¶åç”±äºç‰¹å®šçš„OpImplementationéœ€è¦ç‰¹å®šçš„æ¡ä»¶ï¼Œæ‰€ä»¥åˆæŒ‰ç…§è¿™ä¸ªæ¡ä»¶ï¼ˆconditionï¼‰è¿›è¡Œåˆ†ç»„ï¼Œæ¯ä¸€ç»„è¢«å«ä½œOpSpecializationï¼ˆ`https://github.com/apache/tvm/blob/main/include/tvm/relay/op_strategy.h#L92`ï¼‰

```cpp
/*!
 * \brief Specialized implementations for operators under certain conditions.
 */
class OpSpecializationNode : public Object {
 public:
  /*! \brief List of implementations. */
  Array<OpImplementation> implementations;
  /*! \brief Condition to enable the specialization.
   *    Could be undefined to represent generic case. */
  te::SpecializedCondition condition;

  void VisitAttrs(tvm::AttrVisitor* v) {
    v->Visit("condition", &condition);
    v->Visit("implementations", &implementations);
  }

  static constexpr const char* _type_key = "relay.OpSpecialization";
  TVM_DECLARE_FINAL_OBJECT_INFO(OpSpecializationNode, ExprNode);
};

```

æœ€åä½¿ç”¨ä¸€ä¸ªOpStrategyç±»æ¥è®°å½•è¿™ä¸ªRelay Opçš„æ‰€æœ‰OpImplementationã€‚ï¼ˆ`https://github.com/apache/tvm/blob/main/include/tvm/relay/op_strategy.h#L130`ï¼‰

```cpp
/*!
 * \brief Operator strategy to choose implementation.
 */
class OpStrategyNode : public Object {
 public:
  /*! \brief List of operator specializations. */
  Array<OpSpecialization> specializations;

  void VisitAttrs(tvm::AttrVisitor* v) { v->Visit("specializations", &specializations); }

  static constexpr const char* _type_key = "relay.OpStrategy";
  TVM_DECLARE_FINAL_OBJECT_INFO(OpStrategyNode, ExprNode);
};

/*!
 * \brief Operator strategy class.
 */
class OpStrategy : public ObjectRef {
 public:
  /*!
   * \brief Add an implementation.
   * \param fcompute Compute function
   * \param fschedule Schedule function
   * \param name Name of the implementation
   * \param plevel Priority level of the implementation
   */
  TVM_DLL void AddImplementation(FTVMCompute fcompute, FTVMSchedule fschedule, String name,
                                 int plevel);

  TVM_DEFINE_MUTABLE_OBJECT_REF_METHODS(OpStrategy, ObjectRef, OpStrategyNode);
};
```

å…¶ä¸­ï¼ŒAddImplementationå‡½æ•°é€šè¿‡FFIæœºåˆ¶åœ¨Pythonå±‚ä¹Ÿå¯ä»¥è°ƒç”¨ï¼Œå¤§å¤šæ•°çš„Relay Opéƒ½æ˜¯åœ¨Pythonç«¯æ³¨å†Œå®ƒçš„Strategyã€‚æˆ‘ä»¬ä»¥Relayçš„nn.Softmax Opä¸ºä¾‹çœ‹ä¸€ä¸‹ï¼Œå®ƒçš„Strategyï¼ˆåŒ…å«fcompute+fscheduleï¼‰æ³¨å†Œåœ¨`https://github.com/apache/tvm/blob/main/python/tvm/relay/op/strategy/generic.py#L152`ã€€å’Œ `https://github.com/apache/tvm/blob/main/python/tvm/relay/op/strategy/cuda.py#L78-L94` ã€‚

```python
@override_native_generic_func("softmax_strategy")
def softmax_strategy(attrs, inputs, out_type, target):
    """softmax generic strategy"""
    strategy = _op.OpStrategy()
    strategy.add_implementation(
        wrap_compute_softmax(topi.nn.softmax),
        wrap_topi_schedule(topi.generic.schedule_softmax),
        name="softmax.generic",
    )
    return strategy

@softmax_strategy.register(["cuda", "gpu"])
def softmax_strategy_cuda(attrs, inputs, out_type, target):
    """softmax cuda strategy"""
    strategy = _op.OpStrategy()
    strategy.add_implementation(
        wrap_compute_softmax(topi.nn.softmax),
        wrap_topi_schedule(topi.cuda.schedule_softmax),
        name="softmax.cuda",
    )
    if target.kind.name == "cuda" and "cudnn" in target.libs:
        strategy.add_implementation(
            wrap_compute_softmax(topi.cuda.softmax_cudnn),
            wrap_topi_schedule(topi.cuda.schedule_softmax_cudnn),
            name="softmax.cudnn",
            plevel=15,
        )
    return strategy

```

ç„¶ååœ¨`https://github.com/apache/tvm/blob/main/python/tvm/relay/op/nn/_nn.py#L40`å°†å®ç°çš„Strategyæ³¨å†Œåˆ°nn.softmax opã€‚

```python
# softmax
reg.register_strategy("nn.softmax", strategy.softmax_strategy)
```

å…¶å®Relay Opé™¤äº†Strategyå±æ€§ä¹‹å¤–ï¼Œè¿˜åˆä¸€äº›å…¶å®ƒçš„å±æ€§ï¼Œæ¯”å¦‚æˆ‘ä»¬åœ¨`https://github.com/apache/tvm/blob/main/src/relay/op/nn/convolution.cc#L176` è¿™é‡Œå¯ä»¥çœ‹åˆ°Opè¿˜å¯ä»¥æœ‰FInferCorrectLayoutå’ŒTOpPatternå±æ€§ç”¨äºåç»­ä¼˜åŒ–ï¼ˆæ¯”å¦‚ç®—ç¬¦èåˆPasså°±ä¾èµ–äº†TOpPatternå±æ€§ï¼ŒAnsorçš„data layerout transformä¾èµ–FInferCorrectLayoutå±æ€§ï¼‰ã€‚

```cpp
RELAY_REGISTER_OP("nn.conv1d")
    .describe(R"code(1D convolution layer (e.g. spatial convolution over sequences).
This layer creates a convolution kernel that is convolved
with the layer input to produce a tensor of outputs.
- **data**: This depends on the `layout` parameter. Input is 3D array of shape
            (batch_size, in_channels, width) if `layout` is `NCW`.
- **weight**: (channels, in_channels, kernel_size)
- **out**:  This depends on the `layout` parameter. Output is 3D array of shape
            (batch_size, channels, out_width) if `layout` is `NCW`.
)code" TVM_ADD_FILELINE)
    .set_attrs_type<Conv1DAttrs>()
    .set_num_inputs(2)
    .add_argument("data", "Tensor", "The input tensor.")
    .add_argument("weight", "Tensor", "The weight tensor.")
    .set_support_level(2)
    .add_type_rel("Conv1D", Conv1DRel)
    .set_attr<FInferCorrectLayout>("FInferCorrectLayout", ConvInferCorrectLayout<Conv1DAttrs>)
    .set_attr<TOpPattern>("TOpPattern", kOutEWiseFusable);

```

Relayå°±æš‚æ—¶è®²åˆ°è¿™é‡Œï¼ŒRelay IRåšä¸ºå‡½æ•°å¼é£æ ¼çš„IRç›®å‰æ˜¯TVMå’Œå…¶å®ƒæ·±åº¦å­¦ä¹ æ¡†æ¶äº¤äº’çš„æ¡¥æ¢å¹¶ä¸”ä¹Ÿç»å†äº†å¤šå¹´çš„ç»´æŠ¤å®Œå¤‡æ€§æ˜¯æ¯”è¾ƒå¥½çš„ï¼ˆæ”¯æŒTensorFlowï¼ŒPyTorchï¼ŒPaddleï¼ŒOneFlowå„ç§ä¸»æµæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼‰ã€‚ä½†Relayçš„ç¼ºç‚¹åœ¨äºç”±äºå…±ç”¨äº†TVMçš„ tvm.ir åŸºç¡€è®¾æ–½æ²¡åŠæ³•æ”¯æŒDynamic Shapeå¯¼è‡´Relay IRä¹Ÿæ— æ³•æ”¯æŒDynamic Shapeï¼Œå¹¶ä¸”Relay IRè¿™ç§å‡½æ•°å¼ç¼–ç¨‹çš„é£æ ¼ç›¸æ¯”äºæ•°æ®æµå›¾å½¢å¼çš„è®¡ç®—å›¾æ¥è¯´ä¸æ˜¯å¤ªç›´è§‚ã€‚

## 0x1.4 Relax
ç”±äºRelaxè¿™ä¸ªå‰ç«¯è¿˜æ²¡æœ‰æ­£å¼upstreamåˆ°apache tvmä¸»åˆ†æ”¯ï¼Œæ‰€ä»¥æˆ‘è¿™é‡Œå°±ä¸ä»æºç çš„è§’åº¦æ¥çœ‹ã€‚æˆ‘ä»¬å¯ä»¥ä»Relaxçš„wikiå‘ç°å®ƒä¸ä»…åŸç”Ÿçš„æ”¯æŒåŠ¨æ€Shapeï¼ˆé€šè¿‡æä¾›DynTensorçš„æŠ½è±¡å¹¶å°†Shapeä»Tensorçš„typeä¸­åˆ†ç¦»å‡ºæ¥å®ç°çš„ï¼‰è¿˜åšäº†ä¸€ä¸ªTVM UnifyæŠ½è±¡ï¼Œä¹Ÿå°±æ˜¯å¤©å¥‡åœ¨ã€Šæ–°ä¸€ä»£æ·±åº¦å­¦ä¹ ç¼–è¯‘æŠ€æœ¯å˜é©å’Œå±•æœ›ã€‹ä¸€æ–‡ä¸­æåˆ°çš„ï¼Œè¿™ä¸ªç‰¹ç‚¹å¯ä»¥è®©ä¸åŒçš„æŠ½è±¡ä¹‹é—´ç›¸äº’äº¤äº’å’Œè”åˆä¼˜åŒ–ã€‚è¿™é‡Œæåˆ°çš„æŠ½è±¡åŒ…å«AutoTensorizationç”¨æ¥è§£å†³ç¡¬ä»¶æŒ‡ä»¤å£°æ˜å’Œå¼ é‡ç¨‹åºå¯¹æ¥ï¼ŒTVM FFIï¼ˆPackedFuncï¼‰æœºåˆ¶ä½¿å¾—æˆ‘ä»¬å¯ä»¥çµæ´»åœ°å¼•å…¥ä»»æ„çš„ç®—å­åº“å’Œè¿è¡Œåº“å‡½æ•°å¹¶ä¸”åœ¨å„ä¸ªç¼–è¯‘æ¨¡å—å’Œè‡ªå®šä¹‰æ¨¡å—é‡Œé¢ç›¸äº’è°ƒç”¨ã€‚TensorIRè´Ÿè´£å¼ é‡çº§åˆ«ç¨‹åºå’Œç¡¬ä»¶å¼ é‡æŒ‡ä»¤çš„æ•´åˆã€‚è¿˜æœ‰è¿™é‡Œçš„Relax (Relax Next)ã€‚æˆ‘ä»¬å¯ä»¥ä»ä¸‹é¢çš„ä¾‹å­ä½“ä¼šï¼š

```python
import tvm.script
from tvm.script import tir as T, relax as R

@tvm.script.ir_module
class MyIRModule:
    @T.prim_func
    def tir_exp_func(x: T.handle, y: T.handle): ## <= D2
        X = T.match_buffer(x, (n,), "float32")
        Y = T.match_buffer(y, (n,), "float32")
        with T.grid(n) as i:
            Y[i] = T.exp(X[i]) 

    @R.function
    def relax_func(x: R.Tensor[(n, k), "f32"], w: R.Tensor[_, "f32"]):
        # n, k above are implicitly defined by the signature
        # so we will be able to refer to n, k in the later part of the program
        with R.dataflow(): ### <= D0
            lv0 = R.match_shape(w, (k, m)) ## <= D1
            lv1: R.Tensor[(n, m), "f32"] = R.dot(x, lv0)
            lv2: R.Tensor[(n * m,), "f32"] = R.flatten(lv1) ## <= D1
            lv3: R.Shape = (n * m,)  ## <= D1 
            gv0: R.Tensor[lv2, "f32"] = R.call_tir(lv2, tir_exp_func, [lv3])   ## <= D2
            R.outputs(gv0)

        R.call_packed("custom_inplace_update", gv0)  ## <= D0, D2
        return gv0 
```

æ³¨æ„è¿™é‡Œå±•ç¤ºçš„ä»£ç ç‰‡æ®µæ˜¯Relax wikiæä¾›çš„ï¼Œç”±äºæ²¡æœ‰upstreamä¸»åˆ†æ”¯ï¼Œå®ƒçš„ç”¨æ³•ä¹Ÿè®¸ä¼šæœ‰å¾®å°å˜åŒ–ã€‚æˆ‘ä»¬ä»è¿™ä¸ªä»£ç ä¸­å¯ä»¥çœ‹åˆ°ï¼ŒRelaxæŠŠRelax Functionå’ŒTIR Functionæ”¾åˆ°äº†åŒä¸€ä¸ªIRModuleï¼ˆæœ€å°çš„ç¼–è¯‘å•å…ƒï¼‰ä¹Ÿå°±æ˜¯è¯´åœ¨ä»»æ„æ—¶åˆ»æˆ‘ä»¬éƒ½å¯ä»¥åŒæ—¶æ‹¿åˆ°è¿™ä¸¤ä¸ªä¸åŒå±‚æ¬¡çš„IRè¿›è¡Œä¿®æ”¹ï¼ˆæˆ–è€…è¯´è”åˆä¼˜åŒ–ï¼‰è¿™å°±æ‘†è„±äº†ç¼–è¯‘å™¨èŒƒå¼é‡Œå› ä¸ºLowerå¯¼è‡´ä¸¢å¤±é«˜å±‚è¯­ä¹‰ä¿¡æ¯æ— æ³•è”åˆä¼˜åŒ–çš„é—®é¢˜ã€‚çŸ¥ä¹ä¸Šæ€è¿œæŒ‡å‡ºäº†ä¸€ä¸ªå¾ˆç»å…¸çš„ä¾‹å­ï¼Œæˆ‘è¿™é‡Œé™„ä¸Šä»–å›ç­”é“¾æ¥ï¼ˆ`https://www.zhihu.com/question/522101384/answer/2391922144`ï¼‰å¹¶æˆªå›¾æ¥è¯´æ˜ä¸€ä¸‹ï¼š

![æ¥è‡ªå†¯æ€è¿œçš„å›ç­”ï¼Œä¾µåˆ ï¼šhttps://www.zhihu.com/question/522101384/answer/2391922144](https://img-blog.csdnimg.cn/e326ba715de34b7686d7e8e158e1686f.png)

æ¥ä¸‹æ¥æˆ‘ä»¬ç¿»è¯‘ä¸€ä¸‹Relaxçš„è®¾è®¡å…³é”®ç‚¹æ¥è¿›ä¸€æ­¥ä½“ä¼šRelaxç›¸æ¯”äºRelayçš„å˜åŒ–ï¼ˆä¸­é—´æ’äº†ä¸€äº›ä¸ªäººç†è§£ï¼‰ã€‚

#### D0ï¼šæ•°æ®æµå—ä½œä¸ºç¬¬ä¸€ä¼˜å…ˆçº§çš„æ„é€ 
å¤§éƒ¨åˆ†çš„relax_funcéƒ½å°è£…åœ¨with R.dataflow()æ„é€ é‡Œé¢ã€‚æ•°æ®æµå—ä¸‹çš„æ‰€æœ‰æ“ä½œéƒ½æ˜¯æ²¡æœ‰å‰¯ä½œç”¨çš„ï¼Œå¹¶ä¸”ä¸åŒ…å«é«˜çº§çš„æ§åˆ¶æµï¼ˆæ¯”å¦‚if-then-elseï¼‰æˆ–è€…åµŒå¥—åŒºåŸŸã€‚

ä¸€ä¸ªæ•°æ®æµå—å¯ä»¥æœ‰æ•ˆåœ°è§†ä¸ºåµŒå…¥åœ¨ç¨‹åºé‡Œé¢çš„è®¡ç®—å›¾ã€‚è¯·æ³¨æ„ï¼Œæ•°æ®æµå—é‡Œé¢çš„å¤§å¤šæ•°ç»‘å®šå˜é‡ï¼ˆä¸Šé¢Relaxè„šæœ¬ä¸­çš„lv0, lv1, lv2, lv3ï¼‰æ˜¯localçš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä»…æ˜¯å—å†…å¯è§çš„ã€‚è¿™äº›å˜é‡å¯ä»¥è¢«è§†ä¸ºè®¡ç®—å›¾çš„â€œå†…éƒ¨èŠ‚ç‚¹â€ã€‚æˆ‘ä»¬å¯ä»¥å°†å˜é‡æ ‡è®°ä¸ºè¾“å‡ºï¼ˆgv0ï¼‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥å˜é‡å°†åœ¨ç¨‹åºçš„åé¢éƒ¨åˆ†å¯è§ã€‚è¿™äº›è¾“å‡ºå˜é‡å¯ä»¥è¢«è§†ä¸ºè®¡ç®—å›¾ä¸­çš„è¾“å‡ºèŠ‚ç‚¹ã€‚

è¯·æ³¨æ„ï¼Œ` R.call_packed("custom_inplace_update", gv0)` åœ¨æ•°æ®æµå—ä¹‹å¤–ã€‚æ•°æ®æµå—ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹éƒ½å¯èƒ½äº§ç”Ÿå‰¯ä½œç”¨ã€‚å› æ­¤ï¼Œé™¤éæˆ‘ä»¬è¿›è¡Œæ›´ä»”ç»†çš„åˆ†æï¼Œå¦åˆ™æˆ‘ä»¬æ— æ³•æ‰§è¡Œä¼˜åŒ–ï¼Œä¾‹å¦‚æ ¹æ®æ‹“æ‰‘é¡ºåºé‡æ–°æ’åºè¿™äº›ç»‘å®šã€‚æˆ‘ä»¬é¢„è®¡å¤§å¤šæ•°ä¼˜åŒ–å°†å‘ç”Ÿåœ¨æ•°æ®æµå—çº§åˆ«ã€‚è¿™äº›ä¼˜åŒ–å¯ä»¥ç”±ç†Ÿæ‚‰è®¡ç®—å›¾æ¦‚å¿µçš„ ML å·¥ç¨‹å¸ˆå®Œæˆã€‚éš”ç¦»å’Œè¡¨ç¤ºæœ‰æ•ˆç»„ä»¶çš„èƒ½åŠ›è¿˜ä¸ºéœ€è¦å®ƒä»¬çš„åœ°æ–¹æä¾›äº†æ›´é«˜çº§åˆ«çš„ä¼˜åŒ–æœºä¼šã€‚

#### D1ï¼šå½¢çŠ¶æ¨å¯¼ä½œä¸ºç¬¬ä¸€ä¼˜å…ˆçº§çš„è®¡ç®—
å½¢çŠ¶æ¨å¯¼å¯¹äºåŠ¨æ€æ¨¡å‹å·¥ä½œè´Ÿè½½è‡³å…³é‡è¦ã€‚ åœ¨åŠ¨æ€å½¢çŠ¶è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦åœ¨è¿è¡Œè®¡ç®—ä¹‹å‰è®¡ç®—ä¸­é—´å¼ é‡çš„å½¢çŠ¶ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¤„ç†å½¢çŠ¶æœ¬èº«ä¾èµ–äºæ•°æ®ï¼ˆä¾‹å¦‚unique opï¼‰çš„æƒ…å†µã€‚ æœ€åï¼Œå¤§å¤šæ•°åŠ¨æ€å½¢çŠ¶å·¥ä½œè´Ÿè½½ä»ç„¶åŒ…å«å¤§é‡ï¼ˆéƒ¨åˆ†ï¼‰é™æ€å½¢çŠ¶ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ©ç”¨è¿™äº›é™æ€å½¢çŠ¶ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ã€‚

```python
from tvm.script import relax as R

@R.function
def shape_example(x: R.Tensor[(n, 2, 2), "f32"]):
    with R.dataflow():
        # symbolic and static shape deduction
        lv0: R.Tensor[(n, 4), "f32"] = R.reshape(x, (n, 4)) 
        lv1: R.Tensor[(n * 4,), "f32"] = R.flatten(lv0)
        lv2: R.Shape = (n * 4,)
        # external opaque shape function
        lv3: R.Shape = R.call_packed("myshape_func", lv2)
        lv4: R.Tensor[lv3, "f32"] = R.call_tir(lv3, "custom_func", [lv1]) 
        # data dependent case
        lv5: R.Tensor[_, "f32"] = R.unique(lv4)
        # re-match shape
        lv6: R.Tensor[(m,), "f32"] = R.match_shape(lv5, (m,))
        gv0: R.Tensor[(m,), "f32"] = R.exp(lv6)
        R.outputs(gv0)
    return gv0
```

ä¸Šè¿°ç¨‹åºæ¶µç›–äº†å½¢çŠ¶æ¨æ–­çš„å…¸å‹åœºæ™¯ï¼ˆåœ¨æ³¨é‡Šä¸­æ ‡è®°ï¼‰ã€‚ é‡è¦çš„æ˜¯ï¼Œå½¢çŠ¶ç°åœ¨ä¸å¼ é‡å€¼ä¸€èµ·æˆä¸ºè®¡ç®—çš„ä¸€éƒ¨åˆ†ã€‚ è¿™åæ˜ äº†å½¢çŠ¶çš„è®¡ç®—å¯ä»¥åœ¨è¿è¡Œæ—¶å‘ç”Ÿçš„äº‹å®ã€‚

è€Œæ–‡æœ¬æ ¼å¼ç±»å‹æ³¨é‡Š `lv0: R.Tensor[(n, 4), "f32"]` æ˜¾ç¤ºäº†æ¯ä¸ªShapeçš„å€¼ã€‚ è¿™åªæ˜¯ä¸€ä¸ªè¯­æ³•ç³–ï¼Œä» IR çš„è§’åº¦æ¥çœ‹ï¼ŒShapeå­—æ®µ `(n, 4)` ä¸æ˜¯ `lv0.checked_type` çš„ä¸€éƒ¨åˆ†ã€‚ lv0 çš„ç±»å‹æ˜¯ `DynTensor(rank=2, dtype="f32")`ï¼ŒShapeæ˜¯é™„åŠ åˆ°æ¯ä¸ª Expr çš„ç‰¹æ®Šå€¼å­—æ®µã€‚ æˆ‘ä»¬åšå‡ºè¿™ä¸ªæ˜¾å¼çš„é€‰æ‹©æ˜¯ä¸ºäº†ç®€åŒ–ç±»å‹æ¨æ–­ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸éœ€è¦è¿›å…¥å®Œå…¨ä¾èµ–ç±»å‹çš„é¢†åŸŸã€‚

æœ‰ä¸¤ä¸ªä¸ç¬¦å·Shapeè®¡ç®—ç›¸å…³çš„å…³é”®ç»“æ„ï¼š

##### D1a: match_shape

`value = match_shape(lhs, pattern)`

å½¢çŠ¶åŒ¹é…æ„é€ æ¥å—ä¸€ä¸ª lhs å€¼å’Œpatternï¼ˆæ•´å‹ç¬¦å·è¡¨è¾¾å¼ï¼‰ã€‚ å®ƒæœ‰ä¸¤ä¸ªé‡è½½è¯­ä¹‰ï¼š

- å½“ lhs ä¸º Tensor æ—¶ï¼Œå°† lhs.shape åŒ¹é…åˆ° pattern ä¸­ï¼Œå¦‚æœç¬¬ä¸€æ¬¡å‡ºç°åœ¨ pattern ä¸­ï¼Œåˆ™å¡«å……å¯¹åº”çš„æ•´å‹ç¬¦å·å˜é‡ï¼Œç„¶åè¿”å›ä¸€ä¸ªä¸ lhs ç›¸åŒä½† shape å­—æ®µæ›´æ–°ä¸º pattern çš„ Tensorã€‚
- lhs ä¹Ÿå¯ä»¥æ˜¯ç›´æ¥åŒ¹é… pattern çš„ Shapeã€‚ å½“æˆ‘ä»¬æƒ³è¦åˆ†ç¦»å‡ºä¸å¯¹åº”äºä»»ä½•å¼ é‡å€¼çš„ Shape å‡½æ•°æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚

æ¯”å¦‚ï¼š

```python
from tvm.script import relax as R

@R.function
def shape_example(x: R.Tensor[_, "f32"], y: R.Tensor[_, "f32"]):
    with R.dataflow():
        # the match shape defines n, m because it appears for the first time
        lv0: R.Tensor[(n, m)] = R.match_shape(x, (n, m))
        # the second occurance of n, m will translate into an assertion 
        # that y's shape equals (n, m)
        lv1: R.Tensor[(n, m)] = R.match_shape(y, (n, m)) 
        # we can also call match_shape on shape expressions
        lv2: Shape = R.match_shape(R.shape_of(y), (n, m)) 
```

ç‰¹åˆ«æ³¨æ„è¿™é‡Œlv2çš„Shapeå°±è¢«è®¾ç½®ä¸º(n, m)ï¼Œå¹¶ä¸”match_shapeçš„lhsæ˜¯ä¸€ä¸ªShapeè¡¨è¾¾å¼ï¼Œè€Œä¸æ˜¯Tensorã€‚

##### D1b. ä»ç¬¦å·æ•´æ•°å…ƒç»„æ„é€ Shape
åœ¨æˆ‘ä»¬å¾—åˆ° n å’Œ m ç­‰ç¬¦å·åŒ–æ•´æ•°ä¹‹åã€‚ æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬é‡æ–°ç»„åˆåœ¨ä¸€èµ·ä»¥å½¢æˆä¸€ä¸ª Exprã€‚ ä»»ä½•ç¬¦å·æ•´æ•°è¡¨è¾¾å¼çš„å…ƒç»„éƒ½å¯ä»¥åœ¨ Relax ä¸­è¢«è¯†åˆ«ä¸ºShape å€¼ã€‚ æ¯”å¦‚ (n, m) å°±æ˜¯ä¸€ä¸ªè¡¨ç¤º Shape çš„å€¼ã€‚

##### Shapeä¼ æ’­çš„æ–¹æ³•
é‡è¦çš„æ˜¯ï¼Œç°åœ¨Shapeæ˜¯è®¡ç®—è¿‡ç¨‹ä¸­å€¼çš„ä¸€éƒ¨åˆ†ã€‚ç¼–è¯‘æ—¶Shapeæ¨æ–­å¯ä»¥è¢«çœ‹ä½œæ˜¯å¯¹å‘ç”Ÿåœ¨Shapeä¸Šçš„æ“ä½œçš„å¸¸é‡æŠ˜å ï¼Œç¨‹åºæœ‰å‡ ç§Shapeè®¡ç®—çš„æ–¹æ³•ï¼š

- æ–¹æ³•1: ç¬¦å·åŒ–çš„å½¢çŠ¶ä¼ æ’­ã€‚ å¯ä»¥å°†Shapeåˆ†è§£ä¸ºç¬¦å·æ•´æ•°æ¯”å¦‚ä¸Šä¸ªè„šæœ¬ä¸­çš„nå’Œmï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç¬¦å·æ•´æ•°çš„è¡¨è¾¾å¼æ¥ä»£è¡¨Shapeçš„è®¡ç®—æ¯”å¦‚`(n*4)`ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé™æ€å½¢çŠ¶æ˜¯ç¬¦å·æ•´æ•°çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œç„¶åæˆ‘ä»¬å¯ä»¥é‡æ–°ç»„åˆç¬¦å·æ•´æ•°æ¥æ„é€ ä¸€ä¸ªæ–°çš„Shapeå¦‚`(n*4)`ã€‚
- æ–¹æ³•2: ä¸é€æ˜çš„Shapeå‡½æ•°è°ƒç”¨ã€‚æˆ‘ä»¬è¿˜å¯ä»¥å®ç°ä¸é€æ˜çš„Shapeå‡½æ•°æ¯”å¦‚`myshape_func`ï¼ˆçœ‹ä¸Šä¸Šä¸ªRelaxè„šæœ¬ï¼‰ï¼Œè¿™äº›ä¸é€æ˜çš„Shapeå‡½æ•°æ˜¯å¿«é€Ÿç ´è§£è¿è¡Œæ—¶Shapeå‡½æ•°çš„æœ‰ç”¨fallbackï¼ˆè¿™é‡Œåº”è¯¥æ˜¯è¯´åŠ ä¸Šæ‰‹å·¥å¹²é¢„çš„å½¢çŠ¶æ¨å¯¼ï¼Ÿï¼‰ã€‚
- æ–¹æ³•3ï¼šå¯¹äºæ•°æ®ç›¸å…³çš„Shapeï¼ˆå¦‚Uniqueï¼‰ï¼Œæˆ‘ä»¬å°†ç®€å•åœ°æ¨è¿Ÿåˆ°ä¸€ä¸ªè¿è¡Œæ—¶çš„è°ƒç”¨ `f(inputs)->outpus` å®ƒæ¥æ”¶ä¸€ä¸ªè¾“å…¥å¼ é‡ï¼Œåˆ†é…å¹¶è¿”å›è¾“å‡ºå¼ é‡ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡match_shapeæ„é€ ä»Tensorå€¼ä¸­è·å¾—lv5çš„å½¢çŠ¶ã€‚ï¼ˆçœ‹ä¸Šä¸Šä¸ªRelaxè„šæœ¬ï¼‰

##### Implications for pass writing
å¾ˆå¤šä¼˜åŒ–Passéƒ½éœ€è¦çŸ¥é“Shapeä¿¡æ¯ã€‚æ—¢ç„¶å¾ˆå¤šShapeå¯ä»¥æ˜¯ç¬¦å·åŒ–çš„æ¯”å¦‚ (n, 4)ï¼Œé‚£ä¹ˆç†æƒ³çš„ä¼˜åŒ–Passå°†éœ€è¦æ›´æ³›åŒ–ä¸€ç‚¹ä»¥åˆ©ç”¨ç¬¦å·ä¿¡æ¯ã€‚æ¯”å¦‚åœ¨ä¸Šè¿°è„šæœ¬ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“æ‰€æœ‰çš„`n`éƒ½å¯¹åº”åŒä¸€ä¸ªå€¼ã€‚è¿™ç§çº¦æŸå¾ˆæœ‰ç”¨ã€‚å› ä¸ºç¬¦å·åŒ–çš„æ•´æ•°ï¼ˆæˆ‘ä»¬ä¹‹å‰è®²è¿‡å¯¹åº” `tir.PrimExpr` ï¼‰åŠ¨æ€çš„æ‰§è¡Œå¸¸é‡æŠ˜å ï¼Œå½“è¾“å…¥æ˜¯é™æ€Shapeæ—¶è®¡ç®—çš„ç»“æœä¹Ÿåº”è¯¥åŠ¨æ€çš„æŠ˜å ä¸ºæ•´å½¢å¸¸æ•°ï¼Œä¿ç•™æˆ‘ä»¬æ‰§è¡Œé™æ€Shapeä¼˜åŒ–ä¾èµ–çš„å±æ€§ã€‚å› ä¸ºæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨å…ƒç»„(n, 4)è¡¨ç¤ºæ··åˆçš„é™æ€ç¬¦å·Shapeï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å°è¯•åˆ©ç”¨é™æ€ä¿¡æ¯è¿›è¡Œé¢å¤–çš„ä¼˜åŒ–ã€‚

#### D2ï¼šä¸ TensorIR å’Œ PackedFunc ç›´æ¥äº¤äº’
æˆ‘ä»¬åšå‡ºçš„æœ€åä¸€ä¸ªå…³é”®è®¾è®¡å†³ç­–æ˜¯å…è®¸é«˜å±‚ IR èƒ½å¤Ÿç›´æ¥äº¤äº’å¹¶è°ƒç”¨ä½å±‚ TensorIR å’Œ PackedFuncã€‚ TensorIR å‡½æ•°å’Œè®¸å¤šå¤–éƒ¨åº“é‡‡ç”¨ç›®æ ‡ä¼ é€’çº¦å®šï¼ˆæˆ‘ä»¬éœ€è¦æ˜¾å¼åˆ†é…è¾“å‡ºå¹¶ä½œä¸ºå‚æ•°ä¼ å…¥å‡½æ•°ï¼‰ã€‚ æˆ‘ä»¬ä½¿ç”¨ dps(destination passing) æ¥è¡¨ç¤ºè¿™ä¸ªçº¦å®šã€‚ dps åœ¨ä½çº§ ML ä¼˜åŒ–ä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘ä»¬åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ä¸€æ¬¡æ€§å…¨å±€åˆ†é…ä¸­é—´å­˜å‚¨ï¼Œå¹¶åœ¨æ²¡æœ‰ä¸»åŠ¨å†…å­˜åˆ†é…çš„æƒ…å†µä¸‹æ‰§è¡Œè®¡ç®—ã€‚

è°ƒç”¨ dps å‡½æ•°æ„å‘³ç€åœ¨è°ƒç”¨ä¹‹åï¼Œç»“æœé€šè¿‡å‡½æ•°å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œä¸‹é¢ç¤ºä¾‹ä¸­çš„ç»“æœï¼‰è€Œä¸æ˜¯å‡½æ•°çš„è¿”å›å€¼ä¼ å›ã€‚

```cpp
// not destination passing
int func(int x) {
  return 1;
}
// destination passing
void func(int x, int *result) {  
  *result = 1;
}
```

dps é£æ ¼åœ¨æœ¬è´¨ä¸Šæ„å‘³ç€çªå˜ï¼ˆè¾“å‡ºï¼‰ã€‚ æˆ‘ä»¬éœ€è¦ä¸€ç§å°†è°ƒç”¨æ¡¥æ¥åˆ°Relax Dataflowçš„æ–¹æ³•(å¯ä»¥è§‚å¯Ÿä¸€ä¸‹Relaxè¿™ä¸€èŠ‚å¼€å¤´é‚£éƒ¨åˆ†çš„è„šæœ¬)ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å¯¹ä¸€ç³»åˆ— tir è°ƒç”¨æ‰§è¡Œè®¡ç®—å›¾æ ·å¼çš„é‡å†™ã€‚


##### D2a. call_tir
`call_tir` æ˜¯å°†è°ƒç”¨æ¡¥æ¥åˆ°Relax Dataflowçš„å†…åµŒå‡½æ•°ã€‚å®ƒçš„å‘½åå«ä¹‰æ˜¯ï¼šâ€œè°ƒç”¨ä¸€ä¸ªtirè½¬æ¢â€

```python
def call_tir(output_shape: Shape, lowlevel_func: Expr, inputs: Tuple[Expr]) -> Expr:
    """Example code to demonstrate the semantics of call tir"""
    out_tensor = alloc_tensor(output_shape, current_expr.dtype)
    lowlevel_func(*inputs, out_tensor)
    return out_tensor
```

call_tir æ¥å—è¾“å‡ºå½¢çŠ¶ï¼Œlowlevel_func(can be packed func, tir PrimFunc) å’Œä¸€ä¸ªè¾“å…¥å…ƒç»„ã€‚ call_tir çš„è¯­ä¹‰å¯ä»¥é€šè¿‡ä¸Šé¢çš„ä»£ç æ¥æ¼”ç¤ºã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“æˆ‘ä»¬lower `call_tir` æ—¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦é€‰æ‹©å•ç‹¬çš„åˆ†é…è¾“å‡ºå¼ é‡ã€‚ ç¼–è¯‘å™¨å¯ä»¥é€‰æ‹©åˆ›å»ºä¸­é—´å¼ é‡çš„å†…å­˜è®¡åˆ’ï¼Œå¹¶å°†å®ƒä»¬è”ç³»åœ¨ä¸€èµ·ä»¥å®ç°æœ‰æ•ˆé‡ç”¨ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œcall_tir å†…åµŒå‡½æ•°çš„ `output_shape` å‚æ•°å¯ä»¥æ˜¯ä¸é€æ˜çš„å½¢çŠ¶å€¼ã€ç¬¦å·æ•´æ•°å…ƒç»„æˆ–å¸¸é‡å½¢çŠ¶ï¼ˆæ”¯æŒåŠ¨æ€Shapeï¼‰ã€‚

`lowlevel_func` å¯ä»¥æ˜¯ä»»ä½•å¸¦æœ‰ç­¾åçš„å‡½æ•°ï¼š`fn(input0, input1,... out0, out1...)`

æœ€å¸¸è§çš„ä¸¤ç§æƒ…å†µåŒ…æ‹¬ï¼š(1) TIR å‡½æ•° (2) ä¸é€æ˜çš„packed func

###### å®ç°ç¬”è®°
call_tir å¯ä»¥å®ç°ä¸ºç‰¹æ®Šçš„å†…åµŒå‡½æ•° (Op)ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘å¯¹ IR æ›´æ”¹çš„å½±å“ï¼ˆè€Œä¸æ˜¯ç‹¬ç«‹çš„ IR èŠ‚ç‚¹ï¼‰ã€‚ ä» AST çš„è§’åº¦æ¥çœ‹ï¼Œè¿™å˜ä¸ºï¼š

```python
Call(op=Op::Get("relax.call_tir"), shape, lowlevel_func, inputs)
```

è¿™ä¹Ÿå°†å…è®¸ call_tir çš„æœªæ¥è¿­ä»£è€Œä¸æ”¹å˜ IR æœ¬èº«ï¼Œè¿™å¯èƒ½åœ¨ç‰¹å®šæ—¶é—´ç‚¹éœ€è¦ï¼š

- åœ¨åŒä¸€ä¸ªæ•°ç»„ä¸Šå¯ç”¨å¤šä¸ªçªå˜åºåˆ—ï¼ˆåœ¨ concat ç›¸å…³æ“ä½œçš„æƒ…å†µä¸‹ï¼‰
- å¯ç”¨å°†ç¬¦å·åŒ–çš„Shapeæç¤ºä¼ é€’ç»™èåˆæ“ä½œã€‚

###### å¯¹æ•´åˆçš„å½±å“
D2 ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†è¾ƒä½çº§åˆ«çš„æŠ½è±¡ç›´æ¥åµŒå…¥åˆ°é«˜çº§æŠ½è±¡ï¼ˆR.functionï¼‰ä¸­ã€‚ è¿™é‡Šæ”¾äº†å¾ˆå¤šæœºä¼šï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

- ä½¿ç”¨ä¸åŒçš„ç­–ç•¥é€æ­¥lowerç¨‹åºçš„ä¸åŒéƒ¨åˆ†ã€‚
- æˆ‘ä»¬å¯ä»¥å°†call_tirèŠ‚ç‚¹ä½œä¸ºASTçš„ä¸€éƒ¨åˆ†è¿›è¡Œä¼˜åŒ–ï¼Œç„¶åå°†ä¸€äº›å…³é”®ä¿¡æ¯æ¯”å¦‚data layeroutä¿¡æ¯å¸¦å›åˆ°high levelçš„IRè·å¾—æ›´å¥½çš„ä¼˜åŒ–ç»“æœã€‚
- å°† BYOC æµä½œä¸ºè½¬æ¢çš„è‡ªç„¶éƒ¨åˆ†ï¼ˆé€šè¿‡å°†å›¾çš„ä¸€éƒ¨åˆ†è½¬æ¢ä¸ºä¸é€æ˜æ‰“åŒ…å‡½æ•°çš„è°ƒç”¨ï¼‰ã€‚


è¿™é‡Œçš„ç¬¬äºŒç‚¹å®é™…ä¸Šå¯¹åº”äº†Ansorå¼•å…¥çš„weight layout rewriteï¼Œå³åœ¨ç®—å­auto-tuningä¹‹åï¼Œæˆ‘ä»¬å»åˆ†ææœ€é«˜æ•ˆçš„weight layoutï¼Œå¹¶ä¸”åœ¨ç¼–è¯‘æ—¶æ”¹å†™ï¼Œæ¥æé«˜è¿è¡Œæ—¶çš„æ•ˆç‡ã€‚é‚£ä¹ˆæ²¡æœ‰Relaxä¹‹å‰æ˜¯æ€ä¹ˆå®Œæˆè¿™ä¸ªå·¥ä½œçš„å‘¢ï¼Ÿä¸€ä¸ªop æ›´é€‚åˆçš„weight layoutæ˜¯è¦åœ¨tuningä¹‹åæ‰èƒ½å¤ŸçŸ¥é“çš„ï¼Œè€Œè¿™ä¸ªæ—¶å€™å›¾IRå·²ç»è¢«lowerï¼Œä¸èƒ½ä¿®æ”¹äº†ã€‚æ‰€ä»¥Ansorç”¨äº†ä¸€ä¸ªéå¸¸trickyçš„æ–¹æ³•ï¼Œå…ˆlowerä¸€éæŠŠtuningåšå¥½ï¼Œå†å¸¦ç€è¿™äº›ä¿¡æ¯é‡æ–°lowerä¸€éã€‚æ‰€ä»¥Relaxé€šè¿‡æ¶ˆé™¤lowerçš„è¾¹ç•Œéš”é˜‚å¯ä»¥è¾ƒå¥½çš„è§£å†³è¿™ä¸€é—®é¢˜ã€‚

##### D2b. Packed function calls

æˆ‘ä»¬ä½¿ç”¨ `R.call_packed` æ¥æŒ‡ç¤ºå¯¹Packed Funcçš„è°ƒç”¨ã€‚ ä» AST çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦å¼•å…¥é¢å¤–çš„è°ƒç”¨èŠ‚ç‚¹ï¼Œè€Œæ˜¯å¯ä»¥å¼•å…¥ä¸€ä¸ª ExternFunc æ„é€ ï¼Œå®ƒè¡¨ç¤ºæˆ‘ä»¬å¯ä»¥è°ƒç”¨çš„æ‰“åŒ…å‡½æ•°ã€‚

```cpp
Call(op=ExternFunc("my_packed_func"), *args)
```

`R.call_packed` ä»…ç”¨ä½œè¡¨ç¤ºä¸Šè¿° AST èŠ‚ç‚¹çš„è¯­æ³•ç³–ã€‚ è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç»Ÿä¸€æ‰€æœ‰è°ƒç”¨ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒè¿˜å…è®¸æˆ‘ä»¬åœ¨å¿…è¦æ—¶æ··åˆæ‰“åŒ…å‡½æ•°å’Œ call_tirã€‚

```cpp
lv4: R.Tensor[lv3, "f32"] = R.call_tir(lv3, "custom_func", [lv1]) 
```

å¯¹åº”äºä¸‹é¢çš„ ASTã€‚

```cpp
Call(op=Op::Get("relax.call_tir"), shape, ExternFunc("my_packed_func"), [lv1])
```

å½“æˆ‘ä»¬æƒ³è¦å°†ä½çº§åº“ï¼ˆä¾‹å¦‚ cudnnï¼‰ç›´æ¥é›†æˆåˆ°é«˜çº§è€Œä¸è°ƒç”¨å†…å­˜åˆ†é…æ—¶ï¼Œå¤–éƒ¨æ‰“åŒ…å‡½æ•°ä¸Šçš„ CallTIR ä¼šå¾ˆæœ‰ç”¨ã€‚

å…³äºè¿™ä¸€ç‚¹åœ¨MLCè¯¾ç¨‹ä¸­ä¹Ÿæœ‰æ¼”ç¤ºï¼Œé€šè¿‡dlpackè°ƒç”¨PyTorchçš„Opæ¥åšä¼˜åŒ–ï¼Œæ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥çœ‹ä¸€ä¸‹ï¼Œé“¾æ¥ï¼šhttps://mlc.ai/zh/chapter_end_to_end/index.htmlã€‚


è¿™é‡Œç®€å•åšä¸€ä¸ªæ€»ç»“ï¼ŒRelaxä½œä¸ºä¸‹ä¸€ä»£Relayä¸ä»…åŸç”Ÿæ”¯æŒåŠ¨æ€Shapeä¸”ä½¿ç”¨ä½“éªŒæ›´åŠ é è¿‘PyTorchè¿™ç§æ•°æ®æµå›¾çš„ç¼–ç¨‹æ–¹å¼ã€‚å°¤å…¶é‡è¦çš„æ˜¯Relaxåœ¨ä¸ºTVM Unifyè€ŒæœåŠ¡ï¼Œé€šè¿‡å’ŒTensorIRæŠ½è±¡ï¼ŒTVMFFIï¼ˆPacked Funcï¼‰çš„äº¤äº’ï¼ˆé€šè¿‡MLCæ•™ç¨‹å¯ä»¥çŸ¥é“ï¼Œä¹Ÿå¯ä»¥å’ŒAuto Scheduleäº¤äº’ï¼‰ä½¿å¾—TVM Unifyçš„ç›®æ ‡å¾—åˆ°å®ç°ã€‚

å½“ç„¶æˆ‘ä¹Ÿè¦è¯´ä¸€ä¸‹æˆ‘ç›®å‰çœ‹åˆ°çš„Relaxçš„ä¸å®Œå–„çš„åœ°æ–¹ï¼Œé‚£å°±æ˜¯Relaxç›®å‰å’Œå…¶å®ƒæ·±åº¦å­¦ä¹ æ¡†æ¶å¯¹æ¥è¿˜ä¸å¤Ÿå®Œå–„ï¼Œå¦‚æœèƒ½å®ç°Relayåˆ°Relaxçš„è‡ªåŠ¨è½¬æ¢é‚£å°†æ˜¯ä¸€ä¸ªæŒ¯å¥‹äººå¿ƒçš„æ¶ˆæ¯ï¼Œå¯ä»¥æœ€å°åŒ–æˆ‘ä»¬çš„è¿ç§»æˆæœ¬ã€‚


# 0x3. Tensor Expression(TE)

è®©æˆ‘ä»¬å›åˆ°å¼€å¤´çš„è¿™ä¸ªå›¾ï¼š

![TVMå‰ç«¯æ¶æ„å›¾](https://img-blog.csdnimg.cn/6be2049a969a449bb79911739fd42169.png)

æˆ‘ä»¬å¯ä»¥å‘ç°Relayè¦åˆ°TIRæœ‰2æ¡è·¯å¾„ï¼Œç¬¬ä¸€æ¡å°±æ˜¯ç›´æ¥åˆ°TIRæ¯”å¦‚PrimExpræ´¾ç”Ÿçš„èŠ‚ç‚¹æ¯”å¦‚ä¸€ä¸ªIntImmNodeå¯ä»¥ç›´æ¥æ˜ å°„åˆ°TIRï¼Œå¦å¤–ä¸€æ¡å°±æ˜¯Relayé‡Œé¢ç±»ä¼¼Convçš„Opçš„è®¡ç®—é€»è¾‘æ˜¯ç”¨TOPIæ¥è¡¨è¾¾çš„ï¼ŒTOPIæ˜¯TVMè‡ªå·±çš„ä¸€ä¸ªç®—å­åº“ï¼Œè¿™äº›ç®—å­å¯ä»¥é€šè¿‡TEæ¥è¿›è¡Œè¡¨è¾¾ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬åœ¨å‰ç«¯ä»‹ç»Relaxçš„æ—¶å€™å·²ç»å¯ä»¥çœ‹åˆ°è¦ç›´æ¥ç¼–å†™TIR ASTï¼Œä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨TVMScriptæ¥è¡¨ç¤ºæŠ½è±¡çš„è®¡ç®—é€»è¾‘ï¼Œå¦å¤–ä¸€ç§æ–¹æ³•å°±æ˜¯è¦é€šè¿‡TEï¼ŒTEçš„ä»£ç æ— æ³•è¢«ç›´æ¥ç¼–è¯‘æˆç›®æ ‡ç¡¬ä»¶çš„ä»£ç ï¼Œè€Œæ˜¯éœ€è¦å…ˆLowerä¸ºTIRçš„å…ƒå¼ é‡å‡½æ•°æ‰å¯ä»¥è¿›è¡Œç¼–è¯‘ã€‚å…¶å®æˆ‘ä¹‹å‰å†™è¿‡ä¸€äº›Scheduleç›¸å…³çš„æ–‡ç« æ¯”å¦‚ã€Šã€TVM ä¸‰ä»£ä¼˜åŒ–å·¡ç¤¼ã€‘åœ¨X86ä¸Šå°†æ™®é€šçš„çŸ©é˜µä¹˜æ³•ç®—å­æé€Ÿ90å€ã€‹ï¼Œä¹Ÿéƒ½æ˜¯åŸºäºTEçš„ã€‚ç”±æ­¤å¯è§ï¼ŒTEä¸ä»…æä¾›äº†å¦å¤–ä¸€ç§ç¼–å†™TIR ASTçš„æ–¹æ³•ï¼Œè¿˜æä¾›äº†ä¸€ç³»åˆ—å˜æ¢TIR ASTçš„Scheduleã€‚åœ¨0x5èŠ‚æˆ‘ä»¬ä¼šæä¸€ä¸‹Scheduleã€‚

æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹ç»™äºˆTVM Scriptå†™çš„è¿™ä¸ªå‘é‡åŠ æ³•çš„ä¾‹å­ï¼š

```python
@tvm.script.ir_module
class MyModule:
    @T.prim_func
    def main(a: T.handle, b: T.handle):
        # We exchange data between function by handles, which are similar to pointer.
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # Create buffer from handles.
        A = T.match_buffer(a, (8,), dtype="float32")
        B = T.match_buffer(b, (8,), dtype="float32")
        for i in range(8):
            # A block is an abstraction for computation.
            with T.block("B"):
                # Define a spatial block iterator and bind it to value i.
                vi = T.axis.spatial(8, i)
                B[vi] = A[vi] + 1.0


ir_module = MyModule
print(type(ir_module))
print(ir_module.script())
```

è¾“å‡ºï¼š

```python
<class 'tvm.ir.module.IRModule'>
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[8, "float32"], B: T.Buffer[8, "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i in T.serial(8):
            with T.block("B"):
                vi = T.axis.spatial(8, i)
                T.reads(A[vi])
                T.writes(B[vi])
                B[vi] = A[vi] + T.float32(1)
```

ç„¶åæˆ‘ä»¬ä½¿ç”¨TE DSLæ¥è¡¨è¾¾è¿™ä¸ªå‘é‡åŠ æ³•ï¼š

```python
from tvm import te

A = te.placeholder((8,), dtype="float32", name="A")
B = te.compute((8,), lambda *i: A(*i) + 1.0, name="B")
func = te.create_prim_func([A, B])
ir_module_from_te = IRModule({"main": func})
print(ir_module_from_te.script())
```

è¾“å‡ºï¼š

```python
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(A: T.Buffer[8, "float32"], B: T.Buffer[8, "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0 in T.serial(8):
            with T.block("B"):
                i0_1 = T.axis.spatial(8, i0)
                T.reads(A[i0_1])
                T.writes(B[i0_1])
                B[i0_1] = A[i0_1] + T.float32(1)
```

ä»ä¸¤ä¸ªè¾“å‡ºä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæœ€ååˆ›å»ºçš„IRModuleå…¶å®æ˜¯å®Œå…¨ä¸€æ ·çš„ã€‚ç„¶åè¿™ä¸ªIRModuleå¯ä»¥è¢«ç¼–è¯‘ä¸ºç›®æ ‡ç¡¬ä»¶ä¸Šå¯ä»¥æ‰§è¡Œçš„ä»£ç ã€‚å¦‚æœä½ æƒ³æ›´åŠ æ·±å…¥çš„äº†è§£TEæ˜¯å¦‚ä½•è¢«ç¼–è¯‘æˆTIRçš„ï¼Œå¯ä»¥çœ‹ä¸€ä¸‹ ã€ŠTVM è‡ªåº•å‘ä¸Šï¼ˆä¸‰ï¼‰ï¼šTE çš„æ¦‚å¿µå’Œç¼–è¯‘åŸç†ã€‹ è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘è¿™é‡Œå€Ÿä¸€ä¸‹ä½œè€…æ–‡ç« ä¸­çš„æ ¸å¿ƒå›¾ç®€è¦è¯´æ˜ä¸€ä¸‹ï¼š

![æ¥è‡ª ï¼šhttps://zhuanlan.zhihu.com/p/534313816 ä½œè€…ï¼šKord ä¾µåˆ ](https://img-blog.csdnimg.cn/bf7d73b64c754d81bb6c04b3f74bca84.png)


æˆ‘ä»¬ä»ä¸Šå¾€ä¸‹çœ‹ï¼Œè¿™é‡Œçš„List[PrimExpr]å°±æ˜¯è¿™ä¸ªlambdaè¡¨è¾¾å¼ä¸­çš„PrimExpré›†åˆï¼Œç¬¬ä¸€ä¸ªPrimExpræ˜¯A(*i)ï¼Œç¬¬äºŒä¸ªPrimExpræ˜¯1.0ï¼Œç„¶å+å¯¹åº”äº†TIRä¸­çš„ExprOpï¼ˆ`https://github.com/apache/tvm/blob/main/python/tvm/tir/expr.py#L66`ï¼‰ï¼ŒExprä½œç”¨åœ¨1ä¸ªæˆ–è€…å¤šä¸ªPrimExprä¸Šå¾—åˆ°çš„ç»“æœä»ç„¶æ˜¯PrimExprã€‚å®é™…ä¸Šï¼Œè¿™é‡Œçš„List[PrimExpr]å°±å¯¹åº”äº†è¿™ä¸ªlambdaè¡¨è¾¾å¼çš„ASTè¡¨ç¤ºã€‚æ¥ä¸‹æ¥æˆ‘ä»¬çœ‹ä¸€ä¸‹te.computeçš„ä»£ç ï¼ˆ`https://github.com/apache/tvm/blob/main/python/tvm/tir/expr.py#L66`ï¼‰ï¼š

```python
def compute(shape, fcompute, name="compute", tag="", attrs=None, varargs_names=None):
    """Construct a new tensor by computing over the shape domain.
    The compute rule is result[axis] = fcompute(axis)
    Parameters
    ----------
    shape: Tuple of Expr
        The shape of the tensor
    fcompute: lambda function of indices-> value
        Specifies the input source expression
    name: str, optional
        The name hint of the tensor
    tag: str, optional
        Additional tag information about the compute.
    attrs: dict, optional
        The additional auxiliary attributes about the compute.
    varargs_names: list, optional
        The names to use for each of the varargs. If not supplied, the varargs
        will be called i1, i2, ...
    Returns
    -------
    tensor: Tensor
        The created tensor
    """
    if _tag.TagScope.get_current() is not None:
        if tag != "":
            raise ValueError("nested tag is not allowed for now")
        tag = _tag.TagScope.get_current().tag
    shape = (shape,) if isinstance(shape, tvm.tir.PrimExpr) else shape
    # for python3
    shape = tuple([int(s) if isinstance(s, float) else s for s in shape])
    out_ndim = len(shape)
	  # è·å–è¾“å…¥ç»™lambdaè¡¨è¾¾å¼çš„å‚æ•°åˆ—è¡¨	
    argspec = inspect.getfullargspec(fcompute)
    if len(argspec.args) == 0 and argspec.varargs is None:
        arg_names = ["i%d" % i for i in range(out_ndim)]
    elif argspec.varargs is not None:
        # if there is a varargs, it takes the remaining dimensions of out_ndim
        num_remaining_args = out_ndim - len(argspec.args)
        if varargs_names is not None:
            if len(varargs_names) != num_remaining_args:
                raise RuntimeError(
                    f"Number of varargs ({num_remaining_args}) does not match number"
                    f"of varargs_names ({len(varargs_names)})"
                )
            arg_names = argspec.args + varargs_names
        else:
            arg_names = argspec.args + [f"i{i}" for i in range(out_ndim - len(argspec.args))]
    else:
        arg_names = argspec.args
        # if there are fewer args than out dimensions, the remaining dimensions
        # are implicitly broadcast
        out_ndim = len(arg_names)
    assert argspec.varkw is None, "Variable keyword arguments not supported in fcompute"
    assert argspec.defaults is None, "Default arguments not supported in fcompute"
    assert len(argspec.kwonlyargs) == 0, "Keyword arguments are not supported in fcompute"

    if out_ndim != len(arg_names):
        raise ValueError(
            "Number of args to fcompute does not match dimension, "
            "args=%d, dimension=%d" % (len(arg_names), out_ndim)
        )
		
    dim_var = [tvm.tir.IterVar((0, s), x, 0) for x, s in zip(arg_names, shape[:out_ndim])]
    # åŸºäºlambdaè¡¨è¾¾å¼åˆ›å»ºList[PrimExpr]
    body = fcompute(*[v.var for v in dim_var])
	  
	  # å°†List[PrimExpr]ä¼ ç»™TensorComputeOpè¿›è¡Œè®¡ç®—å¹¶è¿”å›ä¸€ä¸ªtvm.te.Tensor
    if isinstance(body, _tensor.TensorIntrinCall):
        for i, s in enumerate(shape[out_ndim:]):
            var_name = "ax" + str(i)
            dim_var.append(tvm.tir.IterVar((0, s), var_name, 4))
        op_node = _ffi_api.TensorComputeOp(
            name,
            tag,
            dim_var,
            body.reduce_axis,
            out_ndim,
            body.intrin,
            body.tensors,
            body.regions,
            body.scalar_inputs,
        )
    else:
        if not isinstance(body, (list, tuple)):
            body = [body]
        body = convert(body)
        op_node = _ffi_api.ComputeOp(name, tag, attrs, dim_var, body)

    num = op_node.num_outputs
    outputs = tuple(op_node.output(i) for i in range(num))
    return outputs[0] if num == 1 else outputs
```

åœ¨computeçš„å®ç°ä¸­æœ€åè¿”å›çš„æ˜¯TensorComputeOpå¯¹è±¡çš„output()æˆå‘˜ï¼ˆä¹Ÿæ˜¯ä¸€ä¸ªtvm.te.Tensorï¼‰ï¼Œ åŒæ—¶è¿™ä¸ªtvm.te.TensoråŒ…å«è¿™ä¸ªTensorComputeOpå¯¹è±¡ï¼ˆé€šè¿‡`.op`æ¥è®¿é—®ï¼Œåœ¨`https://github.com/apache/tvm/blob/main/python/tvm/te/tensor.py#L108`å¯ä»¥çœ‹åˆ°ï¼‰ã€‚

æœ€å`func = te.create_prim_func([A, B])`è¿™è¡Œä»£ç å®Œæˆäº†TEåˆ°TIRçš„è½¬æ¢ã€‚è¿™ä¸ªapiå¯¹åº”çš„c++å®ç°åœ¨`https://github.com/apache/tvm/blob/v0.8.0/src/te/operation/create_primfunc.cc#L238`è¿™ä¸ªæ–‡ä»¶ï¼Œæ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥è‡ªè¡ŒæŸ¥çœ‹ã€‚åŸºæœ¬æµç¨‹å°±æ˜¯å°†æ‰€æœ‰Operationå¯¹åº”çš„PrimExpr ASTè¿åœ¨ä¸€èµ·æ„æˆä¸€ä¸ªAST Graphï¼Œç„¶åä½¿ç”¨Post-DFSç®—æ³•éå†è¿™ä¸ªAST Graphåˆ†åˆ«å¤„ç†æ¯ä¸€ä¸ªOperationåˆ›å»ºå¯¹åº”çš„TIRèŠ‚ç‚¹ï¼Œæœ€åæ„é€ ä¸€ä¸ªå®Œæ•´çš„TIR PrimFuncã€‚

TEé™¤äº†å¯ä»¥æ„é€ TIRä¹‹å¤–ï¼Œå¦å¤–ä¸€ä¸ªé‡è¦çš„ç‚¹å°±æ˜¯å®ƒæ”¯æŒScheduleï¼ˆ`tvm.te.Schedule`ï¼‰ï¼Œæˆ‘åœ¨[ã€TVM ä¸‰ä»£ä¼˜åŒ–å·¡ç¤¼ã€‘åœ¨X86ä¸Šå°†æ™®é€šçš„çŸ©é˜µä¹˜æ³•ç®—å­æé€Ÿ90å€](https://mp.weixin.qq.com/s/d8v9Q3EAkv8TknP5Hh7N7A) æ–‡ç« ä¸­å¯¹GEMMä¼˜åŒ–çš„ä»‹ç»å°±æ˜¯åŸºäºTE Scheduleæ¥åšå˜æ¢è¿›è¡Œä¼˜åŒ–è®¡ç®—çš„ã€‚

# 0x4. å›¾ä¼˜åŒ–ï¼ˆPassæœºåˆ¶ï¼‰
ç°åœ¨æˆ‘ä»¬æŠŠç›®å…‰è½¬å‘å›¾ä¼˜åŒ–çš„Passã€‚ä¹‹å‰æˆ‘åœ¨[ã€ä»é›¶å¼€å§‹å­¦æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ã€‘ä¸ƒï¼Œä¸‡å­—é•¿æ–‡å…¥é—¨TVM Pass](https://mp.weixin.qq.com/s/IMm1nurpoESFRLxHcEYxcQ) è¿™ç¯‡æ–‡ç« ä¸­ç»“åˆTVMçš„è®¾è®¡æ–‡æ¡£ä»‹ç»äº†TVM Passæœºåˆ¶ä»¥åŠTVMç¼–å†™Passæ—¶æ˜¯å¦‚ä½•éå†èŠ‚ç‚¹å’Œæ”¹å†™èŠ‚ç‚¹çš„ï¼Œè¿™é‡Œæˆ‘ä»¬å†æ•´åˆä¸€ä¸‹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹TVM Passçš„åŸºç±»å®šä¹‰ï¼ˆ`https://github.com/apache/tvm/blob/main/include/tvm/ir/transform.h#L329`ï¼‰ï¼š

```cpp
/*!
 * \brief PassNode is the base type of differnt types of optimization passes.
 * It is designed as a pure class and implemented by different pass subclasses
 * at different granularity of Relay nodes.
 */
class PassNode : public Object {
 public:
  virtual ~PassNode() {}
  /*!
   * \brief Get the pass information/meta data. */
  virtual PassInfo Info() const = 0;

  /*!
   * \brief Transform mod using the default PassContext in the current scope.
   *
   * \param mod The module that an optimization pass runs on.
   *
   * \return The transformed module.
   */
  IRModule operator()(IRModule mod) const {
    return this->operator()(std::move(mod), PassContext::Current());
  }
	...
};
```

ä»operator()çš„å®šä¹‰å¯çŸ¥ï¼ŒPassåšçš„ä¸»è¦æ˜¯IRModuleåˆ°IRModuleçš„å˜æ¢ï¼Œå¦å¤–è¿™é‡Œçš„PassInfoå’ŒPassContextåˆ†åˆ«è¡¨ç¤ºæ¯ä¸ªPassçš„å…³é”®ä¿¡æ¯å’Œå¤šä¸ªPassæ‰§è¡Œè¿‡ç¨‹ä¸­çš„å…±åŒä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬åˆ†åˆ«çœ‹ä¸€ä¸‹å®ƒä»¬çš„å®šä¹‰(`https://github.com/apache/tvm/blob/main/include/tvm/ir/transform.h`)ï¼š

```cpp
/*!
 * \brief Meta data that will be used to help optimization and analysis.
 * \sa PassInfo
 */
class PassInfoNode : public Object {
 public:
  /*! \brief The minimal optimization level that this pass will be enabled. */
  int opt_level;

  /*! \brief The name of an optimization/analysis pass. */
  String name;

  /*! \brief The passes that are required to perform the current pass. */
  Array<String> required;
	...
}

class PassContextNode : public Object {
 public:
  /*! \brief The default optimization level. */
  int opt_level{2};

  /*! \brief The list of required passes. */
  Array<String> required_pass;
  /*! \brief The list of disabled passes. */
  Array<String> disabled_pass;
  /*! \brief The diagnostic context. */
  mutable Optional<DiagnosticContext> diag_ctx;
  /*! \brief Pass specific configurations. */
  Map<String, ObjectRef> config;

  /*! \brief A list of pass instrument implementations. */
  Array<instrument::PassInstrument> instruments;
	...
}
```

è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯åœ¨PassContextNodeå®šä¹‰ä¸­å‡ºç°äº†ä¸€ä¸ª`instrument::PassInstrument`ç±»ï¼Œè¿™ä¸ªç±»æ˜¯ä¸ºå¼€å‘è€…è®¾è®¡çš„ä¸€ä¸ªå·¥å…·ï¼Œå¼€å‘è€…å¯ä»¥å®ç°ä¸€äº›å‡½æ•°è¿è¡Œåœ¨æ¯ä¸ªPassæ‰§è¡Œå‰æˆ–è€…æ‰§è¡Œåï¼ˆ`https://github.com/apache/tvm/blob/main/src/ir/transform.cc#L261`ï¼‰ï¼š

```cpp
IRModule Pass::operator()(IRModule mod, const PassContext& pass_ctx) const {
  const PassNode* node = operator->();
  ICHECK(node != nullptr);
  const PassInfo& pass_info = node->Info();
  if (!pass_ctx.InstrumentBeforePass(mod, pass_info)) {
    DLOG(INFO) << "Skipping pass : " << pass_info->name
               << " with opt level: " << pass_info->opt_level;
    return mod;
  }
  auto ret = node->operator()(std::move(mod), pass_ctx);
  pass_ctx.InstrumentAfterPass(ret, pass_info);
  return std::move(ret);
}
```

æˆ‘ä»¬å¯ä»¥åœ¨`https://github.com/apache/tvm/blob/main/tests/python/relay/test_pass_instrument.py`è¿™ä¸ªæµ‹è¯•æ–‡ä»¶ä¸­æ‰¾åˆ°PassInstrumentæœºåˆ¶çš„ç¤ºä¾‹ç”¨æ³•ï¼Œ è¿™ä¸ªåŠŸèƒ½å¯ä»¥æ–¹ä¾¿çš„è®©æˆ‘ä»¬è§‚å¯Ÿæ¯ä¸€ä¸ªIRModuleç»è¿‡ä¸€ä¸ªPassä¹‹åå˜æˆæ–°çš„IRModuleä¹‹åæœ‰ä»€ä¹ˆå˜åŒ–ï¼Œæ–¹ä¾¿debugæˆ–è€…å¯è§†åŒ–ã€‚

ç„¶åTVMä¸ºäº†æ–¹ä¾¿å®ç°äº†3ä¸ªçº§åˆ«çš„Passï¼Œå³Module-Levelçš„Passç›´æ¥æ“ä½œIRModuleï¼Œä»¥åŠFunction-Levelçš„Passéå†Module ä¸­çš„Functionè¿›è¡Œå¤„ç†ï¼Œè¿˜æœ‰Sequential PassåŒ…å«ä¸€å †é¡ºåºæ‰§è¡Œçš„Passï¼ˆå¯¹æ¯”PyTorchçš„nn.Sequentialï¼‰ã€‚æ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥è‡ªè¡Œé˜…è¯»æºç æˆ–è€…[ã€ä»é›¶å¼€å§‹å­¦æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ã€‘ä¸ƒï¼Œä¸‡å­—é•¿æ–‡å…¥é—¨TVM Pass](https://mp.weixin.qq.com/s/IMm1nurpoESFRLxHcEYxcQ)ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬è®²ä¸€è®²å›¾ä¼˜åŒ–Passéå†ä»¥åŠé‡å†™ASTèŠ‚ç‚¹çš„åŸç†ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬è¿™é‡Œè®²çš„Passæ˜¯TVMå†…ç½®çš„ä½œç”¨äºTIR ASTä¸Šçš„Passï¼Œæˆ‘ä»¬çŸ¥é“TIR ASTæ˜¯ç”±ä¸€ç³»åˆ—PrimExprå’ŒRelayExprï¼ˆéPrimExprï¼‰æ¥è¡¨ç¤ºçš„ï¼Œå®ƒä»¬éƒ½ç»§æ‰¿äº†TVMçš„ExpråŸºç¡€ç±»ã€‚æ‰€ä»¥TVMé’ˆå¯¹TIR ASTçš„éå†ä¸“é—¨åšäº†ä¸€ä¸ªå·¥å…·ç±»ExprFunctoræ¥åšï¼Œå®ƒå®šä¹‰åœ¨`https://github.com/apache/tvm/blob/main/include/tvm/relay/expr_functor.h#L67` ï¼š

```cpp
template <typename R, typename... Args>
class ExprFunctor<R(const Expr& n, Args...)> {
 private:
  using TSelf = ExprFunctor<R(const Expr& n, Args...)>;
  using FType = tvm::NodeFunctor<R(const ObjectRef& n, TSelf* self, Args...)>;

 public:
  /*! \brief the result type of this functor */
  using result_type = R;
  /*! \brief virtual destructor */
  virtual ~ExprFunctor() {}
  /*!
   * \brief Same as call.
   * \param n The expression node.
   * \param args Additional arguments.
   * \return The result of the call
   */
  R operator()(const Expr& n, Args... args) { return VisitExpr(n, std::forward<Args>(args)...); }
  /*!
   * \brief The functor call.
   * \param n The expression node.
   * \param args Additional arguments.
   * \return The result of the call
   */
  virtual R VisitExpr(const Expr& n, Args... args) {
    ICHECK(n.defined()) << "Found null pointer node while traversing AST. The previous pass may "
                           "have generated invalid data.";
    static FType vtable = InitVTable();
    return vtable(n, this, std::forward<Args>(args)...);
  }
  // Functions that can be overriden by subclass
  virtual R VisitExpr_(const ConstantNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const TupleNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const VarNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const GlobalVarNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const FunctionNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const CallNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const LetNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const IfNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const OpNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const TupleGetItemNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const RefCreateNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const RefReadNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const RefWriteNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const ConstructorNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExpr_(const MatchNode* op, Args... args) EXPR_FUNCTOR_DEFAULT;
  virtual R VisitExprDefault_(const Object* op, Args...) {
    LOG(FATAL) << "Do not have a default for " << op->GetTypeKey();
    throw;
  }
  ...
};
```

ä»ç±»çš„å®šä¹‰å¯ä»¥çœ‹åˆ°ExprFunctorä¸»è¦æä¾›äº†VisitExprå‡½æ•°æ¥å£ï¼Œå¹¶æ ¹æ®Exprçš„å…·ä½“ç±»å‹è½¬å‘åˆ°å¯¹åº”çš„ VisitExpr_ ã€‚VisitExpr_ åˆ™ç”±æ´¾ç”Ÿç±»è´Ÿè´£å®ç°ï¼Œå½“ç„¶ä»ä»£ç ä¹Ÿå¯ä»¥çœ‹å‡ºï¼ŒVisitExpr æœ¬èº«ä¹Ÿå¯ä»¥è¢«é‡è½½ã€‚æœ‰äº†è¿™ä¸ªè½¬å‘æœºåˆ¶ä¹‹åï¼Œå°±å¯ä»¥å¾ˆå®¹æ˜“çš„å®ç°ä¸€ä¸ªéå†æ‰€æœ‰ç±»å‹Exprçš„ç±»äº†ï¼Œåœ¨TVMä¸­å«ä½œExprVisitorï¼ˆ`https://github.com/apache/tvm/blob/main/include/tvm/relay/expr_functor.h#L149`ï¼‰ï¼š

```cpp
/*!
 * \brief A simple visitor wrapper around ExprFunctor.
 *  Recursively visit the content.
 *
 * ExprVisitor treats Expr as dataflow graph,
 * and only visit each Expr node once.
 */
class ExprVisitor : public ::tvm::relay::ExprFunctor<void(const Expr& n)> {
 public:
  void VisitExpr(const Expr& expr) override;
  void VisitExpr_(const VarNode* op) override;
  ...

 protected:
  // Internal visiting counter
  std::unordered_map<const Object*, size_t> visit_counter_;
};
```

æ¯”å¦‚å¯¹äº`https://github.com/apache/tvm/blob/main/src/relay/transforms/fold_constant.cc#L68`ä¸­çš„`ConstantFolder`è¿™ä¸ªç±»ï¼Œå°±ç»§æ‰¿äº†`ExprVisitor`ï¼Œå¹¶é€šè¿‡`VisitExpr(expr)`ï¼Œè®¿é—®æ•°æ®ã€‚`ExprVisitor`çš„`VisitExpr`æˆå‘˜å‡½æ•°å®ç°å¦‚ä¸‹(`https://github.com/apache/tvm/blob/main/src/relay/ir/expr_functor.cc#L289`)ï¼š

```cpp
void ExprVisitor::VisitExpr(const Expr& expr) {
  auto it = visit_counter_.find(expr.get());
  if (it != visit_counter_.end()) {
    ++it->second;
  } else {
    using TParent = ExprFunctor<void(const Expr&)>;
    TParent::VisitExpr(expr);
    visit_counter_.insert({expr.get(), 1});
  }
}
```

å¯ä»¥çœ‹åˆ°è¿™ä¸ªç±»å®é™…ä¸Šè°ƒç”¨çš„æ˜¯çˆ¶ç±»(`ExprFunctor`)çš„`VisitExpr`ï¼Œè€Œ`ExprFunctor`çš„`VisitExpr`çš„å®ç°å¦‚ä¸‹ï¼š

```cpp
virtual R VisitExpr(const Expr& n, Args... args) {
    ICHECK(n.defined()) << "Found null pointer node while traversing AST. The previous pass may "
                           "have generated invalid data.";
    static FType vtable = InitVTable();
    return vtable(n, this, std::forward<Args>(args)...);
  }
```

å¯ä»¥çœ‹åˆ°`ExprFunctor`è®¾ç½®äº†`VisitExpr`è™šå‡½æ•°ï¼Œåœ¨è§£ææ—¶ä¼šå›åˆ°`ExprVisitor`æ¥è§£æèŠ‚ç‚¹ï¼Œè€Œ`ConstantFolder`è¿™ä¸ªç±»ç»§æ‰¿äº†`ExprVisitor`ï¼Œè¿™æ ·æˆ‘ä»¬åªéœ€è¦åœ¨`ConstantFolder`ç±»ä¸­é‡å†™å„ä¸ªExprèŠ‚ç‚¹ç±»å‹çš„`VisitExpr_`å‡½æ•°å°±å¯ä»¥äº†ã€‚

åœ¨`ExprFunctor`çš„`VisitExpr`å®ç°ä¸­æœ‰ä¸€ä¸ª`RELAY_EXPR_FUNCTOR_DISPATCH`å®ï¼Œè¿™ä¸ªå®çš„å®šä¹‰å¦‚ä¸‹ï¼š

```cpp
#define RELAY_EXPR_FUNCTOR_DISPATCH(OP)                                                    \
  vtable.template set_dispatch<OP>([](const ObjectRef& n, TSelf* self, Args... args) {     \
    return self->VisitExpr_(static_cast<const OP*>(n.get()), std::forward<Args>(args)...); \
  });

```

è¿™é‡Œçš„`self`å³ä¸º`ExprFunctor`çš„`VisitExpr`çš„å®ç°ä¸­çš„`vtable(n, this, std::forward<Args>(args)...)`ï¼Œè€Œ`this`æŒ‡å‘`ExprFunctor`ã€‚åˆå› ä¸º`ExprVisitor::VisitExpr`æ–¹æ³•è°ƒç”¨çš„æ˜¯`ExprFunctor`çš„å‡½æ•°ï¼Œæ‰€ä»¥è¿™é‡Œçš„`this`æŒ‡å‘çš„æ˜¯`ExprVisitor`å®ä¾‹ã€‚

ä»¥`IfNode`ä¸ºä¾‹å­ï¼Œçœ‹çœ‹`ExprVisitor`çš„`VisitExpr_`å®ç°ã€‚ç”±äº`this`æŒ‡å‘çš„æ˜¯`ExprVisitor`å®ä¾‹ï¼Œæœ€åä¼šåœ¨`ExprVisitor`å®ä¾‹ä¸­ç”Ÿæˆ`visit_counter_`çš„åˆ—è¡¨ã€‚

```cpp
void ExprVisitor::VisitExpr_(const IfNode* op) {
  this->VisitSpan(op->span);
  this->VisitExpr(op->cond);
  this->VisitExpr(op->true_branch);
  this->VisitExpr(op->false_branch);
}
```

`visit_counter_`æ˜¯åœ¨`ExprVisitor`ä¸­å®šä¹‰çš„ä¸€ä¸ª`unordered_map`ï¼Œæ¥æ ‡è®°åœ¨éå†ASTæ—¶æŸç§Expræ˜¯å¦å‡ºç°ï¼ŒåŒæ—¶è®°å½•ä¸‹å‡ºç°çš„æ¬¡æ•°æ¥ä¿è¯æ¯ä¸ªExpréƒ½åªä¼šè¢«è®¿é—®ä¸€æ¬¡ã€‚

```cpp
// Internal visiting counter
  std::unordered_map<const Object*, size_t> visit_counter_;
```

æ˜¾ç„¶ï¼Œå¦‚æœASTå¾ˆå¤æ‚ï¼Œè¿™æ ·é€’å½’å¯èƒ½ä¼šå¯¼è‡´Stack Overflow. ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTVM æä¾›äº† MixedModeVisitor æ¥å®ç°å’Œ ExprVisitor ä¸€æ ·çš„åŠŸèƒ½ï¼Œä½†æ˜¯é¿å…äº† Stack Overflowã€‚

æˆ‘ä»¬ä¸Šé¢æåˆ°å¯¹äºASTé™¤äº†éå†ï¼Œè¿˜æœ‰æ”¹å†™çš„éœ€æ±‚ï¼Œæ‰€ä»¥TVMæä¾›äº†ä¸€ä¸ªExprMutator ï¼ŒåŒæ ·ç»§æ‰¿äº† ExprFunctorã€‚ç±»çš„å®šä¹‰å¦‚ä¸‹ï¼š

```cpp
class ExprMutator : public ::tvm::relay::ExprFunctor<Expr(const Expr&)> {
 public:
  /*!
   * \brief Mutate is alias for VisitExpr
   * \return expr.
   */
  Expr Mutate(const Expr& expr) { return this->VisitExpr(expr); }
  Expr VisitExpr(const Expr& expr) override;
  Expr VisitExpr_(const VarNode* op) override;
  Expr VisitExpr_(const ConstantNode* op) override;
  Expr VisitExpr_(const GlobalVarNode* op) override;
  Expr VisitExpr_(const OpNode* op) override;
  Expr VisitExpr_(const TupleNode* op) override;
  Expr VisitExpr_(const FunctionNode* op) override;
  Expr VisitExpr_(const CallNode* call_node) override;
  Expr VisitExpr_(const LetNode* op) override;
  Expr VisitExpr_(const IfNode* op) override;
  Expr VisitExpr_(const TupleGetItemNode* op) override;
  Expr VisitExpr_(const RefCreateæ¥è¡¨è®°Node* op) override;
  Expr VisitExpr_(const RefReadNode* op) override;
  Expr VisitExpr_(const RefWriteNode* op) override;
  Expr VisitExpr_(const ConstructorNode* op) override;
  Expr VisitExpr_(const MatchNode* op) override;

  /*!
   * \brief Used to visit the types inside of expressions.
   *
   * Can be overloaded to transform the types in arbitrary
   * ways, one way would be to define a sub-class of type
   * visitor for types which transform them appropriately.
   */
  virtual Type VisitType(const Type& t);
  virtual Clause VisitClause(const Clause& c);
  virtual Pattern VisitPattern(const Pattern& c);

 protected:
  /*! \brief Internal map used for memoization. */
  std::unordered_map<Expr, Expr, ObjectPtrHash, ObjectPtrEqual> memo_;
};
```

æ³¨æ„ Mutate åªæ˜¯ VisitExpr çš„åˆ«åã€‚ExprMutator çš„ VisitExpr ä¼šè¿”å›ä¸€ä¸ªä¿®æ”¹åçš„æ–° Expr,  çœ‹ä¸€ä¸‹ VisitExpr çš„å®ç°ï¼š

```cpp
Expr ExprMutator::VisitExpr(const Expr& expr) {
  auto it = this->memo_.find(expr);
  if (it != this->memo_.end()) {
    return it->second;
  } else {
    Expr new_expr = ExprFunctor::VisitExpr(expr);
    memo_[expr] = new_expr;
    return new_expr;
  }
}
```

å¯ä»¥çœ‹åˆ°`memo_`å­˜å‚¨äº†å›¾ä¸­çš„å„ä¸ªèŠ‚ç‚¹ã€‚å‚è€ƒIfNodeçš„å®ç°ï¼š

```cpp
Expr ExprMutator::VisitExpr_(const IfNode* op) {
  auto guard = this->Mutate(op->cond);
  auto true_b = this->Mutate(op->true_branch);
  auto false_b = this->Mutate(op->false_branch);
  if (op->cond.same_as(guard) && op->true_branch.same_as(true_b) &&
      op->false_branch.same_as(false_b)) {
    return GetRef<Expr>(op);
  } else {
    return If(guard, true_b, false_b, op->span);
  }
}
```

å¦‚æœ`IFNode`çš„å­èŠ‚ç‚¹éƒ½æ²¡æœ‰è¢«ä¿®æ”¹ï¼Œé‚£ä¹ˆå°±è¿”å›è¿™ä¸ªèŠ‚ç‚¹æœ¬èº«ã€‚å¦åˆ™åˆ›å»ºæ–°çš„èŠ‚ç‚¹`If(guard, true_b, false_b, op->span);`å¹¶è¿”å›ã€‚è¿™é‡Œæ„é€ æ–°èŠ‚ç‚¹çš„ç±»Ifçš„å®šä¹‰å’Œå®ç°åˆ†åˆ«åœ¨`https://github.com/apache/tvm/blob/main/src/relay/ir/expr.h`å’Œ`https://github.com/apache/tvm/blob/main/src/relay/ir/expr.cc`ä¸­ï¼š

```cpp
class If : public Expr {
 public:
  /*!
   * \brief The constructor
   * \param cond The condition of a if node.
   * \param true_branch The fall through branch
   * \param false_branch The branch for execution when condition is false.
   * \param span The source span of the expression.
   */
  TVM_DLL If(Expr cond, Expr true_branch, Expr false_branch, Span span = Span());

  TVM_DEFINE_OBJECT_REF_METHODS(If, RelayExpr, IfNode);
};

If::If(Expr cond, Expr true_branch, Expr false_branch, Span span) {
  ObjectPtr<IfNode> n = make_object<IfNode>();
  n->cond = std::move(cond);
  n->true_branch = std::move(true_branch);
  n->false_branch = std::move(false_branch);
  n->span = std::move(span);
  data_ = std::move(n);
```

TVMçš„Passé‡Œé¢æœ‰ä¸€ä¸ªç»å…¸çš„ç®—ç¬¦èåˆPassï¼Œä¹‹å‰åœ¨[ã€ä»é›¶å¼€å§‹å­¦æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ã€‘å…«ï¼ŒTVMçš„ç®—ç¬¦èåˆä»¥åŠå¦‚ä½•ä½¿ç”¨TVM Pass Infraè‡ªå®šä¹‰Pass](https://mp.weixin.qq.com/s/QphPwnRE5uANJk2qiqlI6w) è¿™é‡Œè®²è¿‡ï¼Œæ„Ÿå…´è¶£çš„å°ä¼™ä¼´å¯ä»¥çœ‹ä¸€ä¸‹ã€‚

# 0x5. Schedule
æˆ‘è®¤ä¸ºTVMçš„Scheduleä¸»è¦åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šTE Scheduleï¼ŒTIR Scheduleä»¥åŠAuto Scheduleã€‚ç”±äºç²¾åŠ›æœ‰é™æˆ‘è¿˜æ²¡æœ‰æ¢ç´¢Scheduleåœ¨TVMçš„æºç å®ç°ï¼Œä¸è¿‡æœ€è¿‘TVMåœˆå­çš„è¿™ç¯‡æ¥è‡ªKordå¤§ä½¬çš„ã€ŠTVM è‡ªåº•å‘ä¸Šï¼ˆå››ï¼‰ï¼šTE/TIR Schedule çš„åŸç†ã€‹æ–‡ç« ä¸ºæˆ‘ä»¬ç†æ¸…äº†TE/TIR Scheduleçš„åŸç†ï¼Œä¸ªäººæ¨èå¤§å®¶å»é˜…è¯»ã€‚é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/534062007 ã€‚

ç„¶åå…³äºTE Scheduleçš„è°ƒä¼˜ä»¥åŠAuto Scheduleå¯ä»¥çœ‹ä¸€ä¸‹[ã€TVM ä¸‰ä»£ä¼˜åŒ–å·¡ç¤¼ã€‘åœ¨X86ä¸Šå°†æ™®é€šçš„çŸ©é˜µä¹˜æ³•ç®—å­æé€Ÿ90å€](https://mp.weixin.qq.com/s/d8v9Q3EAkv8TknP5Hh7N7A) ä»¥åŠ [ã€tvmç®—å­ä¼˜åŒ–scheduleï¼ˆäºŒï¼‰--GPUç¯‡ã€‘](https://zhuanlan.zhihu.com/p/403370698) è¿™å‡ ç¯‡æ–‡ç« ã€‚



# 0x6. Runtime

## åŸºç¡€æ¦‚å¿µ
### åŸºç¡€æ¦‚å¿µ1: PackedFunc
ä¸ºäº†ä¾¿äºPythonå’ŒC++æ··åˆç¼–ç¨‹ï¼ŒTVMä½¿ç”¨äº†ç»Ÿä¸€çš„PackedFuncæœºåˆ¶ã€‚PackedFuncå¯ä»¥å°†C++çš„å‡½æ•°æ‰“åŒ…æˆç»Ÿä¸€çš„å‡½æ•°æ¥å£å¹¶å¯¼å‡ºåˆ°Pythonç«¯ä¾›ç”¨æˆ·ä½¿ç”¨ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒä»Pythonä¸­æ³¨å†Œä¸€ä¸ªå‡½æ•°ï¼Œå¹¶ä¼ªè£…æˆPackedFuncåœ¨C++å’ŒPythonä¸­è°ƒç”¨ã€‚è¿™é‡Œæ¨èä¸€ç¯‡è®²è§£PackedFuncåŸç†çš„ä¼˜è´¨åšå®¢ï¼šhttps://hjchen2.github.io/2020/01/10/TVM-PackedFunc%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6/ ã€‚
### åŸºç¡€æ¦‚å¿µ2: tvm.runtime.Module
tvm.runtime.Moduleæ˜¯tvmç¼–è¯‘çš„ç»“æœï¼ˆè¿™ä¸€èŠ‚ä¹‹åç®€ç§°Moduleï¼‰ã€‚Moduleä¸­åŒ…å«ä¸€ç³»åˆ—å¯ä»¥è¿è¡Œçš„PackedFuncï¼ˆæ‰€ä»¥è¿™é‡Œçš„Moduleå¯ä»¥çœ‹ä½œ<name, PackedFunc>çš„å“ˆå¸Œè¡¨ï¼‰ï¼Œå¹¶ä¸”Moduleå¯ä»¥importå¦ä¸€ä¸ªModuleï¼Œä»è€Œè®¿é—®å…¶å®ƒModuleçš„PackedFuncã€‚æˆ‘ä»¬çœ‹ä¸€ä¸‹Moduleçš„æ¥å£å®šä¹‰ï¼ˆ`https://github.com/apache/tvm/blob/main/include/tvm/runtime/module.h#L47-L89`ï¼‰ï¼š

```cpp
/*!
 * \brief Module container of TVM.
 */
class Module : public ObjectRef {
 public:
  Module() {}
  // constructor from container.
  explicit Module(ObjectPtr<Object> n) : ObjectRef(n) {}
  /*!
   * \brief Get packed function from current module by name.
   *
   * \param name The name of the function.
   * \param query_imports Whether also query dependency modules.
   * \return The result function.
   *  This function will return PackedFunc(nullptr) if function do not exist.
   * \note Implemented in packed_func.cc
   */
  inline PackedFunc GetFunction(const std::string& name, bool query_imports = false);
  // The following functions requires link with runtime.
  /*!
   * \brief Import another module into this module.
   * \param other The module to be imported.
   *
   * \note Cyclic dependency is not allowed among modules,
   *  An error will be thrown when cyclic dependency is detected.
   */
  inline void Import(Module other);
  ...
};
```

ç„¶åModuleçš„å…·ä½“å®ç°ç”±ModuleNodeè´Ÿè´£ï¼Œå¹¶ä¸”ä¸åŒçš„targetå¯¹åº”ä¸åŒçš„ModuleNodeå®ç°ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹CUDAModuldeNodeçš„å®šä¹‰ï¼ˆ`https://github.com/apache/tvm/blob/main/src/runtime/cuda/cuda_module.cc#L44`ï¼‰, è¯·æ³¨æ„çœ‹ä¸‹é¢çš„æ³¨é‡Š:

```cpp
// Module to support thread-safe multi-GPU execution.
// cuModule is a per-GPU module
// The runtime will contain a per-device module table
// The modules will be lazily loaded
// CUDAModuleNodeå¯¹åº”åˆ°CUDAä¸­çš„CUmodule
class CUDAModuleNode : public runtime::ModuleNode {
 public:
  ...
	// è°ƒç”¨cuModuleGetFunctionä»CUmoduleä¸­è·å–kernel function handle
  PackedFunc GetFunction(const std::string& name, const ObjectPtr<Object>& sptr_to_self) final;

  // è°ƒç”¨cuModuleGetGlobalä»CUmoduleä¸­è·å–å…¨å±€å˜é‡æŒ‡é’ˆ
  CUdeviceptr GetGlobal(int device_id, const std::string& global_name, size_t expect_nbytes) {
    std::lock_guard<std::mutex> lock(mutex_);
    // must recheck under the lock scope
    if (module_[device_id] == nullptr) {
      CUDA_DRIVER_CALL(cuModuleLoadData(&(module_[device_id]), data_.c_str()));
    }
    CUdeviceptr global;
    size_t nbytes;

    CUresult result = cuModuleGetGlobal(&global, &nbytes, module_[device_id], global_name.c_str());
    ICHECK_EQ(nbytes, expect_nbytes);
    if (result != CUDA_SUCCESS) {
      const char* msg;
      cuGetErrorName(result, &msg);
      LOG(FATAL) << "CUDAError: cuModuleGetGlobal " << global_name << " failed with error: " << msg;
    }
    return global;
  }

 private:
  ...
  std::array<CUmodule, kMaxNumGPUs> module_;
  ...
};
```

æˆ‘ä»¬çœ‹ä¸€ä¸‹æ ¸å¿ƒçš„GetFunctionå®ç°ï¼ˆhttps://github.com/apache/tvm/blob/main/src/runtime/cuda/cuda_module.cc#L244-L257ï¼‰ï¼š

```cpp
PackedFunc CUDAModuleNode::GetFunction(const std::string& name,
                                       const ObjectPtr<Object>& sptr_to_self) {
  ICHECK_EQ(sptr_to_self.get(), this);
  ICHECK_NE(name, symbol::tvm_module_main) << "Device function do not have main";
  // å¦‚æœnameæ˜¯tvm_prepare_global_barrierï¼Œåˆ™å°†CUDAPrepGlobalBarrieråŒ…æˆä¸€ä¸ªPackedFuncè¿”å›
  if (name == symbol::tvm_prepare_global_barrier) {
    return PackedFunc(CUDAPrepGlobalBarrier(this, sptr_to_self));
  }
  auto it = fmap_.find(name);
  if (it == fmap_.end()) return PackedFunc();
  const FunctionInfo& info = it->second;
  CUDAWrappedFunc f;
  f.Init(this, sptr_to_self, name, info.arg_types.size(), info.launch_param_tags);
  // è¿”å›kernel function
  return PackFuncVoidAddr(f, info.arg_types);
}
```

è¿™é‡Œé¦–å…ˆæ ¹æ®å‡½æ•°çš„åç§°æ‰¾åˆ°æè¿°è¿™ä¸ªå‡½æ•°çš„FunctionInfoï¼Œè€ŒFunctionInfoé‡Œé¢åŒ…å«äº†launch_param_tagsæˆå‘˜ï¼Œè¿™ä¸ªæˆå‘˜ä¸­å­˜å‚¨äº†CUDA Kernel Launchæ—¶éœ€è¦çš„gridDim/blockDim/SharedMemorySizeï¼Œç„¶åå°†ä¸Šä¸‹æ–‡æ‰“åŒ…åˆ°CUDAWrappedFuncä¸­å¹¶åŒ…è£…ä¸ºä¸€ä¸ªPackFuncè¿”å›ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹CUDAWrappedFuncæ˜¯æ€ä¹ˆæ‰§è¡Œçš„ï¼ˆhttps://github.com/apache/tvm/blob/main/src/runtime/cuda/cuda_module.cc#L164-L203ï¼‰ã€‚

```cpp
// invoke the function with void arguments
  void operator()(TVMArgs args, TVMRetValue* rv, void** void_args) const {
    int device_id;
    CUDA_CALL(cudaGetDevice(&device_id));
    ThreadWorkLoad wl = launch_param_config_.Extract(args);

    if (fcache_[device_id] == nullptr) {
      fcache_[device_id] = m_->GetFunc(device_id, func_name_);
      if (wl.dyn_shmem_size >= (48 << 10)) {
        // Assumption: dyn_shmem_size doesn't change across different invocations of
        // fcache_[device_id]
        CUresult result = cuFuncSetAttribute(
            fcache_[device_id], CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, wl.dyn_shmem_size);
        if (result != CUDA_SUCCESS) {
          LOG(FATAL) << "Failed to set the allowed dynamic shared memory size to "
                     << wl.dyn_shmem_size;
        }
      }
    }
    CUstream strm = static_cast<CUstream>(CUDAThreadEntry::ThreadLocal()->stream);
    CUresult result = cuLaunchKernel(fcache_[device_id], wl.grid_dim(0), wl.grid_dim(1),
                                     wl.grid_dim(2), wl.block_dim(0), wl.block_dim(1),
                                     wl.block_dim(2), wl.dyn_shmem_size, strm, void_args, nullptr);
    if (result != CUDA_SUCCESS && result != CUDA_ERROR_DEINITIALIZED) {
      const char* msg;
      cuGetErrorName(result, &msg);
      std::ostringstream os;
      os << "CUDALaunch Error: " << msg << "\n"
         << " grid=(" << wl.grid_dim(0) << "," << wl.grid_dim(1) << "," << wl.grid_dim(2) << "), "
         << " block=(" << wl.block_dim(0) << "," << wl.block_dim(1) << "," << wl.block_dim(2)
         << ")\n";
      std::string cuda = m_->GetSource("");
      if (cuda.length() != 0) {
        os << "// func_name=" << func_name_ << "\n"
           << "// CUDA Source\n"
           << "// -----------\n"
           << cuda;
      }
      LOG(FATAL) << os.str();
    }
  }
```

ä»è¿™é‡Œå¯ä»¥çœ‹åˆ°CUDAWrappedFuncä¼šæ ¹æ®func_nameåœ¨CUDAModuleNodeä¸­æ‰¾åˆ°CUfunctionç„¶åæ ¹æ®launch_param_config_è¿›è¡ŒKernel Launchã€‚è¿™é‡Œçš„fcache_[device_id]æ˜¯ç”¨æ¥ç¼“å­˜å½“å‰deviceä¸Šçš„CUFunctionçš„ï¼Œé¿å…é‡å¤æŸ¥æ‰¾å¸¦æ¥çš„é¢å¤–å¼€é”€ã€‚å¦å¤–åœ¨CUDAModuleNode::GetFunctionçš„å®šä¹‰ä¸­æåˆ°å¦‚æœnameæ˜¯tvm_prepare_global_barrierï¼Œåˆ™å°†CUDAPrepGlobalBarrieråŒ…æˆä¸€ä¸ªPackedFuncè¿”å›ï¼Œåœ¨CUDA 9.0ä¹‹å‰æ˜¯ä¸æ”¯æŒGlobal Barrierçš„ï¼Œæ‰€ä»¥è¿™é‡ŒTVMé€šè¿‡ç±»ä¼¼spin lockçš„æ–¹å¼ï¼Œè‡ªæ—‹åœ°æ£€æŸ¥ä¸€ä¸ªå…¨å±€å˜é‡çš„å€¼æ¥block çº¿ç¨‹æ‰§è¡Œï¼Œä»è€Œå®ç°Global Barrierã€‚æ ¸å¿ƒå®ç°è§ï¼š

```cpp
class CUDAPrepGlobalBarrier {
 public:
  CUDAPrepGlobalBarrier(CUDAModuleNode* m, ObjectPtr<Object> sptr) : m_(m), sptr_(sptr) {
    std::fill(pcache_.begin(), pcache_.end(), 0);
  }
	// ç”¨ä¸€ä¸ªglobal variableæ¥å®ç°GPUä¸Šçš„global barrierã€‚æ­¤å‡½æ•°ç”¨æ¥set global variable to 1ã€‚
	// ç„¶åkernel functionä¸­ä¼šspinçš„check global variableçš„å€¼ï¼Œä¸º1ä¹‹åï¼Œå†è¿›è¡Œæ¥ä¸‹æ¥çš„æ“ä½œã€‚
	// è¯¦ç»†çœ‹:https://github.com/apache/tvm/pull/362#issuecomment-323781410
  void operator()(const TVMArgs& args, TVMRetValue* rv) const {
    int device_id;
    CUDA_CALL(cudaGetDevice(&device_id));
    if (pcache_[device_id] == 0) {
      pcache_[device_id] =
          m_->GetGlobal(device_id, runtime::symbol::tvm_global_barrier_state, sizeof(unsigned));
    }
    CUDA_DRIVER_CALL(cuMemsetD32(pcache_[device_id], 0, 1));
  }

 private:
  // internal module
  CUDAModuleNode* m_;
  // the resource holder
  ObjectPtr<Object> sptr_;
  // mark as mutable, to enable lazy initialization
  mutable std::array<CUdeviceptr, kMaxNumGPUs> pcache_;
};

```

é™¤äº†CUDAModuleNodeä¹‹å¤–ï¼Œå…¶å®ƒçš„ç¡¬ä»¶æŠ½è±¡éƒ½å®ç°äº†ä¸€ä¸ªå¯¹åº”çš„ModuleNodeæ¯”å¦‚OpenCLModuleNodeï¼ŒROCMModuleNodeç­‰ç­‰ã€‚å€ŸåŠ©Moduleå’ŒPackFuncæˆ‘ä»¬å¯ä»¥å°†ä¸åŒdeviceç”Ÿæˆçš„ä»£ç æ‰“åŒ…æˆç»Ÿä¸€çš„å½¢å¼ã€‚ä½†å¦‚æœæƒ³è¦æ‰§è¡Œè¿™äº›ç”Ÿæˆçš„ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦åšå†…å­˜ç®¡ç†ï¼ŒåŒæ­¥ç­‰ä¸€ç³»åˆ—æ“ä½œï¼ŒTVMå°†è¿™äº›æ“ä½œæŠ½è±¡ä¸ºDeviceAPIã€‚

### åŸºç¡€æ¦‚å¿µ3: DeviceAPI æŠ½è±¡
TVMé€šè¿‡DeviceAPI ç±»æ¥å¯¹ç¡¬ä»¶çš„èƒ½åŠ›è¿›è¡ŒæŠ½è±¡ï¼Œå½¢æˆäº†å‡ ä¸ªç»Ÿä¸€çš„æ¥å£ï¼ˆåœ¨OneFlowä¸­æœ‰ä¸€ä¸ªç¡¬ä»¶æŠ½è±¡æ¨¡å—EPå’Œè¿™ä¸ªç±»ä¼¼ï¼‰ã€‚åªè¦ä¸ºæ¯ä¸€ç§deviceé‡è½½äº†è¿™äº›ç»Ÿä¸€çš„æ¥å£ï¼Œé‚£ä¹ˆæ‰§è¡Œå™¨ï¼ˆruntimeï¼‰å°±å¯ä»¥é€šè¿‡è®¿é—®è¿™äº›ç»Ÿä¸€çš„æ¥å£ä½¿ç”¨deviceçš„æŸç§èƒ½åŠ›ï¼Œæ¯”å¦‚æŸ¥è¯¢å‚æ•°ï¼Œå†…å­˜åˆ†é…ï¼Œæ•°æ®æ‹·è´ï¼ŒåŒæ­¥ç­‰ç­‰ã€‚DeviceAPIçš„å®šä¹‰åœ¨ï¼š`https://github.com/apache/tvm/blob/main/include/tvm/runtime/device_api.h#L71`ã€‚è¿™é‡Œæœ‰ä¸€äº›é€šç”¨çš„æ¥å£æ¯”å¦‚SetDeviceï¼ŒGetAttrï¼ŒGetTargetPropertyï¼ŒAllocDataSpaceç­‰ç­‰ï¼Œç„¶åå¯¹äºä¸åŒçš„deviceæ¯”å¦‚cpuï¼Œcudaï¼Œhexagonï¼Œmetalï¼Œrocmï¼Œvulkanï¼Œopencléƒ½ä¼šåŸºäºå„è‡ªçš„runtime apié‡å†™è¿™äº›æ¥å£ã€‚è¿™äº›æ¥å£å¯¹äºTVMçš„æ‰§è¡Œå¼•æ“éå¸¸é‡è¦ã€‚


Moduleï¼ŒPackFuncï¼ŒDeviceAPIåˆ†åˆ«ä»ä¸åŒçš„è§’åº¦å¯¹ç¡¬ä»¶çš„åŠŸèƒ½è¿›è¡Œäº†å°è£…ï¼Œæ¯”å¦‚Moduleå°è£…äº†åŠ è½½device Moduleï¼ˆæ¯”å¦‚CUModuleï¼‰ï¼ŒåŠ è½½Kernelï¼Œç»Ÿä¸€æ‰“åŒ…è®¾å¤‡ä»£ç ç­‰åŠŸèƒ½ï¼ŒDeviceAPIå°è£…äº†å†…å­˜åˆ†é…é‡Šæ”¾ï¼Œæ•°æ®æ‹·è´ç­‰åŠŸèƒ½ï¼Œä½†è¿™äº›åŠŸèƒ½å¿…é¡»è¦æœ‰ä¸€ä¸ªæ‰§è¡Œå¼•æ“å‡‘åˆ°ä¸€èµ·æ‰å¯ä»¥runèµ·æ¥ã€‚TVMæä¾›äº†2ç§æ‰§è¡Œå¼•æ“ã€‚

### Graph Executor
GraphExecutoræ˜¯TVMä¸ºé™æ€æ¨¡å‹è®¾è®¡çš„æ‰§è¡Œå¼•æ“ï¼ˆä¸æ”¯æŒåŠ¨æ€Shapeå’ŒControl Flowï¼‰ã€‚æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸ªGraphExecutoræ‰§è¡Œä¸€ä¸ªRelay Functionçš„ç¤ºä¾‹ï¼ˆhttps://github.com/BBuf/tvm_mlir_learn/blob/main/relay/simplenet.ipynbï¼‰ï¼š

```python
#coding=utf-8
import tvm
from tvm import relay
import numpy as np
from tvm.contrib import graph_executor

# æ„é€ BN
def batch_norm(data,
                     gamma=None,
                     beta=None,
                     moving_mean=None,
                     moving_var=None,
                     **kwargs):
    name = kwargs.get("name")
    kwargs.pop("name")
    if not gamma:
        gamma = relay.var(name + "_gamma")
    if not beta:
        beta = relay.var(name + "_beta")
    if not moving_mean:
        moving_mean = relay.var(name + "_moving_mean")
    if not moving_var:
        moving_var = relay.var(name + "_moving_var")
    return relay.nn.batch_norm(data,
                               gamma=gamma,
                               beta=beta,
                               moving_mean=moving_mean,
                               moving_var=moving_var,
                               **kwargs)[0]

# æ„é€ å·ç§¯
def conv2d(data, weight=None, **kwargs):
    name = kwargs.get("name")
    kwargs.pop("name")
    if not weight:
        weight = relay.var(name + "_weight")
    return relay.nn.conv2d(data, weight, **kwargs)


# æ„é€ å·ç§¯+BN+ReLUçš„simpleNet
def simplenet(data, name, channels, kernel_size=(3, 3), strides=(1, 1),
               padding=(1, 1), epsilon=1e-5):
    conv = conv2d(
        data=data,
        channels=channels,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_layout='NCHW',
        name=name+'_conv')
    bn = batch_norm(data=conv, epsilon=epsilon, name=name + '_bn')
    act = relay.nn.relu(data=bn)
    return act

data_shape = (1, 3, 224, 224)
kernel_shape = (32, 3, 3, 3)
dtype = "float32"
data = relay.var("data", shape=data_shape, dtype=dtype)
act = simplenet(data, "graph", 32, strides=(2, 2))
func = relay.Function(relay.analysis.free_vars(act), act)

np_data = np.random.uniform(-1, 1, (1, 3, 224, 224))

params = {
    "graph_conv_weight": tvm.nd.array(np.random.uniform(-1, 1, (32, 3, 3, 3)).astype(dtype)),
    "graph_bn_gamma": tvm.nd.array(np.random.uniform(-1, 1, (32)).astype(dtype)),
    "graph_bn_beta": tvm.nd.array(np.random.uniform(-1, 1, (32)).astype(dtype)),
    "graph_bn_moving_mean": tvm.nd.array(np.random.uniform(-1, 1, (32)).astype(dtype)),
    "graph_bn_moving_var": tvm.nd.array(np.random.uniform(-1, 1, (32)).astype(dtype)),
}

print(func)

with tvm.transform.PassContext(opt_level=10):
    lib = relay.build(func, "llvm", params=params)


dev = tvm.cpu(0)
dtype = "float32"
m = graph_executor.GraphModule(lib["default"](dev))
# set inputs
m.set_input("data", tvm.nd.array(np_data.astype(dtype)))
# execute
m.run()
# get outputs
tvm_output = m.get_output(0)
```

è¿™é‡Œé¦–å…ˆåˆ›å»ºäº†ä¸€ä¸ªGraphExecutorå¯¹è±¡å¹¶ä½¿ç”¨Relay Functionçš„ç¼–è¯‘ç»“æœå¯¹å…¶è¿›è¡Œåˆå§‹åŒ–ï¼ŒRelayFunctionçš„ç¼–è¯‘ç»“æœåŒ…å«åºåˆ—åŒ–å›¾ç»“æ„ï¼ˆå¯¹åº”executor_configï¼‰ã€kernelï¼ˆå¯¹åº”modï¼‰ã€weightï¼ˆå¯¹åº”paramsï¼‰ã€‚

![relay.buildè¿”å›ç»“æœï¼šhttps://github.com/apache/tvm/blob/main/python/tvm/relay/build_module.py#L178](https://img-blog.csdnimg.cn/1d15e7109a9741aea44227bc3d62c809.png)

æ¥ä¸‹æ¥ä¸ºGraphExecutorå¯¹è±¡è®¾ç½®è¾“å…¥æ•°æ®ï¼Œç„¶åè°ƒç”¨runå­å‡½æ•°æ¥æ‰§è¡Œkernelï¼Œæœ€åget_outputè·å–è¾“å‡ºç»“æœã€‚GraphExecutorçš„å®ç°ä¸»è¦æœ‰2ä¸ªå‡½æ•°ï¼Œç¬¬ä¸€ä¸ªå‡½æ•°å°±æ˜¯Initï¼ˆhttps://github.com/apache/tvm/blob/main/src/runtime/graph_executor/graph_executor.cc#L77ï¼‰ã€‚

```cpp
/*!
 * \brief Initialize the graph executor with graph and device.
 * \param graph_json The execution graph.
 * \param module The module containing the compiled functions for the host
 * processor.
 * \param devs The devices of the host and devices where graph nodes will be
 * executed on.
 * \param lookup_linked_param_func Linked parameter lookup function. Default is nullptr.
 */
void GraphExecutor::Init(const std::string& graph_json, tvm::runtime::Module module,
                         const std::vector<Device>& devs,
                         const PackedFunc lookup_linked_param_func) {
  std::istringstream is(graph_json);
  dmlc::JSONReader reader(&is);
  this->Load(&reader);
  module_ = module;
  devices_ = devs;
  lookup_linked_param_ = lookup_linked_param_func;
  if (lookup_linked_param_ == nullptr) {
    lookup_linked_param_ = PackedFunc(
        [this](TVMArgs args, TVMRetValue* rv) { this->DefaultLookupLinkedParam(args, rv); });
  }
  this->SetupStorage();
  this->SetupOpExecs();
  for (size_t i = 0; i < input_nodes_.size(); i++) {
    const uint32_t nid = input_nodes_[i];
    std::string& name = nodes_[nid].name;
    input_map_[name] = i;
  }
  for (size_t i = 0; i < outputs_.size(); i++) {
    const uint32_t nid = outputs_[i].node_id;
    std::string& name = nodes_[nid].name;
    output_map_[name] = i;
  }
}
```

è¿™ä¸ªå‡½æ•°ä¸­ä¸»è¦åŒ…å«jsonå‚æ•°è§£æã€‚ä¸ºæ¯ä¸€ä¸ªç®—å­çš„input/output edgeå‡†å¤‡å¯¹åº”çš„memoryï¼ˆå¯¹åº”SetupStorageï¼‰

ä»¥åŠä¸ºæ¯ä¸€ä¸ªç®—å­å‡†å¤‡ä¸€ä¸ªå¯è°ƒç”¨çš„kernel functionç”¨æ¥åšå®é™…çš„è®¡ç®—ï¼ˆå¯¹åº”SetupOpExecsï¼‰ã€‚

> jsonå°±æ˜¯è®¡ç®—å›¾çš„è¡¨ç¤ºï¼Œè¡¨ç¤ºäº†nodeä¹‹é—´çš„è¿æ¥å…³ç³»ï¼Œè¾“å…¥ã€è¾“å‡ºnodeã€è¾“å…¥shapeç­‰ä¿¡æ¯ï¼Œä¸Šé¢çš„ä»£ç ä¸­Load(Read)ä¼šæå–jsonä¸­çš„ä¿¡æ¯ï¼Œå­˜å‚¨åœ¨graph_executoræˆå‘˜å˜é‡ä¸­ã€‚


### Virtual Machine

ç›®å‰æˆ‘åŸºæœ¬æ²¡æœ‰ä½¿ç”¨è¿‡è¿™ç§è¿è¡Œæ—¶ï¼Œå¹¶ä¸”äº†è§£ä¹Ÿæ¯”è¾ƒå°‘ï¼Œæ‰€ä»¥è¿™é‡Œå°±ç•™å‘ä¸å±•å¼€äº†ã€‚VMæ˜¯TVMä¸­æ›´åŠ çµæ´»çš„ä¸€ç§è¿è¡Œæ—¶ï¼Œå®ƒå¯ä»¥æ”¯æŒåŠ¨æ€æ¨¡å‹ï¼ˆä¹Ÿå°±æ˜¯å¸¦åŠ¨æ€Shapeå’ŒControl Flowçš„ï¼‰çš„æ‰§è¡Œã€‚å…¶å®ï¼Œä»MLCçš„è¯¾ä»¶ä¹Ÿå¯ä»¥çœ‹åˆ°Relaxåœ¨å¤„ç†åŠ¨æ€Shapeç¨‹åºæ—¶ä¹Ÿç”¨åˆ°äº†è¿™ä¸ªè¿è¡Œæ—¶ã€‚

ä¸€ä½Intelçš„å·¥ç¨‹å¸ˆåœ¨ã€ŠTVM Runtime System æ¦‚è¿°ã€‹ä»‹ç»äº†TVMçš„Relay Virtual Machineè¿è¡Œæ—¶ï¼Œæ„Ÿå…´è¶£çš„å°ä¼™ä¼´å¯ä»¥å»é˜…è¯»ä¸€ä¸‹ï¼šhttps://zhuanlan.zhihu.com/p/504066888 ã€‚


# 0x7. Codegen

ä¹‹å‰æåˆ°IRModuleæ˜¯ç¼–è¯‘çš„æœ€å°å•å…ƒï¼Œç„¶åå½“æˆ‘ä»¬æ‰§è¡Œç±»ä¼¼äº`mod = tvm.build(ir_module, target="c/cuda/llvm")` å¯ä»¥å°†IRModuleç¼–è¯‘ä¸º`tvm.runtime.Module`ï¼Œè¿™é‡Œçš„targetå‚æ•°å°±æ˜¯ç”¨æ¥é€‰æ‹©ä½¿ç”¨å“ªä¸€ä¸ªCodeGenæ¥ç¼–è¯‘TIR ASTçš„ã€‚æ¯”å¦‚æˆ‘ä»¬è¦ç¼–è¯‘CPUå¯ä»¥æ‰§è¡Œçš„ä»£ç ï¼Œé‚£ä¹ˆtargetå‚æ•°å¯ä»¥é€‰æ‹©"c"æˆ–è€…"llvm"ã€‚å¦‚æœè¦ç¼–è¯‘æˆCUDAä»£ç ï¼Œé‚£ä¹ˆå‚æ•°è®¾ç½®ä¸º"cuda"æˆ–è€…â€œllvmâ€ã€‚ç„¶åtvm.buildä¼šæ ¹æ®targetå‚æ•°æ‰¾å·²ç»æ³¨å†Œçš„buildå‡½æ•°ï¼Œåœ¨TVMä¸­ä½¿ç”¨TVM_REGISTER_GLOBALå®æ³¨å†Œbuildå‡½æ•°ã€‚ä¾‹å¦‚ï¼šhttps://github.com/apache/tvm/blob/main/src/target/source/codegen_c_host.cc#L466 è¿™é‡Œçš„`TVM_REGISTER_GLOBAL("target.build.c").set_body_typed(BuildCHost);` ä»¥åŠ https://github.com/apache/tvm/blob/main/src/target/opt/build_cuda_on.cc#L165 è¿™é‡Œçš„`TVM_REGISTER_GLOBAL("target.build.cuda").set_body_typed(BuildCUDA);` ã€‚

æˆ‘ä»¬è¿™é‡Œä»¥ç”Ÿæˆcä»£ç ä¸ºä¾‹ä»‹ç»ä¸€ä¸‹Codegençš„åŸç†ã€‚å½“target="c"æ—¶ï¼Œtvm.buildè°ƒç”¨çš„æ˜¯æå‰æ³¨å†Œçš„target.build.cçš„å…¨å±€å‡½æ•°ï¼ˆ`https://github.com/apache/tvm/blob/main/src/target/source/codegen_c_host.cc#L390`ï¼‰ã€‚ä»£ç å®ç°å¦‚ä¸‹ï¼š

```cpp
runtime::Module BuildCHost(IRModule mod, Target target) {
  using tvm::runtime::Registry;
  bool output_ssa = false;
  bool emit_asserts = false;

  std::unordered_set<std::string> devices;
  if (mod->GetAttr<Map<GlobalVar, String>>("device_contexts") != nullptr) {
    Map<GlobalVar, String> device_contexts =
        mod->GetAttr<Map<GlobalVar, String>>("device_contexts").value();
    for (auto const& context : device_contexts) {
      devices.insert(context.second.data());
    }
  }
  // åˆå§‹åŒ–CodeGenCHostå¯¹è±¡
  CodeGenCHost cg;
  cg.Init(output_ssa, emit_asserts, target->str(), devices);
  cg.SetConstantsByteAlignment(target->GetAttr<Integer>("constants-byte-alignment").value_or(16));
  PrimFunc aot_executor_fn;

  std::vector<std::pair<tvm::GlobalVar, tvm::BaseFunc>> funcs;
  for (auto kv : mod->functions) {
    // Make sure that the executor function is the last one to be code generated so that all the
    // symbols are available to __tvm_main__
    auto fun_name = std::string(kv.first->name_hint);
    bool is_aot_executor_fn = kv.second->GetAttr<Bool>("runner_function", Bool(false)).value();

    if (is_aot_executor_fn) {
      aot_executor_fn = Downcast<PrimFunc>(kv.second);
      continue;
    }
    funcs.push_back(kv);
  }

  // Sort functions
  std::sort(funcs.begin(), funcs.end(),
            [](std::pair<tvm::GlobalVar, tvm::BaseFunc> kv_a,
               std::pair<tvm::GlobalVar, tvm::BaseFunc> kv_b) {
              std::string name_hint_a = kv_a.first->name_hint;
              std::string name_hint_b = kv_b.first->name_hint;
              return name_hint_a < name_hint_b;
            });

  // Add all functions except __tvm_main__
  // æŠŠIRModuleé‡Œæ‰€æœ‰çš„tir::PrimFuncéƒ½æ”¾åˆ°ç¼–è¯‘åˆ—è¡¨é‡Œé¢
  for (auto& kv : funcs) {
    ICHECK(kv.second->IsInstance<PrimFuncNode>()) << "CodegenCHost: Can only take PrimFunc";
    auto f = Downcast<PrimFunc>(kv.second);
    cg.AddFunction(f);
  }

  // Add __tvm_main__
  if (aot_executor_fn.defined()) {
    cg.AddFunction(aot_executor_fn);
  }

  // NOTE: it's possible that kRuntime attr is not attached when the mod was built with tvm.build().
  // See issue #10373.
  auto opt_runtime = mod->GetAttr<relay::Runtime>(tvm::attr::kRuntime);
  relay::Runtime runtime;
  if (opt_runtime.get() != nullptr) {
    runtime = opt_runtime.value();
  } else {
    runtime = relay::Runtime::Create("cpp", {});
  }
  if (aot_executor_fn.defined() && runtime->name == relay::kTvmRuntimeCpp) {
    cg.InitGlobalContext();
  }

  if (target->GetAttr<Bool>("system-lib").value_or(Bool(false))) {
    ICHECK_EQ(target->GetAttr<String>("runtime").value_or(""), "c")
        << "c target only supports generating C runtime SystemLibs";
  }
  // cg.Finish()æ˜¯æ ¸å¿ƒçš„å‡½æ•°ï¼Œå°†IRModule Lowerä¸ºcä»£ç 
  std::string code = cg.Finish();
  // ç¼–è¯‘cä»£ç å¹¶åˆ›å»ºruntime::Module wrapperã€‚
  return CSourceModuleCreate(code, "c", cg.GetFunctionNames());
}

```

ä¸Šé¢ä»£ç ä¸­çš„æ ¸å¿ƒæ˜¯CodeGenCHostè¿™ä¸ªç±»ï¼Œè¿™ä¸ªç±»å®šä¹‰åœ¨ https://github.com/apache/tvm/blob/main/src/target/source/codegen_c_host.h#L40 ã€‚è¿™ä¸ªç±»åˆç»§æ‰¿è‡ªCodegenCç±»ï¼Œhttps://github.com/apache/tvm/blob/main/src/target/source/codegen_c.h#L59 ã€‚æˆ‘ä»¬çœ‹ä¸€ä¸‹CodegenCç±»çš„å®šä¹‰ï¼ˆç®€åŒ–äº†ä»£ç ï¼‰ï¼š


```cpp
/*!
 * \brief A base class to generate C code.
 *
 *  CodeGenC have two modes: generate SSA formed C code or normal form.
 *
 * **NOTE** CodeGenC does not aim at generating C codes consumed by MSVC or GCC,
 * Rather, it's providing infrastructural abstraction for C variants like CUDA
 * and OpenCL-C. You might find some odd variant features, e.g., type `int3` for
 * a vector of 3 `int`s. For native C code generator, see `CodeGenLLVM`.
 */
class CodeGenC : public ExprFunctor<void(const PrimExpr&, std::ostream&)>,
                 public StmtFunctor<void(const Stmt&)>,
                 public CodeGenSourceBase {
 public:
  /*!
   * \brief Initialize the code generator.
   * \param output_ssa Whether output SSA.
   */
  void Init(bool output_ssa);
  /*!
   * \brief Add the function to the generated module.
   * \param f The function to be compiled.
   * \param whether to append return 0 in the end.
   */
  void AddFunction(const PrimFunc& f);
  /*!
   * \brief Finalize the compilation and return the code.
   * \return The code.
   */
  std::string Finish();
  /*!
   * \brief Print the Stmt n to CodeGenC->stream
   * \param n The statement to be printed.
   */
  void PrintStmt(const Stmt& n) { VisitStmt(n); }
  /*!
   * \brief Print the expression n(or its ssa id if in ssa mode) into os
   * \param n The expression to be printed.
   * \param os The output stream
   */
  void PrintExpr(const PrimExpr& n, std::ostream& os);
  /*!
   * \brief Same as PrintExpr, but simply returns result string
   * \param n The expression to be printed.
   */
  std::string PrintExpr(const PrimExpr& n) {
    std::ostringstream os;
    PrintExpr(n, os);
    return os.str();
  }
  // The following parts are overloadable print operations.
  /*!
   * \brief Print the function header before the argument list
   *
   *  Example: stream << "void";
   */
  virtual void PrintFuncPrefix();  // NOLINT(*)
  /*!
   * \brief Print extra function attributes
   *
   *  Example: __launch_bounds__(256) for CUDA functions
   */
  virtual void PrintExtraAttrs(const PrimFunc& f);
  /*!
   * \brief Print the final return at the end the function.
   */
  virtual void PrintFinalReturn();  // NOLINT(*)
  /*!
   * \brief Insert statement before function body.
   * \param f The function to be compiled.
   */
  virtual void PreFunctionBody(const PrimFunc& f) {}
  /*!
   * \brief Initialize codegen state for generating f.
   * \param f The function to be compiled.
   */
  virtual void InitFuncState(const PrimFunc& f);
  // expression
  void VisitExpr_(const VarNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const LoadNode* op, std::ostream& os) override;        // NOLINT(*)
  void VisitExpr_(const BufferLoadNode* op, std::ostream& os) override;  // NOLINT(*)
  void VisitExpr_(const LetNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const CallNode* op, std::ostream& os) override;        // NOLINT(*)
  void VisitExpr_(const AddNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const SubNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const MulNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const DivNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const ModNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const MinNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const MaxNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const EQNode* op, std::ostream& os) override;          // NOLINT(*)
  void VisitExpr_(const NENode* op, std::ostream& os) override;          // NOLINT(*)
  void VisitExpr_(const LTNode* op, std::ostream& os) override;          // NOLINT(*)
  void VisitExpr_(const LENode* op, std::ostream& os) override;          // NOLINT(*)
  void VisitExpr_(const GTNode* op, std::ostream& os) override;          // NOLINT(*)
  void VisitExpr_(const GENode* op, std::ostream& os) override;          // NOLINT(*)
  void VisitExpr_(const AndNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const OrNode* op, std::ostream& os) override;          // NOLINT(*)
  void VisitExpr_(const CastNode* op, std::ostream& os) override;        // NOLINT(*)
  void VisitExpr_(const NotNode* op, std::ostream& os) override;         // NOLINT(*)
  void VisitExpr_(const SelectNode* op, std::ostream& os) override;      // NOLINT(*)
  void VisitExpr_(const RampNode* op, std::ostream& os) override;        // NOLINT(*)
  void VisitExpr_(const ShuffleNode* op, std::ostream& os) override;     // NOLINT(*)
  void VisitExpr_(const BroadcastNode* op, std::ostream& os) override;   // NOLINT(*)
  void VisitExpr_(const IntImmNode* op, std::ostream& os) override;      // NOLINT(*)
  void VisitExpr_(const FloatImmNode* op, std::ostream& os) override;    // NOLINT(*)
  void VisitExpr_(const StringImmNode* op, std::ostream& os) override;   // NOLINT(*)
  // statment
  void VisitStmt_(const LetStmtNode* op) override;
  void VisitStmt_(const StoreNode* op) override;
  void VisitStmt_(const BufferStoreNode* op) override;
  void VisitStmt_(const ForNode* op) override;
  void VisitStmt_(const WhileNode* op) override;
  void VisitStmt_(const IfThenElseNode* op) override;
  void VisitStmt_(const AllocateNode* op) override;
  void VisitStmt_(const AttrStmtNode* op) override;
  void VisitStmt_(const AssertStmtNode* op) override;
  void VisitStmt_(const EvaluateNode* op) override;
  void VisitStmt_(const SeqStmtNode* op) override;
  void VisitStmt_(const AllocateConstNode* op) override;
  void VisitStmt_(const DeclBufferNode* op) override;
  ...
```

CodegenCç±»çš„å®šä¹‰ä¸­é‡è½½äº†VisitExpr_å’ŒVisitStmt_ä¸¤ç§å‡½æ•°åˆ†åˆ«å¤„ç†TIR ASTä¸­çš„ExpressionèŠ‚ç‚¹ï¼ˆè¡¨è¾¾å¼ï¼‰ å’Œ StatementèŠ‚ç‚¹ï¼ˆè¯­å¥ï¼‰ã€‚Expressionï¼ˆè¡¨è¾¾å¼ï¼‰ä¸­åŒ…å«äº†å¸¸è§çš„å˜é‡å£°æ˜ã€è¿ç®—ã€åˆ¤æ–­ã€å‡½æ•°è°ƒç”¨ï¼Œè€Œ Statementï¼ˆè¯­å¥ï¼‰ä¸­åŒ…å«äº†æ§åˆ¶æµï¼ˆif-elseï¼ŒLoop ç­‰ï¼‰ã€å†…å­˜ç®¡ç†ã€èµ‹å€¼ç­‰æ“ä½œã€‚åœ¨https://github.com/apache/tvm/blob/main/src/target/source/codegen_c.cc ä¸­å¯¹æ¯ä¸€ç§ASTèŠ‚ç‚¹è¿›è¡Œå¯¹åº”çš„ä»£ç ç”Ÿæˆï¼ˆå®šå‘åˆ°ä¸€ä¸ªæ–‡ä»¶è¾“å‡ºæµä¸­ï¼‰ï¼Œæ¯”å¦‚ï¼š

![TIR ASTèŠ‚ç‚¹ä¸€å¯¹ä¸€ç¿»è¯‘ä¸ºCä»£ç ](https://img-blog.csdnimg.cn/458f8ddaea1f47f79e8914c514c226a7.png)

å…¶å®ƒç±»å‹çš„Codegenæ¯”å¦‚CUDAï¼ŒLLVM IRç­‰çš„åŸç†éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªä¸è¿‡æ ¹æ®targetçš„ä¸åŒAST Nodeç¿»è¯‘çš„ç›®æ ‡ä»£ç è¯­å¥çš„è¯­æ³•åˆä¸€ç‚¹åŒºåˆ«è€Œå·²ã€‚

# 0x8. å·¥å…·ä»‹ç»
è¿™ä¸€èŠ‚ä¸ºå¤§å®¶ä»‹ç»2ä¸ªæœ‰ç”¨çš„å·¥å…·ã€‚

ç¬¬ä¸€ä¸ªå·¥å…·æ˜¯ã€ŠFFI Navigator: è·¨è¯­è¨€è°ƒç”¨è·³è½¬IDEæ’ä»¶ã€‹åŸæ–‡è§ï¼šhttps://zhuanlan.zhihu.com/p/103426525 ã€‚è¿™ä¸ªå·¥å…·çš„ä½œç”¨å°±æ˜¯æ”¯æŒtvmé¡¹ç›®ä¸­ä»c++å’Œpythonä¹‹é—´çš„å‡½æ•°è°ƒç”¨è·³è½¬ä»¥åŠç±»å‹objectå®šä¹‰çš„è·³è½¬ã€‚é™¤äº†tvmæœ€è¿‘å°ä¼™ä¼´è¿˜åŠ å…¥äº†å¯¹pytorchï¼Œmxnetï¼Œdglçš„æ”¯æŒï¼Œæœ‰å…´è¶£çš„åŒå­¦ä¹Ÿå¯ä»¥å°è¯•ä¸€ä¸‹ã€‚å¯ä»¥åœ¨vscodeä¸­ç›´æ¥é…ç½®ä½¿ç”¨ã€‚å·¥å…·çš„githubé“¾æ¥ï¼šhttps://github.com/tqchen/ffi-navigator/

ç¬¬äºŒä¸ªå·¥å…·æ˜¯ã€ŠRelay IRå¯è§†åŒ–ã€‹ï¼Œåº”è¯¥ä¹Ÿå¯ä»¥ç”¨åˆ°Relaxä¸Šï¼Œè¿™ä¸ªå·¥å…·æ¥è‡ªä¸€ä¸ªTVMçš„PRï¼ˆhttps://github.com/apache/tvm/pull/3259/filesï¼‰ï¼Œè¿™ä¸ªPRæä¾›äº†ä¸€ä¸ªpython/tvm/relay/visualize.pyæ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥ç¨åŠ ä¿®æ”¹è¿›è¡Œä½¿ç”¨ã€‚ä¿®æ”¹åçš„è„šæœ¬å¦‚ä¸‹ï¼ˆæ³¨æ„è¦æ”¾åˆ°python/tvm/relay/visualize.pyè¿™ä¸ªè·¯å¾„ï¼‰ï¼š

```python
from .expr_functor import ExprFunctor
import networkx as nx

class VisualizeExpr(ExprFunctor):
    def __init__(self):
        super().__init__()
        self.graph = nx.DiGraph()
        self.counter = 0

    def viz(self, expr):
        for param in expr.params:
            self.visit(param)

        return self.visit(expr.body)

    def visit_constant(self, const): # overload this!
        pass

    def visit_var(self, var):
        name = var.name_hint
        self.graph.add_node(name)
        self.graph.nodes[name]['style'] = 'filled'
        self.graph.nodes[name]['fillcolor'] = 'mistyrose'
        return var.name_hint

    def visit_tuple_getitem(self, get_item):
        tuple = self.visit(get_item.tuple_value)
        # self.graph.nodes[tuple]
        index = get_item.index
        # import pdb; pdb.set_trace()
        return tuple

    def visit_call(self, call):
        parents = []
        for arg in call.args:
            parents.append(self.visit(arg))
        # assert isinstance(call.op, _expr.Op)
        name = "{}({})".format(call.op.name, self.counter)
        self.counter += 1
        self.graph.add_node(name)
        self.graph.nodes[name]['style'] = 'filled'
        self.graph.nodes[name]['fillcolor'] = 'turquoise'
        self.graph.nodes[name]['shape'] = 'diamond'
        edges = []
        for i, parent in enumerate(parents):
            edges.append((parent, name, { 'label': 'arg{}'.format(i) }))
        self.graph.add_edges_from(edges)
        return name

def visualize(expr,mydir="relay_ir.png"):
    viz_expr = VisualizeExpr()
    viz_expr.viz(expr)
    graph = viz_expr.graph
    dotg = nx.nx_pydot.to_pydot(graph)
    dotg.write_png(mydir)
```


ç„¶åæˆ‘ä»¬åœ¨tvm_learn/tmp/tvm/python/tvm/relay/__init__.pyæŠŠè¿™ä¸ªvisualizeæ³¨å†Œä¸€ä¸‹ï¼Œæ·»åŠ `from . import visualize` ã€‚

è¿˜éœ€è¦å®‰è£…ä¸€ä¸‹pydotå’Œgraphvizå¯è§†åŒ–åŒ…ï¼š

```powershell
pip3 install pydot
sudo apt-get install graphviz
```

æœ€åæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡å—æ¥åšRelay IRçš„å¯è§†åŒ–äº†ï¼Œè¿˜æ˜¯ä»¥ç¬¬6èŠ‚çš„é‚£ä¸ªä¾‹å­ï¼š

```python
#coding=utf-8
import tvm
from tvm import relay
import numpy as np
from tvm.contrib import graph_executor
from tvm.relay.visualize import visualize

# æ„é€ BN
def batch_norm(data,
                     gamma=None,
                     beta=None,
                     moving_mean=None,
                     moving_var=None,
                     **kwargs):
    name = kwargs.get("name")
    kwargs.pop("name")
    if not gamma:
        gamma = relay.var(name + "_gamma")
    if not beta:
        beta = relay.var(name + "_beta")
    if not moving_mean:
        moving_mean = relay.var(name + "_moving_mean")
    if not moving_var:
        moving_var = relay.var(name + "_moving_var")
    return relay.nn.batch_norm(data,
                               gamma=gamma,
                               beta=beta,
                               moving_mean=moving_mean,
                               moving_var=moving_var,
                               **kwargs)[0]

# æ„é€ å·ç§¯
def conv2d(data, weight=None, **kwargs):
    name = kwargs.get("name")
    kwargs.pop("name")
    if not weight:
        weight = relay.var(name + "_weight")
    return relay.nn.conv2d(data, weight, **kwargs)


# æ„é€ å·ç§¯+BN+ReLUçš„simpleNet
def simplenet(data, name, channels, kernel_size=(3, 3), strides=(1, 1),
               padding=(1, 1), epsilon=1e-5):
    conv = conv2d(
        data=data,
        channels=channels,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_layout='NCHW',
        name=name+'_conv')
    bn = batch_norm(data=conv, epsilon=epsilon, name=name + '_bn')
    act = relay.nn.relu(data=bn)
    return act

data_shape = (1, 3, 224, 224)
kernel_shape = (32, 3, 3, 3)
dtype = "float32"
data = relay.var("data", shape=data_shape, dtype=dtype)
act = simplenet(data, "graph", 32, strides=(2, 2))
func = relay.Function(relay.analysis.free_vars(act), act)

visualize(func)
```

åœ¨å½“å‰ç›®å½•ä¼šç”Ÿæˆå¯è§†åŒ–çš„pngå›¾ç‰‡ï¼Œé¢„è§ˆä¸€ä¸‹ï¼š

![Relay Functionçš„å¯è§†åŒ–ç»“æœ](https://img-blog.csdnimg.cn/200283d093894a9084f33de5b682afe6.png)

æˆ‘ä»¬çŸ¥é“TIR ASTæ˜¯ç”±ä¸€ç³»åˆ—PrimExprå’ŒRelayExprï¼ˆéPrimExprï¼‰æ¥è¡¨ç¤ºçš„ï¼Œå®ƒä»¬éƒ½ç»§æ‰¿äº†TVMçš„ExpråŸºç¡€ç±»ã€‚æ‰€ä»¥TVMé’ˆå¯¹TIR ASTçš„éå†ä¸“é—¨åšäº†ä¸€ä¸ªå·¥å…·ç±»ExprFunctorã€‚è€Œè¿™å¯è§†åŒ–ä¸ªå·¥å…·å°±æ˜¯é€šè¿‡ç»§æ‰¿ExprFunctoræ¥éå†è®¡ç®—å›¾å¹¶è‡ªå®šä¹‰å¯è§†åŒ–æ•ˆæœã€‚


# 0x9. ç»“è®º
è¿™ç¯‡æ–‡ç« å°±æ˜¯å¯¹TVMçš„é‡æ–°æ¢³ç†ï¼Œä»å‰ç«¯åˆ°å›¾ä¼˜åŒ–ä»¥åŠåç«¯ï¼Œæ¯”è¾ƒå®è§‚çš„å™è¿°äº†TVMæ•´ä¸ªæ¶æ„ï¼Œå¸Œæœ›å¯¹å…¥é—¨TVMçš„è¯»è€…æœ‰å¸®åŠ©ã€‚


# 0x10. å‚è€ƒ

## å…¶å®ƒåšå®¢ç²¾é€‰ï¼ˆTVM&MLIR ç›¸å…³ï¼‰
- [æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ TVM ä»£ç ä¸²è®²](https://zhuanlan.zhihu.com/p/446976730)
- [TVM Overview](https://chhzh123.github.io/blogs/2020-03-26-tvm-flow/)
- [TVM - Relay IRè®¡ç®—å›¾å¯è§†åŒ–](https://chhzh123.github.io/blogs/2020-03-25-relay-ir-viz/)
- [TVM - ä»£ç ç”Ÿæˆæµç¨‹](https://chhzh123.github.io/blogs/2020-03-26-tvm-flow/)
- [TVM/VTAä»£ç ç”Ÿæˆæµç¨‹](https://krantz-xrf.github.io/2019/10/24/tvm-workflow.html)
- [tvmç®—å­ä¼˜åŒ–scheduleï¼ˆä¸€ï¼‰--CPUç¯‡](https://zhuanlan.zhihu.com/p/403163009)
- [tvmç®—å­ä¼˜åŒ–scheduleï¼ˆäºŒï¼‰--GPUç¯‡](https://zhuanlan.zhihu.com/p/403370698)
- [TVM Runtime System æ¦‚è¿°](https://zhuanlan.zhihu.com/p/504066888)
- [TVM PackedFuncå®ç°æœºåˆ¶](https://hjchen2.github.io/2020/01/10/TVM-PackedFunc%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6/)
- [å‘å¤–å€ŸåŠ›ï¼šPlutoåŠ©åŠ›MLIRç¼–è¯‘å™¨çš„å¤šé¢ä½“ä¼˜åŒ–](https://mp.weixin.qq.com/s/n33DyOeTjA93HavZBZb94g)
- [TVM è‡ªåº•å‘ä¸Šï¼ˆä¸€ï¼‰ï¼šåŸºæœ¬æ¡†æ¶å’Œæ¦‚å¿µ](https://zhuanlan.zhihu.com/p/532873577)
- [TVM è‡ªåº•å‘ä¸Šï¼ˆäºŒï¼‰ï¼šTIR çš„æ¦‚å¿µå’Œç¼–è¯‘åŸç†](https://zhuanlan.zhihu.com/p/533161438)
- [TVM è‡ªåº•å‘ä¸Šï¼ˆä¸‰ï¼‰ï¼šTE çš„æ¦‚å¿µå’Œç¼–è¯‘åŸç†](https://zhuanlan.zhihu.com/p/534313816)
- [TVM è‡ªåº•å‘ä¸Šï¼ˆå››ï¼‰ï¼šTE/TIR Schedule çš„åŸç†](https://zhuanlan.zhihu.com/p/534062007)
- [é™ˆå¤©å¥‡ MLCè¯¾ç¨‹](https://mlc.ai/zh/index.html)
- [æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨å­¦ä¹ ç¬”è®°å’Œå®è·µä½“ä¼š](https://zhuanlan.zhihu.com/c_1169609848697663488)
- [FFI Navigator: è·¨è¯­è¨€è°ƒç”¨è·³è½¬IDEæ’ä»¶](https://zhuanlan.zhihu.com/p/103426525)

